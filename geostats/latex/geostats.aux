\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\ifx\rEfLiNK\UnDef\gdef \rEfLiNK#1#2{#2}\fi
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:introduction}{{\rEfLiNK{x1-20001}{\csname :autoref\endcsname{chapter}1}}{\rEfLiNK{x1-20001}{\csname :autoref\endcsname{chapter}7}}{\rEfLiNK{x1-20001}{\csname :autoref\endcsname{chapter}Introduction}}{chapter.1}{}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:plotting}{{\rEfLiNK{x1-30002}{\csname :autoref\endcsname{chapter}2}}{\rEfLiNK{x1-30002}{\csname :autoref\endcsname{chapter}11}}{\rEfLiNK{x1-30002}{\csname :autoref\endcsname{chapter}Plotting data}}{chapter.1}{}}
\newlabel{tab:anscombe}{{\rEfLiNK{x1-3002r1}{\csname :autoref\endcsname{minipage}2.1}}{\rEfLiNK{x1-3002r1}{\csname :autoref\endcsname{minipage}11}}{\rEfLiNK{x1-3002r1}{\csname :autoref\endcsname{minipage}Anscombe's quartet of bivariate data pairs.\relax }}{minipage.1}{}}
\newlabel{pg:anscombe}{{\rEfLiNK{x1-3002r2}{\csname :autoref\endcsname{itemize}2}}{\rEfLiNK{x1-3002r2}{\csname :autoref\endcsname{itemize}11}}{\rEfLiNK{x1-3002r2}{\csname :autoref\endcsname{itemize}Anscombe's quartet of bivariate data pairs.\relax }}{itemize.1}{}}
\newlabel{fig:anscombe}{{\rEfLiNK{x1-3004r1}{\csname :autoref\endcsname{document}2.1}}{\rEfLiNK{x1-3004r1}{\csname :autoref\endcsname{document}12}}{\rEfLiNK{x1-3004r1}{\csname :autoref\endcsname{document}Anscombe's quartet shown as bivariate scatter plots.\vspace  \medskipamount \relax }}{document.1}{}}
\newlabel{sec:datatypes}{{\rEfLiNK{x1-40002.1}{\csname :autoref\endcsname{section}2.1}}{\rEfLiNK{x1-40002.1}{\csname :autoref\endcsname{section}12}}{\rEfLiNK{x1-40002.1}{\csname :autoref\endcsname{section}Data types}}{section.1}{}}
\newlabel{tab:catchments}{{\rEfLiNK{x1-4001r2}{\csname :autoref\endcsname{minipage}2.2}}{\rEfLiNK{x1-4001r2}{\csname :autoref\endcsname{minipage}12}}{\rEfLiNK{x1-4001r2}{\csname :autoref\endcsname{minipage}Twenty imaginary river catchments have been analysed for six different types of data: the lithology of the underlying bedrock and its stratigraphic age (where `Cz' is Cenozoic, `Mz' is Mesozoic, `Pz' is Palaeozoic, and `pC' is Precambrian); the number of natural springs in the catchments; the pH of the river water; its Ca/Mg ratio; and the percentage of the catchment area that is covered by vegetation.\vspace  \medskipamount \relax }}{minipage.1}{}}
\newlabel{fig:discrete}{{\rEfLiNK{x1-4006r2}{\csname :autoref\endcsname{document}2.2}}{\rEfLiNK{x1-4006r2}{\csname :autoref\endcsname{document}13}}{\rEfLiNK{x1-4006r2}{\csname :autoref\endcsname{document}Examples of discrete datasets, shown as bar charts. a) Categorical dataset of the dominant lithologies in 20 river catchments. The order of the categories along the horizontal axis is completely arbitrary and can be changed without loss of information. b) Ordinal dataset summarising the stratigraphic age of the catchments, as defined in the caption of Table\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {tab:catchments}. The five categories are listed in chronological order, but span vastly different lengths of time; c) The number of natural springs per catchment.\vspace  \medskipamount \relax }}{document.1}{}}
\newlabel{ftn:Tarantola}{{\rEfLiNK{x1-4009f2}{\csname :autoref\endcsname{enumerate}2}}{\rEfLiNK{x1-4009f2}{\csname :autoref\endcsname{enumerate}13}}{\rEfLiNK{x1-4009f2}{\csname :autoref\endcsname{enumerate}Examples of discrete datasets, shown as bar charts. a) Categorical dataset of the dominant lithologies in 20 river catchments. The order of the categories along the horizontal axis is completely arbitrary and can be changed without loss of information. b) Ordinal dataset summarising the stratigraphic age of the catchments, as defined in the caption of Table\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {tab:catchments}. The five categories are listed in chronological order, but span vastly different lengths of time; c) The number of natural springs per catchment.\vspace  \medskipamount \relax }}{enumerate.1}{}}
\newlabel{fig:continuous}{{\rEfLiNK{x1-4012r3}{\csname :autoref\endcsname{document}2.3}}{\rEfLiNK{x1-4012r3}{\csname :autoref\endcsname{document}14}}{\rEfLiNK{x1-4012r3}{\csname :autoref\endcsname{document}Three continuous datasets, shown as histograms, reporting the following properties of the 20 river catchments of Table\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {tab:catchments}: a) pH of the river water (free to take on positive and negative values); b) the Ca/Mg ratio of the water (non-negative numbers); and c) the proportion of the catchment area covered by vegetation (values between 0 and 100\%).\relax }}{document.1}{}}
\newlabel{sec:histogram}{{\rEfLiNK{x1-50002.2}{\csname :autoref\endcsname{section}2.2}}{\rEfLiNK{x1-50002.2}{\csname :autoref\endcsname{section}14}}{\rEfLiNK{x1-50002.2}{\csname :autoref\endcsname{section}Histograms}}{section.1}{}}
\newlabel{fig:binwidth}{{\rEfLiNK{x1-5002r4}{\csname :autoref\endcsname{minipage}2.4}}{\rEfLiNK{x1-5002r4}{\csname :autoref\endcsname{minipage}14}}{\rEfLiNK{x1-5002r4}{\csname :autoref\endcsname{minipage}Histogram a) uses a bin width of 1\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}pH unit whereas histogram b) uses a bin width of 0.5\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}pH units. The two histograms look considerably different and it is not immediately clear which choice of bin width is best.\relax }}{minipage.1}{}}
\newlabel{fig:binpos}{{\rEfLiNK{x1-5007r5}{\csname :autoref\endcsname{minipage}2.5}}{\rEfLiNK{x1-5007r5}{\csname :autoref\endcsname{minipage}15}}{\rEfLiNK{x1-5007r5}{\csname :autoref\endcsname{minipage}Two histograms of the pH data whose bin widths are the same, but whose bins have been offset by 0.25 pH units. This arbitrary decision strongly affects the appearance of the histogram.\relax }}{minipage.1}{}}
\newlabel{sec:KDE}{{\rEfLiNK{x1-60002.3}{\csname :autoref\endcsname{section}2.3}}{\rEfLiNK{x1-60002.3}{\csname :autoref\endcsname{section}15}}{\rEfLiNK{x1-60002.3}{\csname :autoref\endcsname{section}Kernel density estimation}}{section.1}{}}
\newlabel{fig:rectangles}{{\rEfLiNK{x1-6004r6}{\csname :autoref\endcsname{minipage}2.6}}{\rEfLiNK{x1-6004r6}{\csname :autoref\endcsname{minipage}16}}{\rEfLiNK{x1-6004r6}{\csname :autoref\endcsname{minipage}The rug plot along the bottom axis represents three data points. The grey dashed lines mark rectangular boxes (`kernels') that are centred around each of these data points. The black step function is obtained by taking the sum of these boxes. This procedure removes the need to choose bin locations.\relax }}{minipage.1}{}}
\newlabel{eq:KDE}{{\rEfLiNK{x1-6006r3}{\csname :autoref\endcsname{equation}2.3}}{\rEfLiNK{x1-6006r3}{\csname :autoref\endcsname{equation}16}}{\rEfLiNK{x1-6006r3}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{fig:pHrectKDE}{{\rEfLiNK{x1-6008r7}{\csname :autoref\endcsname{minipage}2.7}}{\rEfLiNK{x1-6008r7}{\csname :autoref\endcsname{minipage}17}}{\rEfLiNK{x1-6008r7}{\csname :autoref\endcsname{minipage}Rectangular KDE of the pH data, constructed using the same procedure as shown in Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:rectangles}, but using a bandwidth $h=0.3276$. To turn the curve into a `density', the area under it has been normalised to unity.\relax }}{minipage.1}{}}
\newlabel{eq:gaussiankernel}{{\rEfLiNK{x1-6010r5}{\csname :autoref\endcsname{equation}2.5}}{\rEfLiNK{x1-6010r5}{\csname :autoref\endcsname{equation}17}}{\rEfLiNK{x1-6010r5}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{fig:pHgaussKDE}{{\rEfLiNK{x1-6013r9}{\csname :autoref\endcsname{minipage}2.9}}{\rEfLiNK{x1-6013r9}{\csname :autoref\endcsname{minipage}17}}{\rEfLiNK{x1-6013r9}{\csname :autoref\endcsname{minipage}Gaussian KDE of the pH data, with bandwidth $h=0.3276$. The continuous curve does more justice to the continuous data than the discrete step function of Figures\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:binwidth}, \let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:binpos} or \let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:pHrectKDE}.\relax }}{minipage.1}{}}
\newlabel{sec:transformations}{{\rEfLiNK{x1-70002.4}{\csname :autoref\endcsname{section}2.4}}{\rEfLiNK{x1-70002.4}{\csname :autoref\endcsname{section}18}}{\rEfLiNK{x1-70002.4}{\csname :autoref\endcsname{section}Data transformations}}{section.1}{}}
\newlabel{fig:negativeKDE}{{\rEfLiNK{x1-7001r11}{\csname :autoref\endcsname{minipage}2.11}}{\rEfLiNK{x1-7001r11}{\csname :autoref\endcsname{minipage}18}}{\rEfLiNK{x1-7001r11}{\csname :autoref\endcsname{minipage}KDE of 20 Ca/Mg ratio measurements. Even though all the measurements are strictly positive, the curve extends into negative data space.\vspace  \medskipamount \relax }}{minipage.1}{}}
\newlabel{fig:logKDE}{{\rEfLiNK{x1-7006r12}{\csname :autoref\endcsname{minipage}2.12}}{\rEfLiNK{x1-7006r12}{\csname :autoref\endcsname{minipage}19}}{\rEfLiNK{x1-7006r12}{\csname :autoref\endcsname{minipage}a) KDE of the Ca/Mg ratio measurements, after applying a logarithmic transformation. Note how the distribution has become more symmetric compared to the linear scale of Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:negativeKDE}. b) The same KDE mapped back to the linear scale. Unlike Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:negativeKDE}, the mapped distribution does not cross over into negative values. \vspace  \medskipamount \relax }}{minipage.1}{}}
\newlabel{fig:vegetationKDE}{{\rEfLiNK{x1-7008r13}{\csname :autoref\endcsname{minipage}2.13}}{\rEfLiNK{x1-7008r13}{\csname :autoref\endcsname{minipage}19}}{\rEfLiNK{x1-7008r13}{\csname :autoref\endcsname{minipage}KDE of 20 vegetation measurements. Even though all the measurements are between 0 and 100\%, the KDE extends beyond these hard limits.\vspace  \medskipamount \relax }}{minipage.1}{}}
\newlabel{eq:logit}{{\rEfLiNK{x1-7010r8}{\csname :autoref\endcsname{equation}2.8}}{\rEfLiNK{x1-7010r8}{\csname :autoref\endcsname{equation}20}}{\rEfLiNK{x1-7010r8}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:invlogit}{{\rEfLiNK{x1-7011r9}{\csname :autoref\endcsname{equation}2.9}}{\rEfLiNK{x1-7011r9}{\csname :autoref\endcsname{equation}20}}{\rEfLiNK{x1-7011r9}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{fig:logitKDE}{{\rEfLiNK{x1-7012r14}{\csname :autoref\endcsname{minipage}2.14}}{\rEfLiNK{x1-7012r14}{\csname :autoref\endcsname{minipage}21}}{\rEfLiNK{x1-7012r14}{\csname :autoref\endcsname{minipage}a) KDE of the vegetation data, after applying a logistic transformation. Note the two horizonal axes. The top axis marks the transformed values on a linear scale that extends from $-\infty $ to $+\infty $. The bottom axis is labelled by the actual vegetation values on a non-linear scale that extends from 0 to 100\%. b) The same distribution mapped back to the 0 -- 100\% interval.\vspace  \medskipamount \relax }}{minipage.1}{}}
\newlabel{sec:multivariate}{{\rEfLiNK{x1-80002.5}{\csname :autoref\endcsname{section}2.5}}{\rEfLiNK{x1-80002.5}{\csname :autoref\endcsname{section}21}}{\rEfLiNK{x1-80002.5}{\csname :autoref\endcsname{section}Multivariate distributions}}{section.1}{}}
\newlabel{fig:KDE2D}{{\rEfLiNK{x1-8001r15}{\csname :autoref\endcsname{minipage}2.15}}{\rEfLiNK{x1-8001r15}{\csname :autoref\endcsname{minipage}21}}{\rEfLiNK{x1-8001r15}{\csname :autoref\endcsname{minipage}Old Faithful eruption measurements. The dataset records 272 observations of 2 variables: the duration of each eruption, and the waiting time between them. Both variables are expressed in minutes. The lower left panel shows the bivariate measurements as grey circles. The contour lines represent a 2-dimensional KDE. The marginal distributions of the waiting times (top) and eruption durations (right) are shown as 1-dimensional KDEs.\relax }}{minipage.1}{}}
\newlabel{sec:ECDF}{{\rEfLiNK{x1-90002.6}{\csname :autoref\endcsname{section}2.6}}{\rEfLiNK{x1-90002.6}{\csname :autoref\endcsname{section}22}}{\rEfLiNK{x1-90002.6}{\csname :autoref\endcsname{section}Empirical cumulative distribution functions}}{section.1}{}}
\newlabel{eq:ECDF}{{\rEfLiNK{x1-9001r10}{\csname :autoref\endcsname{equation}2.10}}{\rEfLiNK{x1-9001r10}{\csname :autoref\endcsname{equation}22}}{\rEfLiNK{x1-9001r10}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{fig:ECDFs}{{\rEfLiNK{x1-9002r16}{\csname :autoref\endcsname{document}2.16}}{\rEfLiNK{x1-9002r16}{\csname :autoref\endcsname{document}22}}{\rEfLiNK{x1-9002r16}{\csname :autoref\endcsname{document}Empirical cumulative distribution functions (ECDFs) of, from left to right: a) the pH data (whose KDE is shown in Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:pHgaussKDE}); b) the Ca/Mg ratios of Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:negativeKDE}; c) the vegetation data of Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:vegetationKDE}; and d) the eruption time data of Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:KDE2D}. Note that ECDFs are only applicable to 1-dimensional datasets.\vspace  \medskipamount \relax }}{document.1}{}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:summary-statistics}{{\rEfLiNK{x1-100003}{\csname :autoref\endcsname{chapter}3}}{\rEfLiNK{x1-100003}{\csname :autoref\endcsname{chapter}23}}{\rEfLiNK{x1-100003}{\csname :autoref\endcsname{chapter}Summary statistics}}{chapter.1}{}}
\newlabel{sec:location}{{\rEfLiNK{x1-110003.1}{\csname :autoref\endcsname{section}3.1}}{\rEfLiNK{x1-110003.1}{\csname :autoref\endcsname{section}23}}{\rEfLiNK{x1-110003.1}{\csname :autoref\endcsname{section}Location}}{section.1}{}}
\newlabel{eq:mean}{{\rEfLiNK{x1-11002r1}{\csname :autoref\endcsname{equation}3.1}}{\rEfLiNK{x1-11002r1}{\csname :autoref\endcsname{equation}23}}{\rEfLiNK{x1-11002r1}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{fig:CaMglocation}{{\rEfLiNK{x1-11008r2}{\csname :autoref\endcsname{minipage}3.2}}{\rEfLiNK{x1-11008r2}{\csname :autoref\endcsname{minipage}25}}{\rEfLiNK{x1-11008r2}{\csname :autoref\endcsname{minipage}KDE and ECDF of the Ca/Mg data (mean = 3.2, median = 2.1, mode = 0.45). The mean is strongly affected by the long `tail' of large outliers. It is 7 times greater than the mode. Only 6 out of 20 catchments (30\%) are larger than the mean of the distribution and only 1 (5\%) is smaller than the mode.\relax }}{minipage.1}{}}
\newlabel{fig:vegetationlocation}{{\rEfLiNK{x1-11010r3}{\csname :autoref\endcsname{document}3.3}}{\rEfLiNK{x1-11010r3}{\csname :autoref\endcsname{document}25}}{\rEfLiNK{x1-11010r3}{\csname :autoref\endcsname{document}KDE and ECDF of a) the vegetation data (mean = 0.51, median = 0.55, mode = 0.97); and b) the geyser eruption data (mean = 3.5, median = 4.0, mode = 4.4). Both of these distributions are `bimodal', meaning that they have two `peaks' in the KDE, corresponding to two steep segments in the ECDFs. The dotted lines mark the highest one of them and ignores the other one. The mean and median fall in between the two modes and are not representative of the data.\vspace  \medskipamount \relax }}{document.1}{}}
\newlabel{sec:dispersion}{{\rEfLiNK{x1-120003.2}{\csname :autoref\endcsname{section}3.2}}{\rEfLiNK{x1-120003.2}{\csname :autoref\endcsname{section}26}}{\rEfLiNK{x1-120003.2}{\csname :autoref\endcsname{section}Dispersion}}{section.1}{}}
\newlabel{eq:stdev}{{\rEfLiNK{x1-12002r3}{\csname :autoref\endcsname{equation}3.3}}{\rEfLiNK{x1-12002r3}{\csname :autoref\endcsname{equation}26}}{\rEfLiNK{x1-12002r3}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:MAD}{{\rEfLiNK{x1-12004r4}{\csname :autoref\endcsname{equation}3.4}}{\rEfLiNK{x1-12004r4}{\csname :autoref\endcsname{equation}27}}{\rEfLiNK{x1-12004r4}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{sec:shape}{{\rEfLiNK{x1-130003.3}{\csname :autoref\endcsname{section}3.3}}{\rEfLiNK{x1-130003.3}{\csname :autoref\endcsname{section}28}}{\rEfLiNK{x1-130003.3}{\csname :autoref\endcsname{section}Shape}}{section.1}{}}
\newlabel{eq:skew}{{\rEfLiNK{x1-13001r5}{\csname :autoref\endcsname{equation}3.5}}{\rEfLiNK{x1-13001r5}{\csname :autoref\endcsname{equation}29}}{\rEfLiNK{x1-13001r5}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{sec:boxplots}{{\rEfLiNK{x1-140003.4}{\csname :autoref\endcsname{section}3.4}}{\rEfLiNK{x1-140003.4}{\csname :autoref\endcsname{section}29}}{\rEfLiNK{x1-140003.4}{\csname :autoref\endcsname{section}Box-and-whisker plots}}{section.1}{}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:probability}{{\rEfLiNK{x1-150004}{\csname :autoref\endcsname{chapter}4}}{\rEfLiNK{x1-150004}{\csname :autoref\endcsname{chapter}31}}{\rEfLiNK{x1-150004}{\csname :autoref\endcsname{chapter}Probability}}{chapter.1}{}}
\newlabel{eq:2H1T}{{\rEfLiNK{x1-15002r2}{\csname :autoref\endcsname{equation}4.2}}{\rEfLiNK{x1-15002r2}{\csname :autoref\endcsname{equation}31}}{\rEfLiNK{x1-15002r2}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:1116}{{\rEfLiNK{x1-15003r3}{\csname :autoref\endcsname{equation}4.3}}{\rEfLiNK{x1-15003r3}{\csname :autoref\endcsname{equation}32}}{\rEfLiNK{x1-15003r3}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{page:multiplication}{{\rEfLiNK{x1-15003r4}{\csname :autoref\endcsname{equation}4}}{\rEfLiNK{x1-15003r4}{\csname :autoref\endcsname{equation}32}}{\rEfLiNK{x1-15003r4}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{page:addition}{{\rEfLiNK{x1-15004f4}{\csname :autoref\endcsname{equation}4}}{\rEfLiNK{x1-15004f4}{\csname :autoref\endcsname{equation}33}}{\rEfLiNK{x1-15004f4}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:additive}{{\rEfLiNK{x1-15005r4}{\csname :autoref\endcsname{equation}4.4}}{\rEfLiNK{x1-15005r4}{\csname :autoref\endcsname{equation}33}}{\rEfLiNK{x1-15005r4}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{sec:permutations}{{\rEfLiNK{x1-160004.1}{\csname :autoref\endcsname{section}4.1}}{\rEfLiNK{x1-160004.1}{\csname :autoref\endcsname{section}34}}{\rEfLiNK{x1-160004.1}{\csname :autoref\endcsname{section}Permutations}}{section.1}{}}
\newlabel{eq:withreplacement}{{\rEfLiNK{x1-16002r5}{\csname :autoref\endcsname{equation}4.5}}{\rEfLiNK{x1-16002r5}{\csname :autoref\endcsname{equation}34}}{\rEfLiNK{x1-16002r5}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:withoutreplacement}{{\rEfLiNK{x1-16004r6}{\csname :autoref\endcsname{equation}4.6}}{\rEfLiNK{x1-16004r6}{\csname :autoref\endcsname{equation}35}}{\rEfLiNK{x1-16004r6}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{sec:combinations}{{\rEfLiNK{x1-170004.2}{\csname :autoref\endcsname{section}4.2}}{\rEfLiNK{x1-170004.2}{\csname :autoref\endcsname{section}36}}{\rEfLiNK{x1-170004.2}{\csname :autoref\endcsname{section}Combinations}}{section.1}{}}
\newlabel{eq:nchoosek}{{\rEfLiNK{x1-17001r7}{\csname :autoref\endcsname{equation}4.7}}{\rEfLiNK{x1-17001r7}{\csname :autoref\endcsname{equation}37}}{\rEfLiNK{x1-17001r7}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{sec:conditionalprobability}{{\rEfLiNK{x1-180004.3}{\csname :autoref\endcsname{section}4.3}}{\rEfLiNK{x1-180004.3}{\csname :autoref\endcsname{section}39}}{\rEfLiNK{x1-180004.3}{\csname :autoref\endcsname{section}Conditional probability}}{section.1}{}}
\newlabel{eq:PA&B}{{\rEfLiNK{x1-18002r9}{\csname :autoref\endcsname{equation}4.9}}{\rEfLiNK{x1-18002r9}{\csname :autoref\endcsname{equation}39}}{\rEfLiNK{x1-18002r9}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:totalprob}{{\rEfLiNK{x1-18003r10}{\csname :autoref\endcsname{equation}4.10}}{\rEfLiNK{x1-18003r10}{\csname :autoref\endcsname{equation}40}}{\rEfLiNK{x1-18003r10}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:Bayes}{{\rEfLiNK{x1-18005r11}{\csname :autoref\endcsname{equation}4.11}}{\rEfLiNK{x1-18005r11}{\csname :autoref\endcsname{equation}41}}{\rEfLiNK{x1-18005r11}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:totalBayes}{{\rEfLiNK{x1-18006r12}{\csname :autoref\endcsname{equation}4.12}}{\rEfLiNK{x1-18006r12}{\csname :autoref\endcsname{equation}41}}{\rEfLiNK{x1-18006r12}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:binomial}{{\rEfLiNK{x1-190005}{\csname :autoref\endcsname{chapter}5}}{\rEfLiNK{x1-190005}{\csname :autoref\endcsname{chapter}43}}{\rEfLiNK{x1-190005}{\csname :autoref\endcsname{chapter}The binomial distribution}}{chapter.1}{}}
\newlabel{fig:goldbar}{{\rEfLiNK{x1-19004r1}{\csname :autoref\endcsname{minipage}5.1}}{\rEfLiNK{x1-19004r1}{\csname :autoref\endcsname{minipage}45}}{\rEfLiNK{x1-19004r1}{\csname :autoref\endcsname{minipage}The probability mass function (PMF) for a binomial experiment with a 2/3 chance of success (and a 1/3 chance of failure) for five gold prospecting claims. The horizontal axis is labelled with the number of claims that produce gold. The vertical axis shows the probability of these respective outcomes.\vspace  \medskipamount \relax }}{minipage.1}{}}
\newlabel{eq:binom}{{\rEfLiNK{x1-19006r1}{\csname :autoref\endcsname{equation}5.1}}{\rEfLiNK{x1-19006r1}{\csname :autoref\endcsname{equation}45}}{\rEfLiNK{x1-19006r1}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:CDF}{{\rEfLiNK{x1-19007r2}{\csname :autoref\endcsname{equation}5.2}}{\rEfLiNK{x1-19007r2}{\csname :autoref\endcsname{equation}46}}{\rEfLiNK{x1-19007r2}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{fig:goldCDF}{{\rEfLiNK{x1-19008r2}{\csname :autoref\endcsname{minipage}5.2}}{\rEfLiNK{x1-19008r2}{\csname :autoref\endcsname{minipage}46}}{\rEfLiNK{x1-19008r2}{\csname :autoref\endcsname{minipage}The cumulative distribution function (CDF) of the binomial distribution. This is the running sum of Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:goldbar}. The horizontal axis is labelled with the number of claims that produce gold. The vertical axis shows the cumulative probability of these respective outcomes. For example, the probability that two or fewer prospectors find gold is 21\%.\relax }}{minipage.1}{}}
\newlabel{sec:binompar}{{\rEfLiNK{x1-200005.1}{\csname :autoref\endcsname{section}5.1}}{\rEfLiNK{x1-200005.1}{\csname :autoref\endcsname{section}46}}{\rEfLiNK{x1-200005.1}{\csname :autoref\endcsname{section}Parameter estimation}}{section.1}{}}
\newlabel{eq:Lbinom}{{\rEfLiNK{x1-20001r3}{\csname :autoref\endcsname{equation}5.3}}{\rEfLiNK{x1-20001r3}{\csname :autoref\endcsname{equation}46}}{\rEfLiNK{x1-20001r3}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:phat}{{\rEfLiNK{x1-20002r4}{\csname :autoref\endcsname{equation}5.4}}{\rEfLiNK{x1-20002r4}{\csname :autoref\endcsname{equation}48}}{\rEfLiNK{x1-20002r4}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{sec:binomH}{{\rEfLiNK{x1-210005.2}{\csname :autoref\endcsname{section}5.2}}{\rEfLiNK{x1-210005.2}{\csname :autoref\endcsname{section}49}}{\rEfLiNK{x1-210005.2}{\csname :autoref\endcsname{section}Hypothesis tests}}{section.1}{}}
\newlabel{it:rejection}{{\rEfLiNK{x1-210065}{\csname :autoref\endcsname{section}5}}{\rEfLiNK{x1-210065}{\csname :autoref\endcsname{section}50}}{\rEfLiNK{x1-210065}{\csname :autoref\endcsname{section}Hypothesis tests}}{section.1}{}}
\newlabel{it:decision}{{\rEfLiNK{x1-210076}{\csname :autoref\endcsname{section}6}}{\rEfLiNK{x1-210076}{\csname :autoref\endcsname{section}50}}{\rEfLiNK{x1-210076}{\csname :autoref\endcsname{section}Hypothesis tests}}{section.1}{}}
\newlabel{pag:notaccepted}{{\rEfLiNK{x1-210076}{\csname :autoref\endcsname{section}6}}{\rEfLiNK{x1-210076}{\csname :autoref\endcsname{section}50}}{\rEfLiNK{x1-210076}{\csname :autoref\endcsname{section}Hypothesis tests}}{section.1}{}}
\newlabel{fig:1sidedbinomialrejection5}{{\rEfLiNK{x1-21008r3}{\csname :autoref\endcsname{minipage}5.3}}{\rEfLiNK{x1-21008r3}{\csname :autoref\endcsname{minipage}51}}{\rEfLiNK{x1-21008r3}{\csname :autoref\endcsname{minipage}a) PMF and b) CDF of a binomial null distribution with $p=2/3$ and $n=5$. The rejection region is marked in black on a). The horizontal dotted line in b) shows the $\alpha =0.05$ cutoff mark. The horizontal dashed line in b) marks the p-value for $k=2$, which is greater than $\alpha $. Therefore, the null hypothesis cannot be rejected.\relax }}{minipage.1}{}}
\newlabel{fig:2sidedbinomialrejection5}{{\rEfLiNK{x1-21016r4}{\csname :autoref\endcsname{minipage}5.4}}{\rEfLiNK{x1-21016r4}{\csname :autoref\endcsname{minipage}52}}{\rEfLiNK{x1-21016r4}{\csname :autoref\endcsname{minipage}a) the same PMF and b) CDF as Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:1sidedbinomialrejection5}. Horizontal dotted lines mark $\alpha /2=0.025$ and $(1-\alpha /2)=0.975$. Their intersections with the CDF are shown as vertical dotted lines and mark the outer edges of the rejection region. The dashed lines mark the observed value and probability, which fall outside the rejection region.\relax }}{minipage.1}{}}
\newlabel{sec:power}{{\rEfLiNK{x1-220005.3}{\csname :autoref\endcsname{section}5.3}}{\rEfLiNK{x1-220005.3}{\csname :autoref\endcsname{section}53}}{\rEfLiNK{x1-220005.3}{\csname :autoref\endcsname{section}Statistical power}}{section.1}{}}
\newlabel{eq:1sidedbinomtest15}{{\rEfLiNK{x1-22001r5}{\csname :autoref\endcsname{equation}5.5}}{\rEfLiNK{x1-22001r5}{\csname :autoref\endcsname{equation}54}}{\rEfLiNK{x1-22001r5}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{fig:1sidedbinomialrejection15}{{\rEfLiNK{x1-22002r5}{\csname :autoref\endcsname{minipage}5.5}}{\rEfLiNK{x1-22002r5}{\csname :autoref\endcsname{minipage}54}}{\rEfLiNK{x1-22002r5}{\csname :autoref\endcsname{minipage}a) PMF and b) CDF of a binomial distribution with $p={2/3}$ and $n=15$. The vertical dashed lines mark the observation ($k=6$). They fall in the black area of the bar chart, and to the left of the dotted line in the CDF, which marks the edge of the rejection region. Therefore, $H_0$ fails the one-sided test.\relax }}{minipage.1}{}}
\newlabel{eq:2sidedbinomtest15}{{\rEfLiNK{x1-22004r6}{\csname :autoref\endcsname{equation}5.6}}{\rEfLiNK{x1-22004r6}{\csname :autoref\endcsname{equation}54}}{\rEfLiNK{x1-22004r6}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{fig:2sidedbinomialrejection15}{{\rEfLiNK{x1-22005r6}{\csname :autoref\endcsname{minipage}5.6}}{\rEfLiNK{x1-22005r6}{\csname :autoref\endcsname{minipage}55}}{\rEfLiNK{x1-22005r6}{\csname :autoref\endcsname{minipage}a) PMF and b) CDF of the binomial distribution with $p=2/3$ and $n=15$. The dotted lines mark the $\alpha /2=0.025$ and $(1-\alpha /2)=0.975$ levels and quantiles. The dashed lines mark the observed value ($k=6$), which falls outside the rejection region. Therefore $H_0$ does not fail the two-sided test.\relax }}{minipage.1}{}}
\newlabel{fig:binomialrejection30}{{\rEfLiNK{x1-22007r7}{\csname :autoref\endcsname{minipage}5.7}}{\rEfLiNK{x1-22007r7}{\csname :autoref\endcsname{minipage}55}}{\rEfLiNK{x1-22007r7}{\csname :autoref\endcsname{minipage}The PMF (a \& b) and CDF (c \& d) for a binomial distribution with $p=2/3$ and $n=30$, illustrating a one-sided (a \& c) and two-sided (b \& d) test. The vertical dashed lines mark the observation $k=12$, which falls inside the rejection regions marked in black (a \& b), and outside the cutoff limits defined by the vertical dotted lines (c \& d).\relax }}{minipage.1}{}}
\newlabel{sec:typeI&II}{{\rEfLiNK{x1-230005.4}{\csname :autoref\endcsname{section}5.4}}{\rEfLiNK{x1-230005.4}{\csname :autoref\endcsname{section}56}}{\rEfLiNK{x1-230005.4}{\csname :autoref\endcsname{section}Type-I and type-II errors}}{section.1}{}}
\newlabel{fig:binompower1}{{\rEfLiNK{x1-23004r8}{\csname :autoref\endcsname{minipage}5.8}}{\rEfLiNK{x1-23004r8}{\csname :autoref\endcsname{minipage}58}}{\rEfLiNK{x1-23004r8}{\csname :autoref\endcsname{minipage}a) -- c) PMFs of the binomial null distribution with $p=2/3$ ($H_0$, grey, background) and alternative distributions ($H_{\!A}$, black and white, foreground) with a) $p=2/5$, b) $p=1/5$ and c) $p=0$. Sample size is $n=5$ for all cases. d) -- f) CDFs for the null distribution (grey) and the alternative distribution (black). The horizontal dotted lines mark $\alpha =0.05$. Their intersections with the CDF of the null distribution are marked by vertical dotted lines. The areas to the left of these lines define the rejection region under a one-sided test and are marked in black in the PMFs of the alternative distributions (a -- c). The dashed horizontal lines mark the intersection of the rejection boundary with the CDF of the alternative distribution. They mark the power of the statistical test ($1-\beta $). Power clearly increases as the alternative distribution drifts away from the null distribution.\relax }}{minipage.1}{}}
\newlabel{fig:binompower2}{{\rEfLiNK{x1-23007r9}{\csname :autoref\endcsname{minipage}5.9}}{\rEfLiNK{x1-23007r9}{\csname :autoref\endcsname{minipage}58}}{\rEfLiNK{x1-23007r9}{\csname :autoref\endcsname{minipage}a) -- c) PMFs of binomial distributions with $p=2/3$ ($H_0$, grey, background) and $p=2/5$ ($H_{\!A}$, black and white, foreground), for sample sizes of a) $n=5$, b) $n=15$ and c) $n=30$. d) -- f) CDFs for the null distribution (grey) and the alternative distribution (black). The horizontal dotted lines mark $\alpha =0.05$, corresponding to a one-sided test. Their intersections with the CDF of the null distribution are marked by vertical dotted lines. The area to the left of these lines define the rejection region and are marked in black in the bar chart. The larger the sample size, the easier it is to reject $H_0$. The dashed horizontal lines mark the intersection of the rejection region with the CDF of the alternative distribution. These mark the power of the statistical test ($1-\beta $). Power clearly increases with sample size.\relax }}{minipage.1}{}}
\newlabel{sec:pitfalls}{{\rEfLiNK{x1-240005.5}{\csname :autoref\endcsname{section}5.5}}{\rEfLiNK{x1-240005.5}{\csname :autoref\endcsname{section}58}}{\rEfLiNK{x1-240005.5}{\csname :autoref\endcsname{section}Pitfalls of statistical hypothesis testing}}{section.1}{}}
\newlabel{fig:binomnvsp}{{\rEfLiNK{x1-24010r10}{\csname :autoref\endcsname{minipage}5.10}}{\rEfLiNK{x1-24010r10}{\csname :autoref\endcsname{minipage}60}}{\rEfLiNK{x1-24010r10}{\csname :autoref\endcsname{minipage}The p-values of a binomial test comparing the null hypothesis $H_0: p=2/3$ with the alternative hypothesis $H_a: p<2/3$, for four parameter estimates $\let \prOteCt \relax \let \prOteCt \relax \Protect \csname acp:c\endcsname {4}{p}$, over a range of sample sizes. For each $\let \prOteCt \relax \let \prOteCt \relax \Protect \csname acp:c\endcsname {4}{p}$, let the outcome $k$ be the closest integer to $n\let \prOteCt \relax \let \prOteCt \relax \Protect \csname acp:c\endcsname {4}{p}$, so that $k/n\approx \let \prOteCt \relax \let \prOteCt \relax \Protect \csname acp:c\endcsname {4}{p}$. The horizontal dotted line marks the 5\% significance level. No matter how little the observed $k/n$-ratio differs from 2/3 $(\approx {0.67})$, this difference always becomes `significant' given a large enough sample.\relax }}{minipage.1}{}}
\newlabel{sec:binomCI}{{\rEfLiNK{x1-250005.6}{\csname :autoref\endcsname{section}5.6}}{\rEfLiNK{x1-250005.6}{\csname :autoref\endcsname{section}61}}{\rEfLiNK{x1-250005.6}{\csname :autoref\endcsname{section}Confidence intervals}}{section.1}{}}
\newlabel{fig:binomcik2n5}{{\rEfLiNK{x1-25007r11}{\csname :autoref\endcsname{minipage}5.11}}{\rEfLiNK{x1-25007r11}{\csname :autoref\endcsname{minipage}62}}{\rEfLiNK{x1-25007r11}{\csname :autoref\endcsname{minipage}a) the PMFs and b) CDFs of the binomial parameter $p$ given $k=2$ successful claims (dashed lines) out of $n=5$ (i.e., $\let \prOteCt \relax \let \prOteCt \relax \Protect \csname acp:c\endcsname {4}{p}=0.4$), for the lower ($p=0.053$, grey) and upper ($p=0.85$, black) limits of a 95\% confidence interval. Dotted horizontal lines mark the 0.025 and 0.975 confidence levels.\vspace  \medskipamount \relax }}{minipage.1}{}}
\newlabel{fig:binomcik4n5}{{\rEfLiNK{x1-25009r12}{\csname :autoref\endcsname{minipage}5.12}}{\rEfLiNK{x1-25009r12}{\csname :autoref\endcsname{minipage}63}}{\rEfLiNK{x1-25009r12}{\csname :autoref\endcsname{minipage}The same as Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:binomcik2n5}, but for $k=4$ successful claims (dashed lines) out of $n=5$ trials (i.e., $\let \prOteCt \relax \let \prOteCt \relax \Protect \csname acp:c\endcsname {4}{p}=0.8$).\vspace  \medskipamount \relax }}{minipage.1}{}}
\newlabel{fig:binomcik12n30}{{\rEfLiNK{x1-25011r13}{\csname :autoref\endcsname{minipage}5.13}}{\rEfLiNK{x1-25011r13}{\csname :autoref\endcsname{minipage}63}}{\rEfLiNK{x1-25011r13}{\csname :autoref\endcsname{minipage}The same as Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:binomcik2n5}, but for $k=12$ successful claims (dashed lines) out of $n=30$ trials (i.e., $\let \prOteCt \relax \let \prOteCt \relax \Protect \csname acp:c\endcsname {4}{p}=0.4$).\vspace  \medskipamount \relax }}{minipage.1}{}}
\newlabel{fig:binomcivsn}{{\rEfLiNK{x1-25013r14}{\csname :autoref\endcsname{minipage}5.14}}{\rEfLiNK{x1-25013r14}{\csname :autoref\endcsname{minipage}64}}{\rEfLiNK{x1-25013r14}{\csname :autoref\endcsname{minipage}95\% confidence intervals for a) $\let \prOteCt \relax \let \prOteCt \relax \Protect \csname acp:c\endcsname {4}{p}=2/3$ and b) $\let \prOteCt \relax \let \prOteCt \relax \Protect \csname acp:c\endcsname {4}{p}=1/5$ for different sample sizes $n$. The horizontal dotted lines mark the maximum likelihood estimates.\relax }}{minipage.1}{}}
\newlabel{eq:binormci}{{\rEfLiNK{x1-25015r8}{\csname :autoref\endcsname{equation}5.8}}{\rEfLiNK{x1-25015r8}{\csname :autoref\endcsname{equation}64}}{\rEfLiNK{x1-25015r8}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:sigmabinom}{{\rEfLiNK{x1-25016r9}{\csname :autoref\endcsname{equation}5.9}}{\rEfLiNK{x1-25016r9}{\csname :autoref\endcsname{equation}64}}{\rEfLiNK{x1-25016r9}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:poisson}{{\rEfLiNK{x1-260006}{\csname :autoref\endcsname{chapter}6}}{\rEfLiNK{x1-260006}{\csname :autoref\endcsname{chapter}67}}{\rEfLiNK{x1-260006}{\csname :autoref\endcsname{chapter}The Poisson distribution}}{chapter.1}{}}
\newlabel{fig:declusteredquakes}{{\rEfLiNK{x1-26002r1}{\csname :autoref\endcsname{document}6.1}}{\rEfLiNK{x1-26002r1}{\csname :autoref\endcsname{document}67}}{\rEfLiNK{x1-26002r1}{\csname :autoref\endcsname{document}The number of US earthquakes of magnitude 5.0 or greater per year between 1917 and 2016, with aftershocks removed.\vspace  \medskipamount \relax }}{document.1}{}}
\newlabel{fig:declusteredquakesperyear}{{\rEfLiNK{x1-26004r2}{\csname :autoref\endcsname{minipage}6.2}}{\rEfLiNK{x1-26004r2}{\csname :autoref\endcsname{minipage}67}}{\rEfLiNK{x1-26004r2}{\csname :autoref\endcsname{minipage} Histogram of the earthquake counts shown in Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:declusteredquakes}.\vspace  \medskipamount \leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\\ \textbf  {mean: 5.43}\\ standard deviation: 2.50\\ \textbf  {variance: 6.24} \relax }}{minipage.1}{}}
\newlabel{fig:zircons}{{\rEfLiNK{x1-26006r3}{\csname :autoref\endcsname{document}6.3}}{\rEfLiNK{x1-26006r3}{\csname :autoref\endcsname{document}67}}{\rEfLiNK{x1-26006r3}{\csname :autoref\endcsname{document}Point-counting results for zircon in sand. Black squares mark zircons and grey circles other minerals.\vspace  \medskipamount \relax }}{document.1}{}}
\newlabel{fig:zirconcounts}{{\rEfLiNK{x1-26008r4}{\csname :autoref\endcsname{document}6.4}}{\rEfLiNK{x1-26008r4}{\csname :autoref\endcsname{document}67}}{\rEfLiNK{x1-26008r4}{\csname :autoref\endcsname{document}The number of zircons counted in each square of Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:zircons}.\vspace  \medskipamount \relax }}{document.1}{}}
\newlabel{fig:zirconhist}{{\rEfLiNK{x1-26010r5}{\csname :autoref\endcsname{minipage}6.5}}{\rEfLiNK{x1-26010r5}{\csname :autoref\endcsname{minipage}68}}{\rEfLiNK{x1-26010r5}{\csname :autoref\endcsname{minipage} Histogram of the zircon counts shown in Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:zirconcounts}.\vspace  \medskipamount \leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\\ \textbf  {mean: 3.50}\\ standard deviation: 1.85\\ \textbf  {variance: 3.40} \relax }}{minipage.1}{}}
\newlabel{sec:PMF}{{\rEfLiNK{x1-270006.1}{\csname :autoref\endcsname{section}6.1}}{\rEfLiNK{x1-270006.1}{\csname :autoref\endcsname{section}68}}{\rEfLiNK{x1-270006.1}{\csname :autoref\endcsname{section}Probability mass function}}{section.1}{}}
\newlabel{eq:poispmf}{{\rEfLiNK{x1-27005r1}{\csname :autoref\endcsname{equation}6.1}}{\rEfLiNK{x1-27005r1}{\csname :autoref\endcsname{equation}68}}{\rEfLiNK{x1-27005r1}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{sec:poispar}{{\rEfLiNK{x1-280006.2}{\csname :autoref\endcsname{section}6.2}}{\rEfLiNK{x1-280006.2}{\csname :autoref\endcsname{section}69}}{\rEfLiNK{x1-280006.2}{\csname :autoref\endcsname{section}Parameter estimation}}{section.1}{}}
\newlabel{eq:poislik}{{\rEfLiNK{x1-28001r2}{\csname :autoref\endcsname{equation}6.2}}{\rEfLiNK{x1-28001r2}{\csname :autoref\endcsname{equation}70}}{\rEfLiNK{x1-28001r2}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:poisLL}{{\rEfLiNK{x1-28003r4}{\csname :autoref\endcsname{equation}6.4}}{\rEfLiNK{x1-28003r4}{\csname :autoref\endcsname{equation}70}}{\rEfLiNK{x1-28003r4}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:lambda=k}{{\rEfLiNK{x1-28007r8}{\csname :autoref\endcsname{equation}6.8}}{\rEfLiNK{x1-28007r8}{\csname :autoref\endcsname{equation}72}}{\rEfLiNK{x1-28007r8}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{sec:poishyp}{{\rEfLiNK{x1-290006.3}{\csname :autoref\endcsname{section}6.3}}{\rEfLiNK{x1-290006.3}{\csname :autoref\endcsname{section}72}}{\rEfLiNK{x1-290006.3}{\csname :autoref\endcsname{section}Hypothesis tests}}{section.1}{}}
\newlabel{it:poisl351sided}{{\rEfLiNK{x1-290066}{\csname :autoref\endcsname{section}6}}{\rEfLiNK{x1-290066}{\csname :autoref\endcsname{section}73}}{\rEfLiNK{x1-290066}{\csname :autoref\endcsname{section}Hypothesis tests}}{section.1}{}}
\newlabel{fig:poishyp}{{\rEfLiNK{x1-29007r7}{\csname :autoref\endcsname{minipage}6.7}}{\rEfLiNK{x1-29007r7}{\csname :autoref\endcsname{minipage}73}}{\rEfLiNK{x1-29007r7}{\csname :autoref\endcsname{minipage}a) PMF and b) CDF of a Poissonian null distribution with $\lambda =3.5$. The rejection region is marked in black on a). The horizontal dotted line in b) shows the $1-\alpha =0.95$ mark. The horizontal dashed line in the CDF marks the cumulative probability for $k=9$, which is greater than the 0.95 cutoff. Therefore the one-sided null hypothesis is rejected.\relax }}{minipage.1}{}}
\newlabel{sec:multipletesting}{{\rEfLiNK{x1-300006.4}{\csname :autoref\endcsname{section}6.4}}{\rEfLiNK{x1-300006.4}{\csname :autoref\endcsname{section}73}}{\rEfLiNK{x1-300006.4}{\csname :autoref\endcsname{section}Multiple testing}}{section.1}{}}
\newlabel{sec:poisCI}{{\rEfLiNK{x1-310006.5}{\csname :autoref\endcsname{section}6.5}}{\rEfLiNK{x1-310006.5}{\csname :autoref\endcsname{section}74}}{\rEfLiNK{x1-310006.5}{\csname :autoref\endcsname{section}Confidence intervals}}{section.1}{}}
\newlabel{fig:poisci}{{\rEfLiNK{x1-31001r8}{\csname :autoref\endcsname{minipage}6.8}}{\rEfLiNK{x1-31001r8}{\csname :autoref\endcsname{minipage}74}}{\rEfLiNK{x1-31001r8}{\csname :autoref\endcsname{minipage}95\% confidence interval for the Poisson parameter $\lambda $ given a single observation of $k=5$ events. The lower ($p=1.62$, grey) and upper ($p=11.67$) limits of the confidence interval are shown as a) PMFs and b) CDFs. Dotted horizontal lines mark the 0.025 and 0.975 confidence levels.\relax }}{minipage.1}{}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:normal}{{\rEfLiNK{x1-320007}{\csname :autoref\endcsname{chapter}7}}{\rEfLiNK{x1-320007}{\csname :autoref\endcsname{chapter}77}}{\rEfLiNK{x1-320007}{\csname :autoref\endcsname{chapter}The normal distribution}}{chapter.1}{}}
\newlabel{sec:CLT}{{\rEfLiNK{x1-330007.1}{\csname :autoref\endcsname{section}7.1}}{\rEfLiNK{x1-330007.1}{\csname :autoref\endcsname{section}79}}{\rEfLiNK{x1-330007.1}{\csname :autoref\endcsname{section}The Central Limit Theorem}}{section.1}{}}
\newlabel{fig:CLTfaithful1}{{\rEfLiNK{x1-33001r2}{\csname :autoref\endcsname{minipage}7.2}}{\rEfLiNK{x1-33001r2}{\csname :autoref\endcsname{minipage}80}}{\rEfLiNK{x1-33001r2}{\csname :autoref\endcsname{minipage}The KDE and rug plot of 272 Old Faithful eruption durations is the marginal distribution of Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:KDE2D}. This distribution has two modes at 2 and 4.5 minutes.\relax }}{minipage.1}{}}
\newlabel{fig:CLTfaithful2}{{\rEfLiNK{x1-33003r3}{\csname :autoref\endcsname{minipage}7.3}}{\rEfLiNK{x1-33003r3}{\csname :autoref\endcsname{minipage}80}}{\rEfLiNK{x1-33003r3}{\csname :autoref\endcsname{minipage}Collect $n=2$ randomly chosen events from the original dasaset of geyser eruptions with replacement and add their durations together. Repeat to create a new dataset of $N=500$ values. The KDE of this distribution has not two but three modes at 4 ($=2\times {2}$), 6.5 ($=2+4.5$), and 9 ($=2\times {4.5}$) minutes, respectively.\relax }}{minipage.1}{}}
\newlabel{fig:CLTfaithful3}{{\rEfLiNK{x1-33005r4}{\csname :autoref\endcsname{minipage}7.4}}{\rEfLiNK{x1-33005r4}{\csname :autoref\endcsname{minipage}80}}{\rEfLiNK{x1-33005r4}{\csname :autoref\endcsname{minipage}Collect $n=3$ randomly chosen events from the geyser eruption dataset and add their durations together. Repeat $N=500$ times to create a third dataset. The KDE of this distribution has four visible modes, including peaks at 6 ($=3\times {2}$), 8.5 ($=2\times {2}+4.5$), 11 ($=2+2\times {4.5}$) and 13.5 ($=3\times {4.5}$) minutes. \relax }}{minipage.1}{}}
\newlabel{fig:CLTfaithful10}{{\rEfLiNK{x1-33007r5}{\csname :autoref\endcsname{minipage}7.5}}{\rEfLiNK{x1-33007r5}{\csname :autoref\endcsname{minipage}80}}{\rEfLiNK{x1-33007r5}{\csname :autoref\endcsname{minipage}Taking $N=500$ samples of $n=10$ randomly selected eruptions and summing their durations produces a fourth dataset whose KDE has a single mode with symmetric tails towards lower and higher values.\relax }}{minipage.1}{}}
\newlabel{eq:gauss}{{\rEfLiNK{x1-33009r6}{\csname :autoref\endcsname{equation}7.6}}{\rEfLiNK{x1-33009r6}{\csname :autoref\endcsname{equation}81}}{\rEfLiNK{x1-33009r6}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{fig:galtonsbeanmachine}{{\rEfLiNK{x1-33010r6}{\csname :autoref\endcsname{minipage}7.6}}{\rEfLiNK{x1-33010r6}{\csname :autoref\endcsname{minipage}81}}{\rEfLiNK{x1-33010r6}{\csname :autoref\endcsname{minipage}Galton's bean machine is a mechanical device that simulates additive physical processes. It consists of a triangular arrangement of pegs above a linear array of containers. When a bead enters the machine from the top, it bounces off the pegs on its way down to the containers. The probability of bouncing to the left is the same as the probability of bouncing to the right. After $n$ bounces, the bead lands in one of the containers, forming a bell shaped (binomial) distribution. With increasing $n$, this distribution converges to a Gaussian form.\relax }}{minipage.1}{}}
\newlabel{sec:multinorm}{{\rEfLiNK{x1-340007.2}{\csname :autoref\endcsname{section}7.2}}{\rEfLiNK{x1-340007.2}{\csname :autoref\endcsname{section}81}}{\rEfLiNK{x1-340007.2}{\csname :autoref\endcsname{section}The multivariate normal distribution}}{section.1}{}}
\newlabel{fig:pop2d}{{\rEfLiNK{x1-34001r7}{\csname :autoref\endcsname{document}7.7}}{\rEfLiNK{x1-34001r7}{\csname :autoref\endcsname{document}82}}{\rEfLiNK{x1-34001r7}{\csname :autoref\endcsname{document}Four synthetic bivariate continuous distributions, defined by black areas and lines. White areas are excluded from the distributions.\vspace  \medskipamount \relax }}{document.1}{}}
\newlabel{fig:rand2d}{{\rEfLiNK{x1-34003r8}{\csname :autoref\endcsname{document}7.8}}{\rEfLiNK{x1-34003r8}{\csname :autoref\endcsname{document}82}}{\rEfLiNK{x1-34003r8}{\csname :autoref\endcsname{document}It is easy to recognise the probability distributions of Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:pop2d} in the scatter plots of $n=100$ random points selected from them.\vspace  \medskipamount \relax }}{document.1}{}}
\newlabel{fig:rand2sum}{{\rEfLiNK{x1-34005r9}{\csname :autoref\endcsname{document}7.9}}{\rEfLiNK{x1-34005r9}{\csname :autoref\endcsname{document}82}}{\rEfLiNK{x1-34005r9}{\csname :autoref\endcsname{document}Four scatter plots with $N=200$ points, each of which represents the sum of a random sample of $n=100$ points drawn from Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:pop2d}.\vspace  \medskipamount \relax }}{document.1}{}}
\newlabel{eq:2dgauss}{{\rEfLiNK{x1-34007r7}{\csname :autoref\endcsname{equation}7.7}}{\rEfLiNK{x1-34007r7}{\csname :autoref\endcsname{equation}82}}{\rEfLiNK{x1-34007r7}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{fig:norm2dmarginal}{{\rEfLiNK{x1-34008r10}{\csname :autoref\endcsname{minipage}7.10}}{\rEfLiNK{x1-34008r10}{\csname :autoref\endcsname{minipage}83}}{\rEfLiNK{x1-34008r10}{\csname :autoref\endcsname{minipage}The main panel shows population 4 of Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:rand2sum} as grey circles, and the best fitting bivariate Gaussian distribution as contours. The side panels show the marginal distributions of the $X$- and $Y$-variable, which are both univariate Gaussian. The means and standard deviations of the marginal distributions equal the means and the standard deviations of the bivariate distribution. The bivariate distribution has a fifth parameter, the covariance $\sigma _{x,y}$, which controls the angle at which the elliptical contours are rotated relative to the axes of the diagram. The significance of these parameters is further explored in Section\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {sec:normalproperties}.\relax }}{minipage.1}{}}
\newlabel{sec:normalproperties}{{\rEfLiNK{x1-350007.3}{\csname :autoref\endcsname{section}7.3}}{\rEfLiNK{x1-350007.3}{\csname :autoref\endcsname{section}83}}{\rEfLiNK{x1-350007.3}{\csname :autoref\endcsname{section}Properties}}{section.1}{}}
\newlabel{fig:musigma}{{\rEfLiNK{x1-35003r11}{\csname :autoref\endcsname{document}7.11}}{\rEfLiNK{x1-35003r11}{\csname :autoref\endcsname{document}83}}{\rEfLiNK{x1-35003r11}{\csname :autoref\endcsname{document}PDFs of the univariate normal distribution for different values of $\mu $ and $\sigma $. $\mu $ controls the position and $\sigma $ the width of the distribution. By definition, the area under the PDF always remains the same (i.e. $\relax {\DOTSI \intop \ilimits@ }\nolimits _{-\infty }^{+\infty }f(x)\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}dx = 1$).\vspace  \medskipamount \relax }}{document.1}{}}
\newlabel{fig:2sigma}{{\rEfLiNK{x1-35005r12}{\csname :autoref\endcsname{document}7.12}}{\rEfLiNK{x1-35005r12}{\csname :autoref\endcsname{document}83}}{\rEfLiNK{x1-35005r12}{\csname :autoref\endcsname{document}PDF (a) and CDF (b) of the normal distribution. The $\mu \pm \sigma $ and $\mu \pm {2}\sigma $ intervals cover $\sim $68\% and $\sim $95\% of the distribution, respectively. \relax }}{document.1}{}}
\newlabel{fig:cov}{{\rEfLiNK{x1-35008r13}{\csname :autoref\endcsname{document}7.13}}{\rEfLiNK{x1-35008r13}{\csname :autoref\endcsname{document}84}}{\rEfLiNK{x1-35008r13}{\csname :autoref\endcsname{document}The standard deviations $\sigma _x$ and $\sigma _y$, and covariance $\sigma _{x,y}$ control the shape and dispersion of the bivariate normal distribution.\vspace  \medskipamount \relax }}{document.1}{}}
\newlabel{sec:normalparameters}{{\rEfLiNK{x1-360007.4}{\csname :autoref\endcsname{section}7.4}}{\rEfLiNK{x1-360007.4}{\csname :autoref\endcsname{section}84}}{\rEfLiNK{x1-360007.4}{\csname :autoref\endcsname{section}Parameter estimation}}{section.1}{}}
\newlabel{eq:Lnorm}{{\rEfLiNK{x1-36001r8}{\csname :autoref\endcsname{equation}7.8}}{\rEfLiNK{x1-36001r8}{\csname :autoref\endcsname{equation}84}}{\rEfLiNK{x1-36001r8}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:LLnorm}{{\rEfLiNK{x1-36002r9}{\csname :autoref\endcsname{equation}7.9}}{\rEfLiNK{x1-36002r9}{\csname :autoref\endcsname{equation}85}}{\rEfLiNK{x1-36002r9}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:stdevgivenmu}{{\rEfLiNK{x1-36004r11}{\csname :autoref\endcsname{equation}7.11}}{\rEfLiNK{x1-36004r11}{\csname :autoref\endcsname{equation}86}}{\rEfLiNK{x1-36004r11}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:stdevrepeat}{{\rEfLiNK{x1-36005r12}{\csname :autoref\endcsname{equation}7.12}}{\rEfLiNK{x1-36005r12}{\csname :autoref\endcsname{equation}86}}{\rEfLiNK{x1-36005r12}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:sxy}{{\rEfLiNK{x1-36009r14}{\csname :autoref\endcsname{equation}7.14}}{\rEfLiNK{x1-36009r14}{\csname :autoref\endcsname{equation}87}}{\rEfLiNK{x1-36009r14}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:errorprop}{{\rEfLiNK{x1-370008}{\csname :autoref\endcsname{chapter}8}}{\rEfLiNK{x1-370008}{\csname :autoref\endcsname{chapter}89}}{\rEfLiNK{x1-370008}{\csname :autoref\endcsname{chapter}Error estimation}}{chapter.1}{}}
\newlabel{fig:darts}{{\rEfLiNK{x1-37001r1}{\csname :autoref\endcsname{document}8.1}}{\rEfLiNK{x1-37001r1}{\csname :autoref\endcsname{document}89}}{\rEfLiNK{x1-37001r1}{\csname :autoref\endcsname{document}Darts board illustration of accuracy (closeness to the centre) and precision (closeness of the measurements). The best case scenario combines high precision with high accuracy. The worst case scenario combines high precision with low accuracy. This is the worst possible situation because the high precision gives false confidence in the data.\vspace  \medskipamount \relax }}{document.1}{}}
\newlabel{sec:linearerrorprop}{{\rEfLiNK{x1-380008.1}{\csname :autoref\endcsname{section}8.1}}{\rEfLiNK{x1-380008.1}{\csname :autoref\endcsname{section}89}}{\rEfLiNK{x1-380008.1}{\csname :autoref\endcsname{section}Error propagation}}{section.1}{}}
\newlabel{eq:zgx}{{\rEfLiNK{x1-38001r1}{\csname :autoref\endcsname{equation}8.1}}{\rEfLiNK{x1-38001r1}{\csname :autoref\endcsname{equation}89}}{\rEfLiNK{x1-38001r1}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:varz}{{\rEfLiNK{x1-38002r2}{\csname :autoref\endcsname{equation}8.2}}{\rEfLiNK{x1-38002r2}{\csname :autoref\endcsname{equation}90}}{\rEfLiNK{x1-38002r2}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:zi-z}{{\rEfLiNK{x1-38004r3}{\csname :autoref\endcsname{equation}8.3}}{\rEfLiNK{x1-38004r3}{\csname :autoref\endcsname{equation}90}}{\rEfLiNK{x1-38004r3}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{fig:errorprop2d}{{\rEfLiNK{x1-38005r2}{\csname :autoref\endcsname{minipage}8.2}}{\rEfLiNK{x1-38005r2}{\csname :autoref\endcsname{minipage}91}}{\rEfLiNK{x1-38005r2}{\csname :autoref\endcsname{minipage}Error propagation of two linear functions, $g_1$ (black line) and $g_2$ (grey line). The input data $\{x_i\}$ are normally distributed around some average value $\let \prOteCt \relax \let \prOteCt \relax \Protect \csname acp:c\endcsname {18}{x}$. Error propagation estimates the dispersion of the inferred quantity $z_i$ from the dispersion of the measurements $x_i$. Deviations $(x_i-\let \prOteCt \relax \let \prOteCt \relax \Protect \csname acp:c\endcsname {18}{x})$ from the average $x$-value are reduced (for function $g_1$) or magnified (for function $g_2$) depending on the slope of the functions, resulting in deviations of the dependent variable that are smaller $(z_{i1}-\let \prOteCt \relax \let \prOteCt \relax \Protect \csname acp:c\endcsname {18}{z}_1)$ or greater $(z_{i2}-\let \prOteCt \relax \let \prOteCt \relax \Protect \csname acp:c\endcsname {18}{z}_i)$ than $(x_i-\let \prOteCt \relax \let \prOteCt \relax \Protect \csname acp:c\endcsname {18}{x})$. \relax }}{minipage.1}{}}
\newlabel{eq:errorprop1d}{{\rEfLiNK{x1-38007r4}{\csname :autoref\endcsname{equation}8.4}}{\rEfLiNK{x1-38007r4}{\csname :autoref\endcsname{equation}91}}{\rEfLiNK{x1-38007r4}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:zfxy}{{\rEfLiNK{x1-38008r5}{\csname :autoref\endcsname{equation}8.5}}{\rEfLiNK{x1-38008r5}{\csname :autoref\endcsname{equation}92}}{\rEfLiNK{x1-38008r5}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:dg}{{\rEfLiNK{x1-38009r6}{\csname :autoref\endcsname{equation}8.6}}{\rEfLiNK{x1-38009r6}{\csname :autoref\endcsname{equation}92}}{\rEfLiNK{x1-38009r6}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:errorprop2d}{{\rEfLiNK{x1-38011r8}{\csname :autoref\endcsname{equation}8.8}}{\rEfLiNK{x1-38011r8}{\csname :autoref\endcsname{equation}93}}{\rEfLiNK{x1-38011r8}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:errorprop2dmatrix}{{\rEfLiNK{x1-38012r9}{\csname :autoref\endcsname{equation}8.9}}{\rEfLiNK{x1-38012r9}{\csname :autoref\endcsname{equation}93}}{\rEfLiNK{x1-38012r9}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{fig:errorprop3d}{{\rEfLiNK{x1-38013r3}{\csname :autoref\endcsname{minipage}8.3}}{\rEfLiNK{x1-38013r3}{\csname :autoref\endcsname{minipage}94}}{\rEfLiNK{x1-38013r3}{\csname :autoref\endcsname{minipage}Error propagation of a bivariate function $z = g(x,y)$. The uncertainty in $z$ is \emph  {approximately} proportional to the slope of the surface $g$ w.r.t. the measurements $x$ and $y$. If the curvature of the 3D surface is minor relative to the uncertainties, then said surface can be \emph  {locally} approximated by a plane. Using such a linear approximation, the size of the error ellipses ($s[x]$, $s[y]$) around the mean measurements (black dots) linearly scales with corresponding deviations in the inferred quantity ($s[z]$).\relax }}{minipage.1}{}}
\newlabel{sec:errorpropexamples}{{\rEfLiNK{x1-390008.2}{\csname :autoref\endcsname{section}8.2}}{\rEfLiNK{x1-390008.2}{\csname :autoref\endcsname{section}94}}{\rEfLiNK{x1-390008.2}{\csname :autoref\endcsname{section}Examples}}{section.1}{}}
\newlabel{eq:addition}{{\rEfLiNK{x1-39002r10}{\csname :autoref\endcsname{equation}8.10}}{\rEfLiNK{x1-39002r10}{\csname :autoref\endcsname{equation}95}}{\rEfLiNK{x1-39002r10}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:subtraction}{{\rEfLiNK{x1-39004r11}{\csname :autoref\endcsname{equation}8.11}}{\rEfLiNK{x1-39004r11}{\csname :autoref\endcsname{equation}96}}{\rEfLiNK{x1-39004r11}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:multiplication}{{\rEfLiNK{x1-39006r12}{\csname :autoref\endcsname{equation}8.12}}{\rEfLiNK{x1-39006r12}{\csname :autoref\endcsname{equation}98}}{\rEfLiNK{x1-39006r12}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:division}{{\rEfLiNK{x1-39008r13}{\csname :autoref\endcsname{equation}8.13}}{\rEfLiNK{x1-39008r13}{\csname :autoref\endcsname{equation}99}}{\rEfLiNK{x1-39008r13}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:exponentiation}{{\rEfLiNK{x1-39010r14}{\csname :autoref\endcsname{equation}8.14}}{\rEfLiNK{x1-39010r14}{\csname :autoref\endcsname{equation}101}}{\rEfLiNK{x1-39010r14}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:logarithms}{{\rEfLiNK{x1-39012r15}{\csname :autoref\endcsname{equation}8.15}}{\rEfLiNK{x1-39012r15}{\csname :autoref\endcsname{equation}102}}{\rEfLiNK{x1-39012r15}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:power}{{\rEfLiNK{x1-39014r16}{\csname :autoref\endcsname{equation}8.16}}{\rEfLiNK{x1-39014r16}{\csname :autoref\endcsname{equation}103}}{\rEfLiNK{x1-39014r16}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:newtons2nd}{{\rEfLiNK{x1-39016r17}{\csname :autoref\endcsname{equation}8.17}}{\rEfLiNK{x1-39016r17}{\csname :autoref\endcsname{equation}103}}{\rEfLiNK{x1-39016r17}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:xvt}{{\rEfLiNK{x1-39017r18}{\csname :autoref\endcsname{equation}8.18}}{\rEfLiNK{x1-39017r18}{\csname :autoref\endcsname{equation}104}}{\rEfLiNK{x1-39017r18}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:ygt2}{{\rEfLiNK{x1-39018r19}{\csname :autoref\endcsname{equation}8.19}}{\rEfLiNK{x1-39018r19}{\csname :autoref\endcsname{equation}104}}{\rEfLiNK{x1-39018r19}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:sx}{{\rEfLiNK{x1-39019r20}{\csname :autoref\endcsname{equation}8.20}}{\rEfLiNK{x1-39019r20}{\csname :autoref\endcsname{equation}105}}{\rEfLiNK{x1-39019r20}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:sy}{{\rEfLiNK{x1-39020r21}{\csname :autoref\endcsname{equation}8.21}}{\rEfLiNK{x1-39020r21}{\csname :autoref\endcsname{equation}106}}{\rEfLiNK{x1-39020r21}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:sdnewton}{{\rEfLiNK{x1-39021r22}{\csname :autoref\endcsname{equation}8.22}}{\rEfLiNK{x1-39021r22}{\csname :autoref\endcsname{equation}107}}{\rEfLiNK{x1-39021r22}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:snowy}{{\rEfLiNK{x1-39023r23}{\csname :autoref\endcsname{equation}8.23}}{\rEfLiNK{x1-39023r23}{\csname :autoref\endcsname{equation}107}}{\rEfLiNK{x1-39023r23}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:haddock}{{\rEfLiNK{x1-39025r24}{\csname :autoref\endcsname{equation}8.24}}{\rEfLiNK{x1-39025r24}{\csname :autoref\endcsname{equation}108}}{\rEfLiNK{x1-39025r24}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{sec:stderr}{{\rEfLiNK{x1-400008.3}{\csname :autoref\endcsname{section}8.3}}{\rEfLiNK{x1-400008.3}{\csname :autoref\endcsname{section}108}}{\rEfLiNK{x1-400008.3}{\csname :autoref\endcsname{section}Standard deviation vs. standard error}}{section.1}{}}
\newlabel{eq:stderrmean}{{\rEfLiNK{x1-40001r25}{\csname :autoref\endcsname{equation}8.25}}{\rEfLiNK{x1-40001r25}{\csname :autoref\endcsname{equation}109}}{\rEfLiNK{x1-40001r25}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{sec:FisherInformation}{{\rEfLiNK{x1-410008.4}{\csname :autoref\endcsname{section}8.4}}{\rEfLiNK{x1-410008.4}{\csname :autoref\endcsname{section}110}}{\rEfLiNK{x1-410008.4}{\csname :autoref\endcsname{section}Fisher Information}}{section.1}{}}
\newlabel{eq:Fisher}{{\rEfLiNK{x1-41001r26}{\csname :autoref\endcsname{equation}8.26}}{\rEfLiNK{x1-41001r26}{\csname :autoref\endcsname{equation}111}}{\rEfLiNK{x1-41001r26}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:multidimFisher}{{\rEfLiNK{x1-41002r27}{\csname :autoref\endcsname{equation}8.27}}{\rEfLiNK{x1-41002r27}{\csname :autoref\endcsname{equation}111}}{\rEfLiNK{x1-41002r27}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:d2LLdl2}{{\rEfLiNK{x1-41003r28}{\csname :autoref\endcsname{equation}8.28}}{\rEfLiNK{x1-41003r28}{\csname :autoref\endcsname{equation}112}}{\rEfLiNK{x1-41003r28}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:poisvar}{{\rEfLiNK{x1-41004r29}{\csname :autoref\endcsname{equation}8.29}}{\rEfLiNK{x1-41004r29}{\csname :autoref\endcsname{equation}113}}{\rEfLiNK{x1-41004r29}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:comparingdistributions}{{\rEfLiNK{x1-420009}{\csname :autoref\endcsname{chapter}9}}{\rEfLiNK{x1-420009}{\csname :autoref\endcsname{chapter}115}}{\rEfLiNK{x1-420009}{\csname :autoref\endcsname{chapter}Comparing distributions}}{chapter.1}{}}
\newlabel{sec:q-q}{{\rEfLiNK{x1-430009.1}{\csname :autoref\endcsname{section}9.1}}{\rEfLiNK{x1-430009.1}{\csname :autoref\endcsname{section}115}}{\rEfLiNK{x1-430009.1}{\csname :autoref\endcsname{section}Q-Q plots}}{section.1}{}}
\newlabel{fig:qqfaithful1}{{\rEfLiNK{x1-43001r1}{\csname :autoref\endcsname{minipage}9.1}}{\rEfLiNK{x1-43001r1}{\csname :autoref\endcsname{minipage}115}}{\rEfLiNK{x1-43001r1}{\csname :autoref\endcsname{minipage}Quantile-quantile (Q-Q) plot of Old Faithful eruption durations. The horizontal axis marks the theoretical quantiles of a normal distribution with the same mean and standard deviation as the data. The vertical axis marks the quantiles of the actual data. If the two distributions being compared are similar, the points in the Q-Q plot will approximately lie on the line y = x. Otherwise they will not. In this example the distribution of eruption durations clearly does not follow a normal distribution.\relax }}{minipage.1}{}}
\newlabel{fig:qqfaithful2}{{\rEfLiNK{x1-43003r2}{\csname :autoref\endcsname{minipage}9.2}}{\rEfLiNK{x1-43003r2}{\csname :autoref\endcsname{minipage}115}}{\rEfLiNK{x1-43003r2}{\csname :autoref\endcsname{minipage}Q-Q plot of the dataset of 500 sums of $n = 2$ randomly selected eruption durations shown in Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:CLTfaithful2}. The resulting trimodal distribution plots closer to the 1:1 line than the original dataset of Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:qqfaithful1} but is still far from normal.\relax }}{minipage.1}{}}
\newlabel{fig:qqfaithful10}{{\rEfLiNK{x1-43005r3}{\csname :autoref\endcsname{minipage}9.3}}{\rEfLiNK{x1-43005r3}{\csname :autoref\endcsname{minipage}116}}{\rEfLiNK{x1-43005r3}{\csname :autoref\endcsname{minipage}Q-Q plot of the dataset of 500 sums of $n = 10$ randomly selected eruption durations shown in Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:CLTfaithful10}. The data plot very close to the 1:1 line, visually confirming that they follow a normal distribution. The sample distribution only deviates from the theoretical distribution at the most extreme quantiles. This indicates that the sample distribution has heavier tails than the normal distribution. This phenomenon will be discussed further in the next section. Increasing $n$ further would remove this effect near the tails and bring the sample distribution even closer to the normal distribution.\relax }}{minipage.1}{}}
\newlabel{fig:qqfaithful12}{{\rEfLiNK{x1-43007r4}{\csname :autoref\endcsname{minipage}9.4}}{\rEfLiNK{x1-43007r4}{\csname :autoref\endcsname{minipage}116}}{\rEfLiNK{x1-43007r4}{\csname :autoref\endcsname{minipage} Q-Q plot comparing datasets $x_1=\unhbox \voidb@x \hbox {\a:mbox {sum}\b:mbox }(X_1)$ and $x_2=\unhbox \voidb@x \hbox {\a:mbox {sum}\b:mbox }(X_2)$ of Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:rand2sum}. Even though the two datasets have markedly different means ($\let \prOteCt \relax \let \prOteCt \relax \Protect \csname acp:c\endcsname {18}{x}_1=50.0$ and $\let \prOteCt \relax \let \prOteCt \relax \Protect \csname acp:c\endcsname {18}{x}_2=59.9$) and slightly different standard deviations ($s[x_1]=3.3$ and $s[x_2]=3.1$), the quantiles of the two datasets plot along a straight line. This means that their distributions are identical in shape. \relax }}{minipage.1}{}}
\newlabel{sec:t}{{\rEfLiNK{x1-440009.2}{\csname :autoref\endcsname{section}9.2}}{\rEfLiNK{x1-440009.2}{\csname :autoref\endcsname{section}116}}{\rEfLiNK{x1-440009.2}{\csname :autoref\endcsname{section}The t-test}}{section.1}{}}
\newlabel{tab:coins}{{\rEfLiNK{x1-440009.2}{\csname :autoref\endcsname{section}9.2}}{\rEfLiNK{x1-440009.2}{\csname :autoref\endcsname{section}116}}{\rEfLiNK{x1-440009.2}{\csname :autoref\endcsname{section}The t-test}}{section.1}{}}
\newlabel{eq:z}{{\rEfLiNK{x1-44001r1}{\csname :autoref\endcsname{equation}9.1}}{\rEfLiNK{x1-44001r1}{\csname :autoref\endcsname{equation}117}}{\rEfLiNK{x1-44001r1}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:t}{{\rEfLiNK{x1-44002r2}{\csname :autoref\endcsname{equation}9.2}}{\rEfLiNK{x1-44002r2}{\csname :autoref\endcsname{equation}117}}{\rEfLiNK{x1-44002r2}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:t1}{{\rEfLiNK{x1-44005r3}{\csname :autoref\endcsname{equation}9.3}}{\rEfLiNK{x1-44005r3}{\csname :autoref\endcsname{equation}118}}{\rEfLiNK{x1-44005r3}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{fig:1samplettest}{{\rEfLiNK{x1-44010r5}{\csname :autoref\endcsname{minipage}9.5}}{\rEfLiNK{x1-44010r5}{\csname :autoref\endcsname{minipage}119}}{\rEfLiNK{x1-44010r5}{\csname :autoref\endcsname{minipage}a) PDF and b) CDF of a t-distribution with 4 degrees of freedom. The one-sided rejection region (for $\alpha =0.05$) is marked in black. The vertical dashed line marks the observed value of $t=-3.2091$. This line plots in the rejection region, leading to the conclusion that $\mu <{19.30}$\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}g/cm\textsuperscript  {3}. The horizontal dashed line marks the p-value ($0.0163<\alpha $).\relax }}{minipage.1}{}}
\newlabel{tab:2setcoins}{{\rEfLiNK{x1-44010r9.2}{\csname :autoref\endcsname{tabular}9.2}}{\rEfLiNK{x1-44010r9.2}{\csname :autoref\endcsname{tabular}119}}{\rEfLiNK{x1-44010r9.2}{\csname :autoref\endcsname{tabular}a) PDF and b) CDF of a t-distribution with 4 degrees of freedom. The one-sided rejection region (for $\alpha =0.05$) is marked in black. The vertical dashed line marks the observed value of $t=-3.2091$. This line plots in the rejection region, leading to the conclusion that $\mu <{19.30}$\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}g/cm\textsuperscript  {3}. The horizontal dashed line marks the p-value ($0.0163<\alpha $).\relax }}{tabular.1}{}}
\newlabel{eq:t2}{{\rEfLiNK{x1-44014r4}{\csname :autoref\endcsname{equation}9.4}}{\rEfLiNK{x1-44014r4}{\csname :autoref\endcsname{equation}120}}{\rEfLiNK{x1-44014r4}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:sp}{{\rEfLiNK{x1-44015r5}{\csname :autoref\endcsname{equation}9.5}}{\rEfLiNK{x1-44015r5}{\csname :autoref\endcsname{equation}120}}{\rEfLiNK{x1-44015r5}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{fig:2samplettest}{{\rEfLiNK{x1-44021r6}{\csname :autoref\endcsname{minipage}9.6}}{\rEfLiNK{x1-44021r6}{\csname :autoref\endcsname{minipage}121}}{\rEfLiNK{x1-44021r6}{\csname :autoref\endcsname{minipage}a) PDF and b) CDF of a t-distribution with 7 degrees of freedom. The two-sided rejection region (for $\alpha =0.05$) is marked in black. The vertical dashed line marks the observed value of $t=-1.857$ and plots outside the rejection region. Therefore the test does not allow us to conclude that $\mu _1{{\relax {\special {t4ht@+&{35}x2260{59}}x}}}\mu _2$. \relax }}{minipage.1}{}}
\newlabel{eq:tconf}{{\rEfLiNK{x1-450009.3}{\csname :autoref\endcsname{section}9.3}}{\rEfLiNK{x1-450009.3}{\csname :autoref\endcsname{section}122}}{\rEfLiNK{x1-450009.3}{\csname :autoref\endcsname{section}Confidence intervals}}{section.1}{}}
\newlabel{eq:tci}{{\rEfLiNK{x1-45001r6}{\csname :autoref\endcsname{equation}9.6}}{\rEfLiNK{x1-45001r6}{\csname :autoref\endcsname{equation}124}}{\rEfLiNK{x1-45001r6}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{fig:tdof}{{\rEfLiNK{x1-45002r7}{\csname :autoref\endcsname{minipage}9.7}}{\rEfLiNK{x1-45002r7}{\csname :autoref\endcsname{minipage}124}}{\rEfLiNK{x1-45002r7}{\csname :autoref\endcsname{minipage}a) PDFs and b) CDFs of the t-distribution for three different degrees of freedom ($df$). For small sample sizes (low $df$), the t-distribution has long tails towards low and high values. With increasing sample size, the tails become shorter and the t-distribution sharper. When $df>30$, the t-distribution is almost indistinguishable from a standard normal distribution with $\mu =0$ and $\sigma =1$.\relax }}{minipage.1}{}}
\newlabel{fig:normconf}{{\rEfLiNK{x1-45004r8}{\csname :autoref\endcsname{minipage}9.8}}{\rEfLiNK{x1-45004r8}{\csname :autoref\endcsname{minipage}125}}{\rEfLiNK{x1-45004r8}{\csname :autoref\endcsname{minipage}The grey and black lines mark the PDFs (a) and CDFs (b) of two normal distributions whose means (vertical dotted lines) are offset by 2 standard errors from the sample average (vertical dashed line). They mark a $\sim $95\% confidence interval for $\mu $. However this simple procedure only works if sample size is large enough for the Central Limit Theorem to apply.\relax }}{minipage.1}{}}
\newlabel{sec:chi2}{{\rEfLiNK{x1-460009.4}{\csname :autoref\endcsname{section}9.4}}{\rEfLiNK{x1-460009.4}{\csname :autoref\endcsname{section}125}}{\rEfLiNK{x1-460009.4}{\csname :autoref\endcsname{section}The $\chi ^2$-test}}{section.1}{}}
\newlabel{eq:chi2}{{\rEfLiNK{x1-46001r7}{\csname :autoref\endcsname{equation}9.7}}{\rEfLiNK{x1-46001r7}{\csname :autoref\endcsname{equation}126}}{\rEfLiNK{x1-46001r7}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{fig:chi2}{{\rEfLiNK{x1-46009r9}{\csname :autoref\endcsname{minipage}9.9}}{\rEfLiNK{x1-46009r9}{\csname :autoref\endcsname{minipage}128}}{\rEfLiNK{x1-46009r9}{\csname :autoref\endcsname{minipage}a) PDF and b) CDF of the $\chi ^2$-distribution with 6 degrees of freedom. The rejection region is marked in black. The observed value for the earthquake data ($\chi ^2=4.70$) is shown as a vertical dashed line. It plots outside the rejection region, indicating that the histogram of the data falls within the expected range of the hypothesised (Poisson) distribution. \relax }}{minipage.1}{}}
\newlabel{sec:contingency}{{\rEfLiNK{x1-470009.5}{\csname :autoref\endcsname{section}9.5}}{\rEfLiNK{x1-470009.5}{\csname :autoref\endcsname{section}128}}{\rEfLiNK{x1-470009.5}{\csname :autoref\endcsname{section}Comparing two or more samples}}{section.1}{}}
\newlabel{tab:observedclasts}{{\rEfLiNK{x1-47001r1}{\csname :autoref\endcsname{center}9.1}}{\rEfLiNK{x1-47001r1}{\csname :autoref\endcsname{center}129}}{\rEfLiNK{x1-47001r1}{\csname :autoref\endcsname{center}Manual point counting data for two samples of sand.\relax }}{center.1}{}}
\newlabel{tab:expectedclasts}{{\rEfLiNK{x1-47003r2}{\csname :autoref\endcsname{center}9.2}}{\rEfLiNK{x1-47003r2}{\csname :autoref\endcsname{center}130}}{\rEfLiNK{x1-47003r2}{\csname :autoref\endcsname{center}Expected grain counts for two sedimentary samples.\relax }}{center.1}{}}
\newlabel{fig:chi22}{{\rEfLiNK{x1-47012r10}{\csname :autoref\endcsname{minipage}9.10}}{\rEfLiNK{x1-47012r10}{\csname :autoref\endcsname{minipage}132}}{\rEfLiNK{x1-47012r10}{\csname :autoref\endcsname{minipage}a) PDF and b) CDF of the $\chi ^2$-distribution with 3 degrees of freedom. The rejection region is marked in black. The observed value ($\chi ^2=0.86$) is shown as a vertical dashed line. It plots outside the rejection region, indicating that the histogram of the data falls within the expected range of the hypothesised (Poisson) distribution. \relax }}{minipage.1}{}}
\newlabel{sec:cherrypicking}{{\rEfLiNK{x1-480009.6}{\csname :autoref\endcsname{section}9.6}}{\rEfLiNK{x1-480009.6}{\csname :autoref\endcsname{section}132}}{\rEfLiNK{x1-480009.6}{\csname :autoref\endcsname{section}Cherry picking (Type-I errors revisited)}}{section.1}{}}
\newlabel{fig:cherrypicking}{{\rEfLiNK{x1-48001r11}{\csname :autoref\endcsname{document}9.11}}{\rEfLiNK{x1-48001r11}{\csname :autoref\endcsname{document}133}}{\rEfLiNK{x1-48001r11}{\csname :autoref\endcsname{document}a) predicted frequency distribution for the zircon count data of example\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}2 in Chapter\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {ch:poisson}, following a Poisson distribution with $\lambda =3.50$ and $n=48$; b) -- d) three sample distributions with the $\chi ^2$ statistic and p-values for comparison with distribution a; e) $\chi ^2$-distribution with 5 degrees of freedom. The $\chi ^2$-test would flag sample d as being `significantly different' from the predicted histogram a. Sample c also looks somewhat different from the predicted distribution a, but this difference falls within the expected range of random sampling variability of the Poisson distribution. Sample b is identical to the prediction a. This should raise suspicion. It is extremely unlikely for a sample to fit the prediction so well.\vspace  \medskipamount \relax }}{document.1}{}}
\newlabel{sec:effectsize}{{\rEfLiNK{x1-490009.7}{\csname :autoref\endcsname{section}9.7}}{\rEfLiNK{x1-490009.7}{\csname :autoref\endcsname{section}133}}{\rEfLiNK{x1-490009.7}{\csname :autoref\endcsname{section}Effect size (Type-II errors revisited)}}{section.1}{}}
\newlabel{tab:observedsand}{{\rEfLiNK{x1-49001r3}{\csname :autoref\endcsname{center}9.3}}{\rEfLiNK{x1-49001r3}{\csname :autoref\endcsname{center}133}}{\rEfLiNK{x1-49001r3}{\csname :autoref\endcsname{center}Automated point counting data for two samples of sand.\relax }}{center.1}{}}
\newlabel{tab:predictedsand}{{\rEfLiNK{x1-49005r4}{\csname :autoref\endcsname{center}9.4}}{\rEfLiNK{x1-49005r4}{\csname :autoref\endcsname{center}134}}{\rEfLiNK{x1-49005r4}{\csname :autoref\endcsname{center}Predicted point counts.\relax }}{center.1}{}}
\newlabel{eq:effectsize}{{\rEfLiNK{x1-49013r10}{\csname :autoref\endcsname{equation}9.10}}{\rEfLiNK{x1-49013r10}{\csname :autoref\endcsname{equation}135}}{\rEfLiNK{x1-49013r10}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{sec:nonparametric}{{\rEfLiNK{x1-500009.8}{\csname :autoref\endcsname{section}9.8}}{\rEfLiNK{x1-500009.8}{\csname :autoref\endcsname{section}136}}{\rEfLiNK{x1-500009.8}{\csname :autoref\endcsname{section}Non-parametric tests}}{section.1}{}}
\newlabel{tab:mannwhitneygeneric}{{\rEfLiNK{x1-500009.8}{\csname :autoref\endcsname{section}9.8}}{\rEfLiNK{x1-500009.8}{\csname :autoref\endcsname{section}137}}{\rEfLiNK{x1-500009.8}{\csname :autoref\endcsname{section}Non-parametric tests}}{section.1}{}}
\newlabel{tab:mannwhitneycoins}{{\rEfLiNK{x1-50001r5}{\csname :autoref\endcsname{center}9.5}}{\rEfLiNK{x1-50001r5}{\csname :autoref\endcsname{center}139}}{\rEfLiNK{x1-50001r5}{\csname :autoref\endcsname{center}The same data as table\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {tab:2setcoins} but with the second sample marked in bold for future use.\relax }}{center.1}{}}
\newlabel{fig:wilcox}{{\rEfLiNK{x1-50009r12}{\csname :autoref\endcsname{minipage}9.12}}{\rEfLiNK{x1-50009r12}{\csname :autoref\endcsname{minipage}140}}{\rEfLiNK{x1-50009r12}{\csname :autoref\endcsname{minipage}a) PMF and b) CDF of the Wilcoxon test statistic for comparison of two samples containing 5 and 4 items, respectively. The two-sided rejection region is marked in black. The observed value ($W=26$) is shown as a vertical dashed line. It plots outside the rejection region, leaving the possibility open that the two samples might have come from the same distribution.\relax }}{minipage.1}{}}
\newlabel{eq:KS}{{\rEfLiNK{x1-50011r11}{\csname :autoref\endcsname{equation}9.11}}{\rEfLiNK{x1-50011r11}{\csname :autoref\endcsname{equation}141}}{\rEfLiNK{x1-50011r11}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{fig:KS}{{\rEfLiNK{x1-50012r13}{\csname :autoref\endcsname{minipage}9.13}}{\rEfLiNK{x1-50012r13}{\csname :autoref\endcsname{minipage}141}}{\rEfLiNK{x1-50012r13}{\csname :autoref\endcsname{minipage}The two-sample Kolmogorov-Smirnov statistic is the maximum vertical distance between two ECDFs. This example compares the cumulative distributions of 121 detrital zircon U--Pb ages from the Yellow River with 116 detrital zircon U--Pb ages from a sand dune in the adjacent Mu Us desert. The KS-distance is 0.3006.\relax }}{minipage.1}{}}
\newlabel{fig:KSdens}{{\rEfLiNK{x1-50020r14}{\csname :autoref\endcsname{minipage}9.14}}{\rEfLiNK{x1-50020r14}{\csname :autoref\endcsname{minipage}142}}{\rEfLiNK{x1-50020r14}{\csname :autoref\endcsname{minipage}a) PMF and b) CDF of the Kolmogorov-Smirnov statistic for comparison of two samples containing 116 and 121 items, respectively. The vertical dashed lines mark the test statistic for the two sand samples of Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:KS}. The rejection region is marked in black on the PMF and groups all values of the test statistic that exceed the 95 percentile ($D=0.174$). $H_0$ is rejected.\relax }}{minipage.1}{}}
\newlabel{fig:KSnorm}{{\rEfLiNK{x1-50022r15}{\csname :autoref\endcsname{minipage}9.15}}{\rEfLiNK{x1-50022r15}{\csname :autoref\endcsname{minipage}143}}{\rEfLiNK{x1-50022r15}{\csname :autoref\endcsname{minipage}Kolmogorov-Smirnov statistic for the comparison of a sample with a normal distribution that has the same mean and standard deviation. In this case $D=0.27$, which can be compared with a lookup table for a \textit  {one sample} Kolmogorov-Smirnov test. The outcome of this test (which is not elaborated in these notes) is a rejection of the null hypothesis.\relax }}{minipage.1}{}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:regression}{{\rEfLiNK{x1-5100010}{\csname :autoref\endcsname{chapter}10}}{\rEfLiNK{x1-5100010}{\csname :autoref\endcsname{chapter}145}}{\rEfLiNK{x1-5100010}{\csname :autoref\endcsname{chapter}Regression}}{chapter.1}{}}
\newlabel{eq:Rb-Sr}{{\rEfLiNK{x1-51001r1}{\csname :autoref\endcsname{equation}10.1}}{\rEfLiNK{x1-51001r1}{\csname :autoref\endcsname{equation}145}}{\rEfLiNK{x1-51001r1}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:y=a+bx}{{\rEfLiNK{x1-51002r2}{\csname :autoref\endcsname{equation}10.2}}{\rEfLiNK{x1-51002r2}{\csname :autoref\endcsname{equation}145}}{\rEfLiNK{x1-51002r2}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{tab:Rb-Sr}{{\rEfLiNK{x1-51003r1}{\csname :autoref\endcsname{center}10.1}}{\rEfLiNK{x1-51003r1}{\csname :autoref\endcsname{center}146}}{\rEfLiNK{x1-51003r1}{\csname :autoref\endcsname{center}Rb--Sr composition of eight aliquots ($1\leq {i}\leq {8}$) of the same sample.\relax }}{center.1}{}}
\newlabel{fig:Rb-Sr}{{\rEfLiNK{x1-51005r1}{\csname :autoref\endcsname{minipage}10.1}}{\rEfLiNK{x1-51005r1}{\csname :autoref\endcsname{minipage}146}}{\rEfLiNK{x1-51005r1}{\csname :autoref\endcsname{minipage}Isochron plot for the Rb--Sr data. The scatter plot of eight \textsuperscript  {87}Rb/\textsuperscript  {86}Sr- and \textsuperscript  {87}Sr/\textsuperscript  {86}Sr-ratios forms an array of points along a line whose intercept marks the initial \textsuperscript  {87}Sr/\textsuperscript  {86}Sr-composition, and whose slope is a function of the age ($t = \qopname  \relax o{ln}[1+\beta _1]/\lambda $). The linear trend is not perfect due to analytical uncertainty, which has dispersed the data. \relax }}{minipage.1}{}}
\newlabel{eq:y=a+bx+e}{{\rEfLiNK{x1-51007r3}{\csname :autoref\endcsname{equation}10.3}}{\rEfLiNK{x1-51007r3}{\csname :autoref\endcsname{equation}146}}{\rEfLiNK{x1-51007r3}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{sec:corrcoef}{{\rEfLiNK{x1-5200010.1}{\csname :autoref\endcsname{section}10.1}}{\rEfLiNK{x1-5200010.1}{\csname :autoref\endcsname{section}146}}{\rEfLiNK{x1-5200010.1}{\csname :autoref\endcsname{section}The correlation coefficient}}{section.1}{}}
\newlabel{eq:rho}{{\rEfLiNK{x1-52001r4}{\csname :autoref\endcsname{equation}10.4}}{\rEfLiNK{x1-52001r4}{\csname :autoref\endcsname{equation}147}}{\rEfLiNK{x1-52001r4}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:r}{{\rEfLiNK{x1-52002r5}{\csname :autoref\endcsname{equation}10.5}}{\rEfLiNK{x1-52002r5}{\csname :autoref\endcsname{equation}147}}{\rEfLiNK{x1-52002r5}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{fig:r}{{\rEfLiNK{x1-52003r2}{\csname :autoref\endcsname{document}10.2}}{\rEfLiNK{x1-52003r2}{\csname :autoref\endcsname{document}148}}{\rEfLiNK{x1-52003r2}{\csname :autoref\endcsname{document}Four synthetic bivariate normal datasets exhibiting different degrees of correlation between the x- and y-variable. Panel a) displays no correlation, b) a weak positive correlation, c) a strong positive correlation, and d) a strong negative correlation.\\\relax }}{document.1}{}}
\newlabel{eq:tr}{{\rEfLiNK{x1-52005r6}{\csname :autoref\endcsname{equation}10.6}}{\rEfLiNK{x1-52005r6}{\csname :autoref\endcsname{equation}148}}{\rEfLiNK{x1-52005r6}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{fig:tr}{{\rEfLiNK{x1-52012r3}{\csname :autoref\endcsname{minipage}10.3}}{\rEfLiNK{x1-52012r3}{\csname :autoref\endcsname{minipage}150}}{\rEfLiNK{x1-52012r3}{\csname :autoref\endcsname{minipage}a) PDF and b) CDF of a t-distribution with 6 degrees of freedom. The two-sided rejection region (for $\alpha = 0.05$) is marked in black. The vertical dashed line marks the observed value of t = 13.98 and plots inside the rejection region. Therefore the test rejects the null hypothesis that $\rho =0$, leading to the conclusion that the data are significantly correlated.\relax }}{minipage.1}{}}
\newlabel{sec:leastsquares}{{\rEfLiNK{x1-5300010.2}{\csname :autoref\endcsname{section}10.2}}{\rEfLiNK{x1-5300010.2}{\csname :autoref\endcsname{section}150}}{\rEfLiNK{x1-5300010.2}{\csname :autoref\endcsname{section}Least Squares}}{section.1}{}}
\newlabel{eq:ss}{{\rEfLiNK{x1-53001r7}{\csname :autoref\endcsname{equation}10.7}}{\rEfLiNK{x1-53001r7}{\csname :autoref\endcsname{equation}150}}{\rEfLiNK{x1-53001r7}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:fitab}{{\rEfLiNK{x1-53007r9}{\csname :autoref\endcsname{equation}10.9}}{\rEfLiNK{x1-53007r9}{\csname :autoref\endcsname{equation}152}}{\rEfLiNK{x1-53007r9}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{sec:MLregression}{{\rEfLiNK{x1-5400010.3}{\csname :autoref\endcsname{section}10.3}}{\rEfLiNK{x1-5400010.3}{\csname :autoref\endcsname{section}152}}{\rEfLiNK{x1-5400010.3}{\csname :autoref\endcsname{section}Maximum Likelihood}}{section.1}{}}
\newlabel{eq:normalresid}{{\rEfLiNK{x1-54001r10}{\csname :autoref\endcsname{equation}10.10}}{\rEfLiNK{x1-54001r10}{\csname :autoref\endcsname{equation}153}}{\rEfLiNK{x1-54001r10}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:Lregression}{{\rEfLiNK{x1-54002r11}{\csname :autoref\endcsname{equation}10.11}}{\rEfLiNK{x1-54002r11}{\csname :autoref\endcsname{equation}153}}{\rEfLiNK{x1-54002r11}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:sigmabeta}{{\rEfLiNK{x1-54004r13}{\csname :autoref\endcsname{equation}10.13}}{\rEfLiNK{x1-54004r13}{\csname :autoref\endcsname{equation}154}}{\rEfLiNK{x1-54004r13}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:sigmahatregression}{{\rEfLiNK{x1-54005r14}{\csname :autoref\endcsname{equation}10.14}}{\rEfLiNK{x1-54005r14}{\csname :autoref\endcsname{equation}155}}{\rEfLiNK{x1-54005r14}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:ciregression}{{\rEfLiNK{x1-54006r15}{\csname :autoref\endcsname{equation}10.15}}{\rEfLiNK{x1-54006r15}{\csname :autoref\endcsname{equation}155}}{\rEfLiNK{x1-54006r15}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:envelope}{{\rEfLiNK{x1-54007r16}{\csname :autoref\endcsname{equation}10.16}}{\rEfLiNK{x1-54007r16}{\csname :autoref\endcsname{equation}156}}{\rEfLiNK{x1-54007r16}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{fig:envelope}{{\rEfLiNK{x1-54008r5}{\csname :autoref\endcsname{minipage}10.5}}{\rEfLiNK{x1-54008r5}{\csname :autoref\endcsname{minipage}156}}{\rEfLiNK{x1-54008r5}{\csname :autoref\endcsname{minipage}The grey area represents a 95\% confidence envelope for the least squares regression of the Rb--Sr data. The curvature of this envelope reflects the increased uncertainty that is caused by \textbf  {extrapolating} the data. The width of the confidence envelope is the smallest near the average of the measurements ($\let \prOteCt \relax \let \prOteCt \relax \Protect \csname acp:c\endcsname {18}{x}$, $\let \prOteCt \relax \let \prOteCt \relax \Protect \csname acp:c\endcsname {18}{y}$) and increases indefinitely beyond that.\relax }}{minipage.1}{}}
\newlabel{eq:prediction-interval}{{\rEfLiNK{x1-54010r17}{\csname :autoref\endcsname{equation}10.17}}{\rEfLiNK{x1-54010r17}{\csname :autoref\endcsname{equation}156}}{\rEfLiNK{x1-54010r17}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{fig:prediction-interval}{{\rEfLiNK{x1-54011r6}{\csname :autoref\endcsname{document}10.6}}{\rEfLiNK{x1-54011r6}{\csname :autoref\endcsname{document}156}}{\rEfLiNK{x1-54011r6}{\csname :autoref\endcsname{document}Confidence envelopes (grey) and prediction intervals (dashed lines) for three synthetic datasets of increasing size. Whereas the width of the confidence interval approaches zero for large datasets, the width of the prediction interval only decreases slightly. \relax }}{document.1}{}}
\newlabel{sec:regression-caveats}{{\rEfLiNK{x1-5500010.4}{\csname :autoref\endcsname{section}10.4}}{\rEfLiNK{x1-5500010.4}{\csname :autoref\endcsname{section}157}}{\rEfLiNK{x1-5500010.4}{\csname :autoref\endcsname{section}Common mistakes}}{section.1}{}}
\newlabel{fig:regression-p-hacking}{{\rEfLiNK{x1-55002r7}{\csname :autoref\endcsname{minipage}10.7}}{\rEfLiNK{x1-55002r7}{\csname :autoref\endcsname{minipage}157}}{\rEfLiNK{x1-55002r7}{\csname :autoref\endcsname{minipage}18 scatter plots of bivariate random uniform data, with indication of the coefficient of determination ($r^2$) and the p-value for correlation. The one `significant' result (p-value = 0.00063) is a Type-I error.\relax }}{minipage.1}{}}
\newlabel{fig:regression-outlier}{{\rEfLiNK{x1-55005r8}{\csname :autoref\endcsname{minipage}10.8}}{\rEfLiNK{x1-55005r8}{\csname :autoref\endcsname{minipage}157}}{\rEfLiNK{x1-55005r8}{\csname :autoref\endcsname{minipage}A synthetic dataset of ten clustered points around $\{x=1,y=1\}$ plus one outlier at $\{x=5,y=5\}$. The cluster consists of the same values as the first panel of Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:regression-p-hacking}. But whereas the latter dataset was characterised by a coefficient of determination of almost zero ($r^2=0.0014$), the new dataset has a coefficient of determination that is close to one ($r^2=0.9$). Such is the disproportionate effect of the additional data point. \relax }}{minipage.1}{}}
\newlabel{fig:spurious}{{\rEfLiNK{x1-55008r9}{\csname :autoref\endcsname{minipage}10.9}}{\rEfLiNK{x1-55008r9}{\csname :autoref\endcsname{minipage}158}}{\rEfLiNK{x1-55008r9}{\csname :autoref\endcsname{minipage}a--c) Three random datasets of 50 points each, drawn from normal distributions with means $\mu _x=\mu _y=\mu _z=100$ and standard deviations $\sigma _x=\sigma _y=1$ and $\sigma _z=10$, respectively. The three datasets are independent, so $\rho _{x,y}=\rho _{x,z}=\rho _{y,z}=0$. However, the ratio of $x/z$ is strongly correlated with d) $y/z$, and with e) $z$. This correlation is entirely spurious and has no scientific value.\\\relax }}{minipage.1}{}}
\newlabel{eq:spurious}{{\rEfLiNK{x1-55011r18}{\csname :autoref\endcsname{equation}10.18}}{\rEfLiNK{x1-55011r18}{\csname :autoref\endcsname{equation}158}}{\rEfLiNK{x1-55011r18}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:spuriousxzyz}{{\rEfLiNK{x1-55012r19}{\csname :autoref\endcsname{equation}10.19}}{\rEfLiNK{x1-55012r19}{\csname :autoref\endcsname{equation}159}}{\rEfLiNK{x1-55012r19}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:spuriouszxz}{{\rEfLiNK{x1-55013r20}{\csname :autoref\endcsname{equation}10.20}}{\rEfLiNK{x1-55013r20}{\csname :autoref\endcsname{equation}159}}{\rEfLiNK{x1-55013r20}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:spuriousxzyzindepxyz}{{\rEfLiNK{x1-55014r21}{\csname :autoref\endcsname{equation}10.21}}{\rEfLiNK{x1-55014r21}{\csname :autoref\endcsname{equation}159}}{\rEfLiNK{x1-55014r21}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{sec:weightedregression}{{\rEfLiNK{x1-5600010.5}{\csname :autoref\endcsname{section}10.5}}{\rEfLiNK{x1-5600010.5}{\csname :autoref\endcsname{section}160}}{\rEfLiNK{x1-5600010.5}{\csname :autoref\endcsname{section}Weighted regression}}{section.1}{}}
\newlabel{tab:york}{{\rEfLiNK{x1-56001r3}{\csname :autoref\endcsname{center}10.3}}{\rEfLiNK{x1-56001r3}{\csname :autoref\endcsname{center}161}}{\rEfLiNK{x1-56001r3}{\csname :autoref\endcsname{center}Synthetic three-aliquot dataset. $X$ and $Y$ the \emph  {true} values; $x$ and $y$ are three \emph  {measurements}, $s[x]$ and $s[y]$ their respective standard errors, and $s[x,y]$ their covariance. The uncertainties differ between the three samples, which are therefore heteroscedastic.\relax }}{center.1}{}}
\newlabel{fig:errorfit}{{\rEfLiNK{x1-56003r10}{\csname :autoref\endcsname{minipage}10.10}}{\rEfLiNK{x1-56003r10}{\csname :autoref\endcsname{minipage}161}}{\rEfLiNK{x1-56003r10}{\csname :autoref\endcsname{minipage}Synthetic data of Table\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {tab:york}. The white squares are the true population values ($X$ and $Y$) of three aliquots. The black squares are three measurements ($x$ and $y$). The ellipses represent 95\% confidence regions for bivariate normal distributions with means $x$ and $y$, and (co)variances $s[x]^2$, $s[y]^2$ and $s[x,y]$. The true values fall on a line with intercept $\beta _0=10$ and slope $\beta _1=1$ (dotted line). The unweighted least squares fit (dashed line) has an intercept of $\beta _0=1.9$ and slope $\beta _1=1.63$. This poor result is entirely due to the third data point, whose disproportionally large uncertainties are not properly accounted for by the ordinary least squares regression algorithm. \relax }}{minipage.1}{}}
\newlabel{eq:york}{{\rEfLiNK{x1-56005r22}{\csname :autoref\endcsname{equation}10.22}}{\rEfLiNK{x1-56005r22}{\csname :autoref\endcsname{equation}162}}{\rEfLiNK{x1-56005r22}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{fig:yorkfit}{{\rEfLiNK{x1-56006r11}{\csname :autoref\endcsname{minipage}10.11}}{\rEfLiNK{x1-56006r11}{\csname :autoref\endcsname{minipage}162}}{\rEfLiNK{x1-56006r11}{\csname :autoref\endcsname{minipage}a) the same as Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:errorfit}, but with the weighted regression result added as a solid line. Its intercept is $\beta _0=9.4$ and its slope is $\beta _1=1.05$. These values are much closer to the true values (dotted line) than the ordinary least squares solution (dashed line) is. b) Zooming into aliquot\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}3 shows the true value of the independent variable ($X_3$), its measured value ($x_3$), and the fitted value $\a:boldsymbol \boldsymbol  {x}\b:boldsymbol _3$. \relax }}{minipage.1}{}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:fractals}{{\rEfLiNK{x1-5700011}{\csname :autoref\endcsname{chapter}11}}{\rEfLiNK{x1-5700011}{\csname :autoref\endcsname{chapter}163}}{\rEfLiNK{x1-5700011}{\csname :autoref\endcsname{chapter}Fractals and chaos}}{chapter.1}{}}
\newlabel{fig:recentquakes}{{\rEfLiNK{x1-57002r1}{\csname :autoref\endcsname{minipage}11.1}}{\rEfLiNK{x1-57002r1}{\csname :autoref\endcsname{minipage}163}}{\rEfLiNK{x1-57002r1}{\csname :autoref\endcsname{minipage}Histogram for 20,000 recent earthquakes of magnitude $\geq {4.5}$ from the USGS earthquake catalog.\relax }}{minipage.1}{}}
\newlabel{fig:recentlogquakes}{{\rEfLiNK{x1-57004r2}{\csname :autoref\endcsname{minipage}11.2}}{\rEfLiNK{x1-57004r2}{\csname :autoref\endcsname{minipage}163}}{\rEfLiNK{x1-57004r2}{\csname :autoref\endcsname{minipage} The histogram of the logarithm of the 20,000 earthquakes in Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:recentquakes} is still positively skewed.\relax }}{minipage.1}{}}
\newlabel{sec:power-law}{{\rEfLiNK{x1-5800011.1}{\csname :autoref\endcsname{section}11.1}}{\rEfLiNK{x1-5800011.1}{\csname :autoref\endcsname{section}163}}{\rEfLiNK{x1-5800011.1}{\csname :autoref\endcsname{section}Power law distributions}}{section.1}{}}
\newlabel{fig:gutenberg}{{\rEfLiNK{x1-58001r3}{\csname :autoref\endcsname{minipage}11.3}}{\rEfLiNK{x1-58001r3}{\csname :autoref\endcsname{minipage}164}}{\rEfLiNK{x1-58001r3}{\csname :autoref\endcsname{minipage} Bivariate scatter plot of $y=\qopname  \relax o{log}_{10}[N/N_\circ ]$ against earthquake magnitude, where $N$ is the number of earthquakes exceeding a given magnitude and $N_\circ $ is the total number of registered earthquakes, which is 20,000 for the dataset of Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:recentquakes}. \relax }}{minipage.1}{}}
\newlabel{eq:gutenberg}{{\rEfLiNK{x1-58003r1}{\csname :autoref\endcsname{equation}11.1}}{\rEfLiNK{x1-58003r1}{\csname :autoref\endcsname{equation}164}}{\rEfLiNK{x1-58003r1}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{fig:Finland}{{\rEfLiNK{x1-58007r4}{\csname :autoref\endcsname{minipage}11.4}}{\rEfLiNK{x1-58007r4}{\csname :autoref\endcsname{minipage}165}}{\rEfLiNK{x1-58007r4}{\csname :autoref\endcsname{minipage}A map of central Finland (axis labels mark latitude and longitude), with water marked in black and land marked in white. Finland is also known as ``the land of a thousand lakes''. But in reality there are far more than 1000 lakes in Finland. The small area shown in this figure already contains 2327 of them. Most of these lakes are small, but there are also a few big ones that cover areas of more than 1000\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}km\textsuperscript  {2}.\relax }}{minipage.1}{}}
\newlabel{fig:Finlandpowerlaw}{{\rEfLiNK{x1-58009r5}{\csname :autoref\endcsname{minipage}11.5}}{\rEfLiNK{x1-58009r5}{\csname :autoref\endcsname{minipage}166}}{\rEfLiNK{x1-58009r5}{\csname :autoref\endcsname{minipage}Plotting the number of lakes exceeding a certain size against that size on a log-log scale yields a linear array of points similar to the Gutenberg-Richter Law of Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:gutenberg}. Extrapolating this trend towards the left would reveal that there are millions of puddles in Finland.\relax }}{minipage.1}{}}
\newlabel{sec:britain}{{\rEfLiNK{x1-5900011.2}{\csname :autoref\endcsname{section}11.2}}{\rEfLiNK{x1-5900011.2}{\csname :autoref\endcsname{section}166}}{\rEfLiNK{x1-5900011.2}{\csname :autoref\endcsname{section}How long is the coast of Britain?}}{section.1}{}}
\newlabel{fig:loglogbritain}{{\rEfLiNK{x1-59004r7}{\csname :autoref\endcsname{minipage}11.7}}{\rEfLiNK{x1-59004r7}{\csname :autoref\endcsname{minipage}166}}{\rEfLiNK{x1-59004r7}{\csname :autoref\endcsname{minipage}Setting out the length of the British coastline against the length of the measuring rod on a log-log scale produces a linear trend with a slope of $-0.253$. This line can be extrapolated to lower values to estimate the length that would be measured with even smaller measuring rods. For example, if we were to measure the British coast with a 30\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}cm long ruler, then this would produce a result of $\qopname  \relax o{exp}(9.18-0.253\qopname  \relax o{ln}[3\times {10}^{-4}])=42,060$\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}km! \relax }}{minipage.1}{}}
\newlabel{fig:fractaldimbritain}{{\rEfLiNK{x1-59006r8}{\csname :autoref\endcsname{minipage}11.8}}{\rEfLiNK{x1-59006r8}{\csname :autoref\endcsname{minipage}167}}{\rEfLiNK{x1-59006r8}{\csname :autoref\endcsname{minipage}The same data as Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:loglogbritain} but plotting the number of polygonal segments on the y-axis instead of the length of those segments. Note how the slope of the best fit line equals the slope of Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:loglogbritain} minus one.\relax }}{minipage.1}{}}
\newlabel{fig:Britainboxes}{{\rEfLiNK{x1-59008r9}{\csname :autoref\endcsname{document}11.9}}{\rEfLiNK{x1-59008r9}{\csname :autoref\endcsname{document}167}}{\rEfLiNK{x1-59008r9}{\csname :autoref\endcsname{document}Box counting of the British coast using (from left to right) a $16\times {16}$, $32\times {32}$, $64\times {64}$, and $128\times {128}$ grid. Black squares overlap with the coastline, white squares do not. The legends in the upper right corner of each subpanel specify the number of black squares relative to the total number of squares.\vspace  \medskipamount \relax }}{document.1}{}}
\newlabel{fig:Britainboxcounts}{{\rEfLiNK{x1-59010r10}{\csname :autoref\endcsname{minipage}11.10}}{\rEfLiNK{x1-59010r10}{\csname :autoref\endcsname{minipage}167}}{\rEfLiNK{x1-59010r10}{\csname :autoref\endcsname{minipage}Plotting the number of boxes against their size (i.e. width in pixels) on a log-log diagram yields a linear array with a slope of $-1.26$. This is similar to the value obtained by the polygonal line segment method of Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:fractaldimbritain}. \relax }}{minipage.1}{}}
\newlabel{fig:Corsica}{{\rEfLiNK{x1-59012r11}{\csname :autoref\endcsname{document}11.11}}{\rEfLiNK{x1-59012r11}{\csname :autoref\endcsname{document}167}}{\rEfLiNK{x1-59012r11}{\csname :autoref\endcsname{document}Box counting of the river network on the island of Corsica (France). Black boxes overlap with rivers, white boxes do not. Legends are as in Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:Britainboxes}.\vspace  \medskipamount \relax }}{document.1}{}}
\newlabel{fig:Corsicaboxcounts}{{\rEfLiNK{x1-59014r12}{\csname :autoref\endcsname{minipage}11.12}}{\rEfLiNK{x1-59014r12}{\csname :autoref\endcsname{minipage}167}}{\rEfLiNK{x1-59014r12}{\csname :autoref\endcsname{minipage}Log-log plot of the box counting results of Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:Corsica}. The power law appears to be a law of nature. \relax }}{minipage.1}{}}
\newlabel{sec:fractals}{{\rEfLiNK{x1-6000011.3}{\csname :autoref\endcsname{section}11.3}}{\rEfLiNK{x1-6000011.3}{\csname :autoref\endcsname{section}168}}{\rEfLiNK{x1-6000011.3}{\csname :autoref\endcsname{section}Fractals}}{section.1}{}}
\newlabel{fig:koch1}{{\rEfLiNK{x1-60001r13}{\csname :autoref\endcsname{minipage}11.13}}{\rEfLiNK{x1-60001r13}{\csname :autoref\endcsname{minipage}168}}{\rEfLiNK{x1-60001r13}{\csname :autoref\endcsname{minipage}A 1\textsuperscript  {st} order Koch curve is constructed by (1) dividing a straight line segment into three segments of equal length; (2) drawing an equilateral triangle that has the middle segment from step 1 as its base and points outward; and (3) removing the line segment that is the base of the triangle from step 2. \relax }}{minipage.1}{}}
\newlabel{fig:koch2}{{\rEfLiNK{x1-60003r14}{\csname :autoref\endcsname{minipage}11.14}}{\rEfLiNK{x1-60003r14}{\csname :autoref\endcsname{minipage}168}}{\rEfLiNK{x1-60003r14}{\csname :autoref\endcsname{minipage}A 2\textsuperscript  {nd} order Koch curve is derived from a 1\textsuperscript  {st} order Koch curve by replacing each straight line segment in the 1\textsuperscript  {st} order curve with a scaled down version of that 1\textsuperscript  {st} order curve.\relax }}{minipage.1}{}}
\newlabel{fig:koch3}{{\rEfLiNK{x1-60005r15}{\csname :autoref\endcsname{minipage}11.15}}{\rEfLiNK{x1-60005r15}{\csname :autoref\endcsname{minipage}168}}{\rEfLiNK{x1-60005r15}{\csname :autoref\endcsname{minipage}A 3\textsuperscript  {rd} order Koch curve is derived from a 2\textsuperscript  {nd} order Koch curve by replacing each straight line segment in the 2\textsuperscript  {nd} order curve with a scaled down version of the 1\textsuperscript  {st} order Koch curve.\relax }}{minipage.1}{}}
\newlabel{fig:koch6}{{\rEfLiNK{x1-60007r16}{\csname :autoref\endcsname{minipage}11.16}}{\rEfLiNK{x1-60007r16}{\csname :autoref\endcsname{minipage}168}}{\rEfLiNK{x1-60007r16}{\csname :autoref\endcsname{minipage}This is a 6\textsuperscript  {th} order Koch curve, which is generated by replacing each straight line segment in a 5\textsuperscript  {th} order curve with a scaled down version of the 1\textsuperscript  {st} order curve.\relax }}{minipage.1}{}}
\newlabel{fig:kochboxes}{{\rEfLiNK{x1-60009r17}{\csname :autoref\endcsname{document}11.17}}{\rEfLiNK{x1-60009r17}{\csname :autoref\endcsname{document}169}}{\rEfLiNK{x1-60009r17}{\csname :autoref\endcsname{document}Box counting of the 6\textsuperscript  {th} order Koch curve. The larger the boxes, the fewer of them are needed to cover the entire curve. The recursive order of the Koch curve can be increased indefinitely, and so does the number of small boxes needed to cover them.\vspace  \medskipamount \relax }}{document.1}{}}
\newlabel{fig:kochboxcounts}{{\rEfLiNK{x1-60011r18}{\csname :autoref\endcsname{minipage}11.18}}{\rEfLiNK{x1-60011r18}{\csname :autoref\endcsname{minipage}169}}{\rEfLiNK{x1-60011r18}{\csname :autoref\endcsname{minipage}Log-log plot setting out the number of boxes needed to cover the Koch curve of Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:koch6} against the size of those boxes. The best fitting line has a slope of $-1.26$, which is similar to the slope of the box-counting results for the British coastline (Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:Britainboxcounts}). Box sizes are expressed in arbitrary units (pixel widths).\relax }}{minipage.1}{}}
\newlabel{fig:sierpinski}{{\rEfLiNK{x1-60013r19}{\csname :autoref\endcsname{document}11.19}}{\rEfLiNK{x1-60013r19}{\csname :autoref\endcsname{document}169}}{\rEfLiNK{x1-60013r19}{\csname :autoref\endcsname{document}The \textbf  {Sierpinski carpet} is generated using a recursive algorithm that is built on a grid of eight black squares surrounding a white square. Each level of recursion replaces each black square with the same pattern. From left to right, this figure shows the first four levels of recursion for this algorithm. The end result is an arrangement of small and large holes that shares many characteristics with the size distribution of Finnish lakes shown in Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:Finland}.\vspace  \medskipamount \relax }}{document.1}{}}
\newlabel{fig:sierpinskiboxcounts}{{\rEfLiNK{x1-60015r20}{\csname :autoref\endcsname{minipage}11.20}}{\rEfLiNK{x1-60015r20}{\csname :autoref\endcsname{minipage}169}}{\rEfLiNK{x1-60015r20}{\csname :autoref\endcsname{minipage}Log-log plot of the box counting results for the Sierpinski carpet. Note how the slope of the best fitting line ($-1.89$) is steeper than that of the Koch curve (Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:kochboxcounts}). It is similar to the slope that would be obtained by box-counting the Finnish lakes of Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:Finland}, which is similar to $x-1$ where $x$ is the slope of the size-frequency plot of the Finnish lakes (Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:Finlandpowerlaw}).\relax }}{minipage.1}{}}
\newlabel{eq:fractaldim}{{\rEfLiNK{x1-60017r2}{\csname :autoref\endcsname{equation}11.2}}{\rEfLiNK{x1-60017r2}{\csname :autoref\endcsname{equation}170}}{\rEfLiNK{x1-60017r2}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:Minkowski}{{\rEfLiNK{x1-60018r3}{\csname :autoref\endcsname{equation}11.3}}{\rEfLiNK{x1-60018r3}{\csname :autoref\endcsname{equation}170}}{\rEfLiNK{x1-60018r3}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{fig:cantor}{{\rEfLiNK{x1-60020r21}{\csname :autoref\endcsname{minipage}11.21}}{\rEfLiNK{x1-60020r21}{\csname :autoref\endcsname{minipage}171}}{\rEfLiNK{x1-60020r21}{\csname :autoref\endcsname{minipage}The Cantor set is generated using a recursive algorithm that is built on a line segment whose middle third is removed. Each level of recursion replaces each black line by the same pattern. From top to bottom, this figure shows the first five levels of recursion for this algorithm. \relax }}{minipage.1}{}}
\newlabel{fig:cantorloglog}{{\rEfLiNK{x1-60022r22}{\csname :autoref\endcsname{minipage}11.22}}{\rEfLiNK{x1-60022r22}{\csname :autoref\endcsname{minipage}171}}{\rEfLiNK{x1-60022r22}{\csname :autoref\endcsname{minipage}The y-axis shows the number of linear segments in the Cantor set that exceed the length shown on the x-axis. The values form a power law with a fractal dimension of $D=\qopname  \relax o{ln}[2]/\qopname  \relax o{ln}[3]=0.631$. With a fractal dimension between zero and one, the Cantor set falls somewhere between a point and a line.\relax }}{minipage.1}{}}
\newlabel{sec:chaos}{{\rEfLiNK{x1-6100011.4}{\csname :autoref\endcsname{section}11.4}}{\rEfLiNK{x1-6100011.4}{\csname :autoref\endcsname{section}171}}{\rEfLiNK{x1-6100011.4}{\csname :autoref\endcsname{section}Chaos}}{section.1}{}}
\newlabel{fig:pendulum}{{\rEfLiNK{x1-61001r23}{\csname :autoref\endcsname{minipage}11.23}}{\rEfLiNK{x1-61001r23}{\csname :autoref\endcsname{minipage}171}}{\rEfLiNK{x1-61001r23}{\csname :autoref\endcsname{minipage}A pendulum is swinging above three magnets. The force ($F_m$) exerted on the pendulum scales with the square of its bob's distance ($|d|$) to the magnets ($F_m(i) \propto 1/|d(i)|^2$, where $1\leq {i}\leq {3}$ marks each of the magnets). The pendulum slows down due to friction ($F_f(i) \propto v$ where $v$ is the velocity of the bob) and eventually comes to a standstill above one of the magnets. On this figure it has done so above the first magnet.\relax }}{minipage.1}{}}
\newlabel{fig:3magnets1}{{\rEfLiNK{x1-61003r24}{\csname :autoref\endcsname{minipage}11.24}}{\rEfLiNK{x1-61003r24}{\csname :autoref\endcsname{minipage}171}}{\rEfLiNK{x1-61003r24}{\csname :autoref\endcsname{minipage}This figure shows the three magnet configuration of Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:pendulum} in map view. The black line marks the trajectory of the pendulum after it was pushed southward from a position towards the northwest of the three magnets. After describing a circular motion, the bob of the pendulum accelerates towards the second magnet, gets deflected by it, and slows down. It then heads towards the third magnet and the first magnet before returning to the second magnet and coming to a standstill there. \relax }}{minipage.1}{}}
\newlabel{fig:3magnets2}{{\rEfLiNK{x1-61005r25}{\csname :autoref\endcsname{minipage}11.25}}{\rEfLiNK{x1-61005r25}{\csname :autoref\endcsname{minipage}172}}{\rEfLiNK{x1-61005r25}{\csname :autoref\endcsname{minipage}The experimental setup shown in this figure is nearly identical to that of Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:3magnets1}. The only difference is a slight offset of the initial position. The first stage of the resulting trajectory is nearly identical to that of Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:3magnets1}. The bob makes a circular motion towards the second magnet and decelerates. But after passing the second magnet, its course diverges from the first experiment. It moves towards the first magnet, to the third magnet and then back to the first magnet before coming to a standstill. Thus, the slight difference in initial position has produced a completely different end result.\relax }}{minipage.1}{}}
\newlabel{fig:3magnets}{{\rEfLiNK{x1-61007r26}{\csname :autoref\endcsname{minipage}11.26}}{\rEfLiNK{x1-61007r26}{\csname :autoref\endcsname{minipage}172}}{\rEfLiNK{x1-61007r26}{\csname :autoref\endcsname{minipage}This intricate picture paints the initial positions of the magnetic pendulum experiment according to its outcomes. White, grey and black pixels in this $512\times {512}$ image mark initial positions that resulted in a final position at the first, second and third magnet, respectively. The resulting pattern is simple in the immediate vicinity of the magnets, but complex further away. It has all the characteristics of a fractal, exhibiting the same level of complexity regardless of scale. The pattern is determinisitic in the sense that the same grid of initial conditions produces exactly the same pattern. But it is chaotic because even tiny changes in the initial position or velocity may produce completely different patterns. \relax }}{minipage.1}{}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:unsupervised}{{\rEfLiNK{x1-6200012}{\csname :autoref\endcsname{chapter}12}}{\rEfLiNK{x1-6200012}{\csname :autoref\endcsname{chapter}175}}{\rEfLiNK{x1-6200012}{\csname :autoref\endcsname{chapter}Unsupervised learning}}{chapter.1}{}}
\newlabel{sec:PCA}{{\rEfLiNK{x1-6300012.1}{\csname :autoref\endcsname{section}12.1}}{\rEfLiNK{x1-6300012.1}{\csname :autoref\endcsname{section}175}}{\rEfLiNK{x1-6300012.1}{\csname :autoref\endcsname{section}Principal Component Analysis}}{section.1}{}}
\newlabel{eq:X}{{\rEfLiNK{x1-63001r1}{\csname :autoref\endcsname{equation}12.1}}{\rEfLiNK{x1-63001r1}{\csname :autoref\endcsname{equation}175}}{\rEfLiNK{x1-63001r1}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{fig:PCA2Ddata}{{\rEfLiNK{x1-63002r1}{\csname :autoref\endcsname{minipage}12.1}}{\rEfLiNK{x1-63002r1}{\csname :autoref\endcsname{minipage}175}}{\rEfLiNK{x1-63002r1}{\csname :autoref\endcsname{minipage}Three bivariate data points that can be visualised on a two-dimensional scatter plot. There is no need to use unsupervised learning in this case. But we will use this simple dataset as a toy example to understand how Principal Component Analysis works.\vspace  \medskipamount \relax }}{minipage.1}{}}
\newlabel{eq:XMY}{{\rEfLiNK{x1-63005r2}{\csname :autoref\endcsname{equation}12.2}}{\rEfLiNK{x1-63005r2}{\csname :autoref\endcsname{equation}176}}{\rEfLiNK{x1-63005r2}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:eigen}{{\rEfLiNK{x1-63010r4}{\csname :autoref\endcsname{equation}12.4}}{\rEfLiNK{x1-63010r4}{\csname :autoref\endcsname{equation}177}}{\rEfLiNK{x1-63010r4}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:P}{{\rEfLiNK{x1-63012r5}{\csname :autoref\endcsname{equation}12.5}}{\rEfLiNK{x1-63012r5}{\csname :autoref\endcsname{equation}177}}{\rEfLiNK{x1-63012r5}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{fig:PCA2D1}{{\rEfLiNK{x1-63013r2}{\csname :autoref\endcsname{minipage}12.2}}{\rEfLiNK{x1-63013r2}{\csname :autoref\endcsname{minipage}178}}{\rEfLiNK{x1-63013r2}{\csname :autoref\endcsname{minipage}PCA decomposition of Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:PCA2Ddata}. The data $X$ are shown as numbers, $M$ as a square, and $1_{2,1}M \pm Q \sqrt  {\Lambda }$ as a cross. The first principal direction (running from the upper left to the lower right) has been stretched by a factor of $\sqrt  {\Lambda _2/\Lambda _1} = 3.5$ w.r.t the second principal direction, which runs perpendicular to it.\vspace  \medskipamount \relax }}{minipage.1}{}}
\newlabel{fig:PCA2D2}{{\rEfLiNK{x1-63015r3}{\csname :autoref\endcsname{minipage}12.3}}{\rEfLiNK{x1-63015r3}{\csname :autoref\endcsname{minipage}178}}{\rEfLiNK{x1-63015r3}{\csname :autoref\endcsname{minipage}Projection of the three data points on the two principal directions yields two principal components ($P$ in Equation\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {eq:P}), representing one-dimensional representations of the two-dimensional data.\vspace  \medskipamount \relax }}{minipage.1}{}}
\newlabel{fig:PCA2D3}{{\rEfLiNK{x1-63017r4}{\csname :autoref\endcsname{minipage}12.4}}{\rEfLiNK{x1-63017r4}{\csname :autoref\endcsname{minipage}178}}{\rEfLiNK{x1-63017r4}{\csname :autoref\endcsname{minipage}A biplot of both principal components along with the loadings of the two variables shown as arrows. The first principal component shows that the most important difference is that between samples 2 and 3 (which are rich in $a$) and sample\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}1 (which is poor in $a$). The second principal component captures the remaining variance, with samples\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}1 and 3 being richer in $b$ than sample\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}2. The bottom-left and top-right axes represent the scores and loadings, respectively, after shrinking the former and expanding the latter by a factor of $\sqrt  {n\Lambda }$, where $n=3$ is the number of samples.\vspace  \medskipamount \relax }}{minipage.1}{}}
\newlabel{tab:USArrests}{{\rEfLiNK{x1-63019r1}{\csname :autoref\endcsname{minipage}12.1}}{\rEfLiNK{x1-63019r1}{\csname :autoref\endcsname{minipage}179}}{\rEfLiNK{x1-63019r1}{\csname :autoref\endcsname{minipage}\texttt  {USArrests} is a dataset that is built into the \texttt  {R} programming environment. It contains crime statistics (in arrests per 100,000 residents) for murder, assault and rape in each of the 50 US states in 1973. Also given is the percentage of the population living in urban areas. Thus, \texttt  {USArrests} is a four-column table that cannot readily be visualised on a two-dimensional surface.\relax }}{minipage.1}{}}
\newlabel{fig:USArrests}{{\rEfLiNK{x1-63021r5}{\csname :autoref\endcsname{minipage}12.5}}{\rEfLiNK{x1-63021r5}{\csname :autoref\endcsname{minipage}179}}{\rEfLiNK{x1-63021r5}{\csname :autoref\endcsname{minipage} PCA biplot of American crime statistics. The grey labels mark the different states, whereas the black vectors mark the crimes and the percentage of the population that lives in urban areas. States that have a lot of crime plot on the left hand side of the diagram, states with little crime plot towards the right. Heavily urbanised states plot at the top of the diagram, rural states plot near the bottom.\vspace  \medskipamount \relax }}{minipage.1}{}}
\newlabel{sec:MDS}{{\rEfLiNK{x1-6400012.2}{\csname :autoref\endcsname{section}12.2}}{\rEfLiNK{x1-6400012.2}{\csname :autoref\endcsname{section}180}}{\rEfLiNK{x1-6400012.2}{\csname :autoref\endcsname{section}Multidimensional Scaling}}{section.1}{}}
\newlabel{eq:euclidean}{{\rEfLiNK{x1-64001r6}{\csname :autoref\endcsname{equation}12.6}}{\rEfLiNK{x1-64001r6}{\csname :autoref\endcsname{equation}180}}{\rEfLiNK{x1-64001r6}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:d}{{\rEfLiNK{x1-64002r7}{\csname :autoref\endcsname{equation}12.7}}{\rEfLiNK{x1-64002r7}{\csname :autoref\endcsname{equation}180}}{\rEfLiNK{x1-64002r7}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:m}{{\rEfLiNK{x1-64006r8}{\csname :autoref\endcsname{equation}12.8}}{\rEfLiNK{x1-64006r8}{\csname :autoref\endcsname{equation}181}}{\rEfLiNK{x1-64006r8}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{tab:eurodist}{{\rEfLiNK{x1-64007r2}{\csname :autoref\endcsname{center}12.2}}{\rEfLiNK{x1-64007r2}{\csname :autoref\endcsname{center}181}}{\rEfLiNK{x1-64007r2}{\csname :autoref\endcsname{center}Table of road distances (in km) between European cities. The full dataset comprises 21 cities.\relax }}{center.1}{}}
\newlabel{fig:eurodist}{{\rEfLiNK{x1-64009r6}{\csname :autoref\endcsname{minipage}12.6}}{\rEfLiNK{x1-64009r6}{\csname :autoref\endcsname{minipage}182}}{\rEfLiNK{x1-64009r6}{\csname :autoref\endcsname{minipage}MDS configuration of the European city distance data (Table\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {tab:eurodist}). Cities (such as Lyon and Geneva) that are close together in the real world plot close together on the MDS configuration. And cities (such as Stockholm and Athens) that are far apart in the real world plot on opposite ends of the MDS configuration. Whilst the MDS configuration preserves the distances, it does not preserve the orientation of the cities. In this figure, the y-axis has been flipped, and the city locations are rotated $\sim $$15^\circ $ in a clockwise sense compared to the real map of Europe.\vspace  \medskipamount \relax }}{minipage.1}{}}
\newlabel{fig:Shepard}{{\rEfLiNK{x1-64011r7}{\csname :autoref\endcsname{minipage}12.7}}{\rEfLiNK{x1-64011r7}{\csname :autoref\endcsname{minipage}182}}{\rEfLiNK{x1-64011r7}{\csname :autoref\endcsname{minipage}The Shepard plot of the European city distances shows a good agreement between the true input distances from Table\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {tab:eurodist} (x-axis) and the fitted distances measured on the MDS configuration of Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:eurodist} (y-axis). There are 21 cities in the dataset, resulting in $21\times {20}/2=210$ pairwise distances. Hence there are 210 data points on this scatter plot. Most of the scatter of the data around the best fit line is caused by the fact that the input data are \emph  {road distances}, which do not perfectly agree with the straight line map distances.\vspace  \medskipamount \relax }}{minipage.1}{}}
\newlabel{eq:stress}{{\rEfLiNK{x1-64013r9}{\csname :autoref\endcsname{equation}12.9}}{\rEfLiNK{x1-64013r9}{\csname :autoref\endcsname{equation}182}}{\rEfLiNK{x1-64013r9}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{tab:S}{{\rEfLiNK{x1-64014r3}{\csname :autoref\endcsname{center}12.3}}{\rEfLiNK{x1-64014r3}{\csname :autoref\endcsname{center}183}}{\rEfLiNK{x1-64014r3}{\csname :autoref\endcsname{center}Rule of thumb for interpreting the goodness of fit of an MDS configuration.\relax }}{center.1}{}}
\newlabel{eq:DZd}{{\rEfLiNK{x1-64016r10}{\csname :autoref\endcsname{equation}12.10}}{\rEfLiNK{x1-64016r10}{\csname :autoref\endcsname{equation}184}}{\rEfLiNK{x1-64016r10}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{fig:DZmds}{{\rEfLiNK{x1-64017r8}{\csname :autoref\endcsname{minipage}12.8}}{\rEfLiNK{x1-64017r8}{\csname :autoref\endcsname{minipage}184}}{\rEfLiNK{x1-64017r8}{\csname :autoref\endcsname{minipage}MDS configuration of the detrital zircon U--Pb data. Samples that have similar age distributions (such as `Y' and `T') are characterised by low K-S statistics (e.g., $d[Y,T]=0.07$) and plot close together. Samples that have greatly differing age distributions (such as `L' and `8') are characterised by high K-S statistics (e.g., $d[L,8]=0.63$) and plot far apart on the MDS map.\vspace  \medskipamount \relax }}{minipage.1}{}}
\newlabel{sec:kmeans}{{\rEfLiNK{x1-6500012.3}{\csname :autoref\endcsname{section}12.3}}{\rEfLiNK{x1-6500012.3}{\csname :autoref\endcsname{section}184}}{\rEfLiNK{x1-6500012.3}{\csname :autoref\endcsname{section}K-means clustering}}{section.1}{}}
\newlabel{fig:kmeans1}{{\rEfLiNK{x1-65001r9}{\csname :autoref\endcsname{minipage}12.9}}{\rEfLiNK{x1-65001r9}{\csname :autoref\endcsname{minipage}184}}{\rEfLiNK{x1-65001r9}{\csname :autoref\endcsname{minipage}A two-dimensional dataset to illustrate the k-means clustering algorithm. There are 150 data points. In this first exercise we will try to classify them into three groups.\relax }}{minipage.1}{}}
\newlabel{fig:kmeans2}{{\rEfLiNK{x1-65004r10}{\csname :autoref\endcsname{minipage}12.10}}{\rEfLiNK{x1-65004r10}{\csname :autoref\endcsname{minipage}185}}{\rEfLiNK{x1-65004r10}{\csname :autoref\endcsname{minipage}Data points 48, 100 and 130 were randomly selected from the dataset and assigned as the centroids of clusters 1 (circle), 2 (triangle) and 3 (cross).\relax }}{minipage.1}{}}
\newlabel{fig:kmeans3}{{\rEfLiNK{x1-65007r11}{\csname :autoref\endcsname{minipage}12.11}}{\rEfLiNK{x1-65007r11}{\csname :autoref\endcsname{minipage}185}}{\rEfLiNK{x1-65007r11}{\csname :autoref\endcsname{minipage}Replace each of the question marks in Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:kmeans2} with the symbol (circle, triangle or cross) that is closest to it, using the Euclidean distance of Equation\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {eq:euclidean}.\relax }}{minipage.1}{}}
\newlabel{fig:kmeans4}{{\rEfLiNK{x1-65010r12}{\csname :autoref\endcsname{minipage}12.12}}{\rEfLiNK{x1-65010r12}{\csname :autoref\endcsname{minipage}185}}{\rEfLiNK{x1-65010r12}{\csname :autoref\endcsname{minipage}The grey symbols are the same as the black symbols in Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:kmeans3}. The black symbols are the average $\{x,y\}$-positions of all the samples within each cluster. These are different than the previous values shown in Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:kmeans2}. The new values form the centroids of the clusters that will be used in the next iteration of the k-means algorithm.\relax }}{minipage.1}{}}
\newlabel{fig:kmeans5}{{\rEfLiNK{x1-65013r13}{\csname :autoref\endcsname{minipage}12.13}}{\rEfLiNK{x1-65013r13}{\csname :autoref\endcsname{minipage}185}}{\rEfLiNK{x1-65013r13}{\csname :autoref\endcsname{minipage}Final classification of the data. Each of the 150 data points has been assigned to one of the three clusters whose centroids are marked as large and bold symbols.\relax }}{minipage.1}{}}
\newlabel{fig:Iris}{{\rEfLiNK{x1-65015r14}{\csname :autoref\endcsname{minipage}12.14}}{\rEfLiNK{x1-65015r14}{\csname :autoref\endcsname{minipage}186}}{\rEfLiNK{x1-65015r14}{\csname :autoref\endcsname{minipage}Two-dimensional marginal distributions of R.A. Fisher's iris dataset, which comprises four variables measured in 150 different flowers belonging to three different species: \emph  {setosa} (circles), \emph  {versicolor} (triangles) and \emph  {virginica} (crosses). The two-dimensional dataset of Figures\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:kmeans1}--\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:kmeans5} was derived from panel (4,2), which sets out petal width against sepal width. The classification shown in Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:kmeans5} does a decent job at classifying the 150 flowers into three groups but the classification is not perfect. For example, the flower in the lower left corner of panel (4,2) belongs to \emph  {setosa} but was incorrectly classified as \emph  {versicolor} in Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:kmeans5}. \vspace  \medskipamount \relax }}{minipage.1}{}}
\newlabel{tab:kmeansIris}{{\rEfLiNK{x1-65017r4}{\csname :autoref\endcsname{center}12.4}}{\rEfLiNK{x1-65017r4}{\csname :autoref\endcsname{center}186}}{\rEfLiNK{x1-65017r4}{\csname :autoref\endcsname{center}Classification results of the k-means algorithm applied to Fisher's iris data.\relax }}{center.1}{}}
\newlabel{sec:hierarchical}{{\rEfLiNK{x1-6600012.4}{\csname :autoref\endcsname{section}12.4}}{\rEfLiNK{x1-6600012.4}{\csname :autoref\endcsname{section}187}}{\rEfLiNK{x1-6600012.4}{\csname :autoref\endcsname{section}Hierarchical clustering}}{section.1}{}}
\newlabel{fig:hierarchical1}{{\rEfLiNK{x1-66001r15}{\csname :autoref\endcsname{minipage}12.15}}{\rEfLiNK{x1-66001r15}{\csname :autoref\endcsname{minipage}187}}{\rEfLiNK{x1-66001r15}{\csname :autoref\endcsname{minipage}A simple bivariate dataset that will be used to illustrate the hierarchical clustering algorithm:\relax }}{minipage.1}{}}
\newlabel{it:hierachical1}{{\rEfLiNK{x1-660031}{\csname :autoref\endcsname{enumerate}1}}{\rEfLiNK{x1-660031}{\csname :autoref\endcsname{enumerate}187}}{\rEfLiNK{x1-660031}{\csname :autoref\endcsname{enumerate}A simple bivariate dataset that will be used to illustrate the hierarchical clustering algorithm:\relax }}{enumerate.1}{}}
\newlabel{fig:hierarchical2}{{\rEfLiNK{x1-66005r16}{\csname :autoref\endcsname{minipage}12.16}}{\rEfLiNK{x1-66005r16}{\csname :autoref\endcsname{minipage}188}}{\rEfLiNK{x1-66005r16}{\csname :autoref\endcsname{minipage}First step of the hierarchical clustering process. The closest two samples (1 and 3) have been grouped together into a new cluster (grey line, left). The results can also be visualised as a tree or \textbf  {dendrogram} (right).\vspace  \medskipamount \relax }}{minipage.1}{}}
\newlabel{it:hierarchical2}{{\rEfLiNK{x1-660073}{\csname :autoref\endcsname{enumerate}3}}{\rEfLiNK{x1-660073}{\csname :autoref\endcsname{enumerate}188}}{\rEfLiNK{x1-660073}{\csname :autoref\endcsname{enumerate}First step of the hierarchical clustering process. The closest two samples (1 and 3) have been grouped together into a new cluster (grey line, left). The results can also be visualised as a tree or \textbf  {dendrogram} (right).\vspace  \medskipamount \relax }}{enumerate.1}{}}
\newlabel{fig:hierarchical3}{{\rEfLiNK{x1-66009r17}{\csname :autoref\endcsname{minipage}12.17}}{\rEfLiNK{x1-66009r17}{\csname :autoref\endcsname{minipage}189}}{\rEfLiNK{x1-66009r17}{\csname :autoref\endcsname{minipage}The second step of the hierarchical clustering process shown as a scatter plot (left) and a dendrogram (right). The first order cluster is nested inside the second order one.\vspace  \medskipamount \relax }}{minipage.1}{}}
\newlabel{it:hierarchical3}{{\rEfLiNK{x1-660115}{\csname :autoref\endcsname{enumerate}5}}{\rEfLiNK{x1-660115}{\csname :autoref\endcsname{enumerate}189}}{\rEfLiNK{x1-660115}{\csname :autoref\endcsname{enumerate}The second step of the hierarchical clustering process shown as a scatter plot (left) and a dendrogram (right). The first order cluster is nested inside the second order one.\vspace  \medskipamount \relax }}{enumerate.1}{}}
\newlabel{fig:hierarchical4}{{\rEfLiNK{x1-66013r18}{\csname :autoref\endcsname{minipage}12.18}}{\rEfLiNK{x1-66013r18}{\csname :autoref\endcsname{minipage}190}}{\rEfLiNK{x1-66013r18}{\csname :autoref\endcsname{minipage}The third step of the hierarchical clustering process shown as a scatter plot (left) and a dendrogram (right). The third cluster does not share any elements with the first two clusters.\vspace  \medskipamount \relax }}{minipage.1}{}}
\newlabel{tab:hclustIris}{{\rEfLiNK{x1-66022r5}{\csname :autoref\endcsname{center}12.5}}{\rEfLiNK{x1-66022r5}{\csname :autoref\endcsname{center}191}}{\rEfLiNK{x1-66022r5}{\csname :autoref\endcsname{center}Classification results of the hierarchical clustering algorithm applied to Fisher's iris data.\relax }}{center.1}{}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:supervised}{{\rEfLiNK{x1-6700013}{\csname :autoref\endcsname{chapter}13}}{\rEfLiNK{x1-6700013}{\csname :autoref\endcsname{chapter}193}}{\rEfLiNK{x1-6700013}{\csname :autoref\endcsname{chapter}Supervised learning}}{chapter.1}{}}
\newlabel{sec:LDA}{{\rEfLiNK{x1-6800013.1}{\csname :autoref\endcsname{section}13.1}}{\rEfLiNK{x1-6800013.1}{\csname :autoref\endcsname{section}193}}{\rEfLiNK{x1-6800013.1}{\csname :autoref\endcsname{section}Discriminant Analysis}}{section.1}{}}
\newlabel{eq:bayesRule}{{\rEfLiNK{x1-68001r1}{\csname :autoref\endcsname{equation}13.1}}{\rEfLiNK{x1-68001r1}{\csname :autoref\endcsname{equation}193}}{\rEfLiNK{x1-68001r1}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:bayesTheorem}{{\rEfLiNK{x1-68002r2}{\csname :autoref\endcsname{equation}13.2}}{\rEfLiNK{x1-68002r2}{\csname :autoref\endcsname{equation}194}}{\rEfLiNK{x1-68002r2}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:multiNorm}{{\rEfLiNK{x1-68003r3}{\csname :autoref\endcsname{equation}13.3}}{\rEfLiNK{x1-68003r3}{\csname :autoref\endcsname{equation}194}}{\rEfLiNK{x1-68003r3}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:quadRule}{{\rEfLiNK{x1-68004r4}{\csname :autoref\endcsname{equation}13.4}}{\rEfLiNK{x1-68004r4}{\csname :autoref\endcsname{equation}194}}{\rEfLiNK{x1-68004r4}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{fig:QDA}{{\rEfLiNK{x1-68005r1}{\csname :autoref\endcsname{document}13.1}}{\rEfLiNK{x1-68005r1}{\csname :autoref\endcsname{document}195}}{\rEfLiNK{x1-68005r1}{\csname :autoref\endcsname{document}Quadratic discriminant analysis of synthetic data: a) the bivariate training data comprise three classes that are marked by crosses, circles and triangles, respectively; b) fitting three bivariate normal distributions to the data produces c) curved decision boundaries between the three distributions.\\\relax }}{document.1}{}}
\newlabel{eq:linRule}{{\rEfLiNK{x1-68007r5}{\csname :autoref\endcsname{equation}13.5}}{\rEfLiNK{x1-68007r5}{\csname :autoref\endcsname{equation}195}}{\rEfLiNK{x1-68007r5}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{fig:LDA}{{\rEfLiNK{x1-68008r2}{\csname :autoref\endcsname{document}13.2}}{\rEfLiNK{x1-68008r2}{\csname :autoref\endcsname{document}195}}{\rEfLiNK{x1-68008r2}{\csname :autoref\endcsname{document}Linear discriminant analysis of a second synthetic dataset: a) the bivariate training data comprise the same three classes as Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:QDA}; b) fitting three bivariate normal distributions to the data with the same covariance matrix c) produces linear decision boundaries between the three distributions.\\\relax }}{document.1}{}}
\newlabel{fig:PCAvsLDA}{{\rEfLiNK{x1-68010r3}{\csname :autoref\endcsname{document}13.3}}{\rEfLiNK{x1-68010r3}{\csname :autoref\endcsname{document}196}}{\rEfLiNK{x1-68010r3}{\csname :autoref\endcsname{document}a) an equal mixture of two bivariate normal datasets; b) PCA extracts the major axis of the best fitting error ellipse to the merged dataset as the first principal component; c) LDA fits two error ellipses to the data and extracts a function (the first linear discriminant) that maximises the distance between them. In this example, this produces a line that is perpendicular to the first principal component.\\\relax }}{document.1}{}}
\newlabel{fig:LDAiris}{{\rEfLiNK{x1-68012r4}{\csname :autoref\endcsname{minipage}13.4}}{\rEfLiNK{x1-68012r4}{\csname :autoref\endcsname{minipage}196}}{\rEfLiNK{x1-68012r4}{\csname :autoref\endcsname{minipage}The first two linear discriminants of the four-dimensional iris data represent a two-dimensional projection of these data that maximises the differences between the three different species of flowers. They are defined as\\ LD1 = 0.83 $\times $ (sepal length - 5.84) + 1.53 $\times $ (sepal width - 3.06) - 2.20 $\times $ (petal length - 3.76) - 2.81 $\times $ (petal width - 1.20); and\\ LD2 = 0.024 $\times $ (sepal length - 5.85) + 2.16 $\times $ (sepal width - 3.06) - 0.93 $\times $ (petal length - 3.76) + 2.84 $\times $ (petal width - 1.20).\\\relax }}{minipage.1}{}}
\newlabel{fig:LDAnewiris}{{\rEfLiNK{x1-68014r5}{\csname :autoref\endcsname{minipage}13.5}}{\rEfLiNK{x1-68014r5}{\csname :autoref\endcsname{minipage}196}}{\rEfLiNK{x1-68014r5}{\csname :autoref\endcsname{minipage} Using the two discriminant functions of Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:LDAiris}:\\ LD1 = $0.83\times (6.0-5.84)+1.53\times (3.0-3.06)-2.20\times (5.0-3.76)-2.81\times (1.5-1.20)=-3.53$; and\\ LD2 = $0.024\times (6.0-5.85)+2.16\times (3.0-3.06)-0.93\times (5.0-3.76)+2.84\times (1.5-1.20)=-0.42$.\\ Plotting these two coordinates on the discrimination diagram of Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:LDAiris} suggests that the new flower belongs to the \textit  {versicolor} species. \\\relax }}{minipage.1}{}}
\newlabel{sec:CART}{{\rEfLiNK{x1-6900013.2}{\csname :autoref\endcsname{section}13.2}}{\rEfLiNK{x1-6900013.2}{\csname :autoref\endcsname{section}197}}{\rEfLiNK{x1-6900013.2}{\csname :autoref\endcsname{section}Decision trees}}{section.1}{}}
\newlabel{fig:CARTdata}{{\rEfLiNK{x1-69001r6}{\csname :autoref\endcsname{minipage}13.6}}{\rEfLiNK{x1-69001r6}{\csname :autoref\endcsname{minipage}197}}{\rEfLiNK{x1-69001r6}{\csname :autoref\endcsname{minipage}Synthetic dataset of bivariate measurements that belong to two classes. The black circles are split into two data clouds. The white circles form a `c'-shape surrounding one of the modes of the black population. Discriminant analysis is unable to deal with this situation.\\\relax }}{minipage.1}{}}
\newlabel{eq:Q}{{\rEfLiNK{x1-69003r6}{\csname :autoref\endcsname{equation}13.6}}{\rEfLiNK{x1-69003r6}{\csname :autoref\endcsname{equation}198}}{\rEfLiNK{x1-69003r6}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{fig:Qtries}{{\rEfLiNK{x1-69004r7}{\csname :autoref\endcsname{document}13.7}}{\rEfLiNK{x1-69004r7}{\csname :autoref\endcsname{document}198}}{\rEfLiNK{x1-69004r7}{\csname :autoref\endcsname{document}Evaluation of three candidate split points using the Gini index $Q$. The best first order split corresponds to $x=-4.902$ (panel c).\\\relax }}{document.1}{}}
\newlabel{fig:Q2}{{\rEfLiNK{x1-69006r8}{\csname :autoref\endcsname{minipage}13.8}}{\rEfLiNK{x1-69006r8}{\csname :autoref\endcsname{minipage}198}}{\rEfLiNK{x1-69006r8}{\csname :autoref\endcsname{minipage}Second order partition of the right hand side of Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:Qtries}.c). The results can be visualised as a dendrogram or tree (right). The `leaves' of this tree are annotated as $n_\circ /n_\bullet $ where $n_\circ $ is the number of training data of class $\circ $ and $n_\bullet $ is the number of training data of class $\bullet $.\\\relax }}{minipage.1}{}}
\newlabel{fig:overfittedCARTscatter}{{\rEfLiNK{x1-69008r9}{\csname :autoref\endcsname{minipage}13.9}}{\rEfLiNK{x1-69008r9}{\csname :autoref\endcsname{minipage}199}}{\rEfLiNK{x1-69008r9}{\csname :autoref\endcsname{minipage}A recursive binary partition tree that perfectly classifies all the samples in Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:CARTdata}. The annotations of the dendrogram have been removed to reduce clutter. \\\relax }}{minipage.1}{}}
\newlabel{it:cv1}{{\rEfLiNK{x1-690112}{\csname :autoref\endcsname{enumerate}2}}{\rEfLiNK{x1-690112}{\csname :autoref\endcsname{enumerate}199}}{\rEfLiNK{x1-690112}{\csname :autoref\endcsname{enumerate}A recursive binary partition tree that perfectly classifies all the samples in Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:CARTdata}. The annotations of the dendrogram have been removed to reduce clutter. \\\relax }}{enumerate.1}{}}
\newlabel{it:cv2}{{\rEfLiNK{x1-690123}{\csname :autoref\endcsname{enumerate}3}}{\rEfLiNK{x1-690123}{\csname :autoref\endcsname{enumerate}199}}{\rEfLiNK{x1-690123}{\csname :autoref\endcsname{enumerate}A recursive binary partition tree that perfectly classifies all the samples in Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:CARTdata}. The annotations of the dendrogram have been removed to reduce clutter. \\\relax }}{enumerate.1}{}}
\newlabel{eq:cp}{{\rEfLiNK{x1-69015r7}{\csname :autoref\endcsname{equation}13.7}}{\rEfLiNK{x1-69015r7}{\csname :autoref\endcsname{equation}199}}{\rEfLiNK{x1-69015r7}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:Ta}{{\rEfLiNK{x1-69016r8}{\csname :autoref\endcsname{equation}13.8}}{\rEfLiNK{x1-69016r8}{\csname :autoref\endcsname{equation}200}}{\rEfLiNK{x1-69016r8}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{fig:cvCART}{{\rEfLiNK{x1-69017r10}{\csname :autoref\endcsname{minipage}13.10}}{\rEfLiNK{x1-69017r10}{\csname :autoref\endcsname{minipage}200}}{\rEfLiNK{x1-69017r10}{\csname :autoref\endcsname{minipage}A plot of cross validated (CV) prediction error versus the number of nodes in the collection of nested subtrees shows a minimum at five splits. The CV error of small trees is caused by bias, whereas the CV error of large tree is a result of variance. There typically exist several trees with CV errors close to the minimum. Therefore, a `1-SE rule' is used, i.e., choosing the smallest tree whose misclassification rate does not exceed the minimum CV error plus one standard error of the smallest CV error (dotted line).\\\relax }}{minipage.1}{}}
\newlabel{fig:optimalCART}{{\rEfLiNK{x1-69019r11}{\csname :autoref\endcsname{minipage}13.11}}{\rEfLiNK{x1-69019r11}{\csname :autoref\endcsname{minipage}201}}{\rEfLiNK{x1-69019r11}{\csname :autoref\endcsname{minipage}The optimal tree for the bivariate test data. This tree misclassifies 19 of the 300 samples in the training data (6.3\%). The 10-fold cross validation error is 18\%.\\\relax }}{minipage.1}{}}
\newlabel{fig:irisCART}{{\rEfLiNK{x1-69021r12}{\csname :autoref\endcsname{minipage}13.12}}{\rEfLiNK{x1-69021r12}{\csname :autoref\endcsname{minipage}201}}{\rEfLiNK{x1-69021r12}{\csname :autoref\endcsname{minipage}The optimal tree for Fisher's iris data. There are four variables (instead of two for the previous example) and three classes (instead of two for the previous example). The optimal tree misclassifies 6 of the 150 flowers in the training data (4\%). The 10-fold cross validation error is 6\%.\\\relax }}{minipage.1}{}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:compositional}{{\rEfLiNK{x1-7000014}{\csname :autoref\endcsname{chapter}14}}{\rEfLiNK{x1-7000014}{\csname :autoref\endcsname{chapter}203}}{\rEfLiNK{x1-7000014}{\csname :autoref\endcsname{chapter}Compositional data}}{chapter.1}{}}
\newlabel{sec:ratios}{{\rEfLiNK{x1-7100014.1}{\csname :autoref\endcsname{section}14.1}}{\rEfLiNK{x1-7100014.1}{\csname :autoref\endcsname{section}203}}{\rEfLiNK{x1-7100014.1}{\csname :autoref\endcsname{section}Ratios of Jeffreys quantities}}{section.1}{}}
\newlabel{sec:logratios}{{\rEfLiNK{x1-7200014.2}{\csname :autoref\endcsname{section}14.2}}{\rEfLiNK{x1-7200014.2}{\csname :autoref\endcsname{section}205}}{\rEfLiNK{x1-7200014.2}{\csname :autoref\endcsname{section}Logratio transformations}}{section.1}{}}
\newlabel{fig:ACNK}{{\rEfLiNK{x1-72001r1}{\csname :autoref\endcsname{minipage}14.1}}{\rEfLiNK{x1-72001r1}{\csname :autoref\endcsname{minipage}206}}{\rEfLiNK{x1-72001r1}{\csname :autoref\endcsname{minipage}The A--CN--K diagram is a widely used graphical device in chemical weathering studies. It is a ternary diagram of Al\textsubscript  {2}O\textsubscript  {3}, CaO + Na\textsubscript  {2}O and K\textsubscript  {2}O, where the CaO refers to the silicate component of the sediment only (ignoring carbonates). The composition on the diagram results from the competing effects of initial starting composition and chemical weathering. With increasing weathering intensity, A--CN--K compositions get pulled towards the Al\textsubscript  {2}O\textsubscript  {3} apex of the ternary diagram. This figure shows a synthetic dataset of 20 A--CN--K measurements that have been affected by variable weathering intensities.\vspace  \medskipamount \relax }}{minipage.1}{}}
\newlabel{fig:ACNKarithmeticmean}{{\rEfLiNK{x1-72003r2}{\csname :autoref\endcsname{minipage}14.2}}{\rEfLiNK{x1-72003r2}{\csname :autoref\endcsname{minipage}207}}{\rEfLiNK{x1-72003r2}{\csname :autoref\endcsname{minipage} The black square represents the average A--CN--K composition of the 20 samples of Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:ACNK}. It is obtained by taking the arithmetic mean of all the Al\textsubscript  {2}O\textsubscript  {3}, (CaO + Na\textsubscript  {2}O) and K\textsubscript  {2}O concentrations using Equation\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {eq:mean}, and plotting the resulting 3-element vector as a new sample. This average composition plots outside the sample cloud, which is a meaningless result not unlike the vegetation example of Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:vegetationlocation}.a.\vspace  \medskipamount \relax }}{minipage.1}{}}
\newlabel{fig:ACNKnaive}{{\rEfLiNK{x1-72005r3}{\csname :autoref\endcsname{minipage}14.3}}{\rEfLiNK{x1-72005r3}{\csname :autoref\endcsname{minipage}208}}{\rEfLiNK{x1-72005r3}{\csname :autoref\endcsname{minipage}The polygon represents a `2-sigma confidence region' for the arithmetic mean of Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:ACNKarithmeticmean}. It is obtained by 1) calculating the standard deviations of the Al\textsubscript  {2}O\textsubscript  {3}, (CaO + Na\textsubscript  {2}O) and K\textsubscript  {2}O concentrations using Equation\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {eq:stdev}; 2) multiplying these values by two; and 3) subtracting or adding these values to the arithmetic mean of Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:ACNKarithmeticmean}. The resulting `2-sigma' bounds plot outside the ternary diagram, in physically impossible negative data space.\vspace  \medskipamount \relax }}{minipage.1}{}}
\newlabel{eq:alr}{{\rEfLiNK{x1-72007r1}{\csname :autoref\endcsname{equation}14.1}}{\rEfLiNK{x1-72007r1}{\csname :autoref\endcsname{equation}208}}{\rEfLiNK{x1-72007r1}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:inverse-logratio-transformation}{{\rEfLiNK{x1-72008r2}{\csname :autoref\endcsname{equation}14.2}}{\rEfLiNK{x1-72008r2}{\csname :autoref\endcsname{equation}209}}{\rEfLiNK{x1-72008r2}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{fig:alr}{{\rEfLiNK{x1-72009r4}{\csname :autoref\endcsname{document}14.4}}{\rEfLiNK{x1-72009r4}{\csname :autoref\endcsname{document}209}}{\rEfLiNK{x1-72009r4}{\csname :autoref\endcsname{document}The additive logratio transformation (Equation\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {eq:alr}) maps data from an $n$-dimensional compositional space to an $(n-1)$-dimensional Euclidean space. For the A--CN--K data, it maps the data from a ternary diagram ($n=3$) to a bivariate ($n-1=2$) dataspace using Equation\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {eq:alr}. In this transformed space, it is safe to calculate arithmetic means (square) and confidence regions (ellipse). After completion of these calculations, the result can be mapped back to the ternary diagram using the inverse logratio transformation (Equation\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {eq:inverse-logratio-transformation}).\vspace  \medskipamount \relax }}{document.1}{}}
\newlabel{sec:compositionalPCA}{{\rEfLiNK{x1-7300014.3}{\csname :autoref\endcsname{section}14.3}}{\rEfLiNK{x1-7300014.3}{\csname :autoref\endcsname{section}210}}{\rEfLiNK{x1-7300014.3}{\csname :autoref\endcsname{section}PCA of compositional data}}{section.1}{}}
\newlabel{tab:Major}{{\rEfLiNK{x1-73001r1}{\csname :autoref\endcsname{center}14.1}}{\rEfLiNK{x1-73001r1}{\csname :autoref\endcsname{center}210}}{\rEfLiNK{x1-73001r1}{\csname :autoref\endcsname{center}The major element composition (in weight percent) of 16 samples of Namib dune sand.\relax }}{center.1}{}}
\newlabel{eq:Xcomp}{{\rEfLiNK{x1-73003r3}{\csname :autoref\endcsname{equation}14.3}}{\rEfLiNK{x1-73003r3}{\csname :autoref\endcsname{equation}210}}{\rEfLiNK{x1-73003r3}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{fig:abc}{{\rEfLiNK{x1-73004r5}{\csname :autoref\endcsname{minipage}14.5}}{\rEfLiNK{x1-73004r5}{\csname :autoref\endcsname{minipage}211}}{\rEfLiNK{x1-73004r5}{\csname :autoref\endcsname{minipage}Ternary diagram of the synthetic toy example of Equation\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {eq:Xcomp}. Component $c$ has been multiplied by a factor of three to avoid an unsightly overlap between the plot symbols of samples 2 and 3, whose compositions are very similar.\vspace  \medskipamount \relax }}{minipage.1}{}}
\newlabel{fig:alraxes}{{\rEfLiNK{x1-73006r6}{\csname :autoref\endcsname{minipage}14.6}}{\rEfLiNK{x1-73006r6}{\csname :autoref\endcsname{minipage}211}}{\rEfLiNK{x1-73006r6}{\csname :autoref\endcsname{minipage}There are six different ways to form logratios in the ternary data space. These six logratios define three coordinate axes that intersect each other at 60$^\circ $ angles. As a consequence, distances in alr-space depend on the choice of logratios.\vspace  \medskipamount \relax }}{minipage.1}{}}
\newlabel{eq:clr}{{\rEfLiNK{x1-73008r4}{\csname :autoref\endcsname{equation}14.4}}{\rEfLiNK{x1-73008r4}{\csname :autoref\endcsname{equation}212}}{\rEfLiNK{x1-73008r4}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:Xc}{{\rEfLiNK{x1-73009r5}{\csname :autoref\endcsname{equation}14.5}}{\rEfLiNK{x1-73009r5}{\csname :autoref\endcsname{equation}212}}{\rEfLiNK{x1-73009r5}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:PCAcomp}{{\rEfLiNK{x1-73010r6}{\csname :autoref\endcsname{equation}14.6}}{\rEfLiNK{x1-73010r6}{\csname :autoref\endcsname{equation}213}}{\rEfLiNK{x1-73010r6}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{fig:clrPCA}{{\rEfLiNK{x1-73012r7}{\csname :autoref\endcsname{minipage}14.7}}{\rEfLiNK{x1-73012r7}{\csname :autoref\endcsname{minipage}213}}{\rEfLiNK{x1-73012r7}{\csname :autoref\endcsname{minipage}Compositional biplot of the toy data using the clr transformation. The biplot tells us that sample\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}1 is rich in component $b$, whereas samples\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}2 and 3 are rich in component $a$. The difference between samples\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}2 and 3 is due to a small difference in component $c$. Compare with the ternary diagram (Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:abc}) to verify these conclusions.\vspace  \medskipamount \relax }}{minipage.1}{}}
\newlabel{fig:majorPCA}{{\rEfLiNK{x1-73014r8}{\csname :autoref\endcsname{minipage}14.8}}{\rEfLiNK{x1-73014r8}{\csname :autoref\endcsname{minipage}214}}{\rEfLiNK{x1-73014r8}{\csname :autoref\endcsname{minipage}Compositional biplot of the major element concentration data of Table\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {tab:Major}, with the samples shown in grey and the components in black. Samples that plot close together (such as N1 and N14) are compositionally similar, whereas samples that plot far apart (such as N8 and T8) are compositionally different. One should be careful not to interpret the individual rays of compositional biplots. For example, one cannot safely conclude that N8 is enriched in MnO relative to T8. However, it is safe to say that N8 has a higher MnO/Na\textsubscript  {2}O-ratio than T8. \vspace  \medskipamount \relax }}{minipage.1}{}}
\newlabel{fig:links}{{\rEfLiNK{x1-73016r9}{\csname :autoref\endcsname{center}14.9}}{\rEfLiNK{x1-73016r9}{\csname :autoref\endcsname{center}214}}{\rEfLiNK{x1-73016r9}{\csname :autoref\endcsname{center}Subcompositional analysis of Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:majorPCA}. a) Box plots of the CaO/MgO-logratio (short link) and the K\textsubscript  {2}O/MgO-logratio (long link); b) The collinearity of the links between TiO\textsubscript  {2}, CaO, MnO and MgO indicates strong correlation of the samples in ln[MnO/MgO] -- ln[TiO\textsubscript  {2}/CaO] space; c) The link between P\textsubscript  {2}O\textsubscript  {5} and TiO\textsubscript  {2} is orthogonal to that between between K\textsubscript  {2}O and CaO, reflecting independence of the corresponding logratios.\relax }}{center.1}{}}
\newlabel{sec:compositionalLDA}{{\rEfLiNK{x1-7400014.4}{\csname :autoref\endcsname{section}14.4}}{\rEfLiNK{x1-7400014.4}{\csname :autoref\endcsname{section}214}}{\rEfLiNK{x1-7400014.4}{\csname :autoref\endcsname{section}LDA of compositional data}}{section.1}{}}
\newlabel{fig:AFM}{{\rEfLiNK{x1-74001r10}{\csname :autoref\endcsname{minipage}14.10}}{\rEfLiNK{x1-74001r10}{\csname :autoref\endcsname{minipage}215}}{\rEfLiNK{x1-74001r10}{\csname :autoref\endcsname{minipage}A dataset of igneous rock compositions from Iceland and the Cascade Mountains on a ternary A--F--M diagram (where A = Na\textsubscript  {2}O+K\textsubscript  {2}O, F = Fe\textsubscript  {2}O\textsubscript  {3}+FeO and M = MgO). The white dots define a `Fenner' trend marking the tholeiitic suite of igneous rocks from Iceland. The black dots define a `Bowen' trend, marking the calc-alkaline suite of rocks from the Cascades.\vspace  \medskipamount \relax }}{minipage.1}{}}
\newlabel{fig:lrAFM}{{\rEfLiNK{x1-74003r11}{\csname :autoref\endcsname{minipage}14.11}}{\rEfLiNK{x1-74003r11}{\csname :autoref\endcsname{minipage}215}}{\rEfLiNK{x1-74003r11}{\csname :autoref\endcsname{minipage}The additive logratio transformation liberates the A--F--M data from the confines of the ternary diagram and maps them to a Euclidean dataspace in which logratios are free to take any value from $-\infty $ to $+\infty $. In this space, the ln(M/F) vs. ln(A/F) values of the tholeiitic and calc-alkali rock suites are clustered into two distinct clouds of roughly equal size that have all the hallmarks of bivariate normal distributions with a shared covariance matrix. Thus the transformed data seems well suited for linear discriminant analysis.\vspace  \medskipamount \relax }}{minipage.1}{}}
\newlabel{fig:LDAAFM}{{\rEfLiNK{x1-74005r12}{\csname :autoref\endcsname{minipage}14.12}}{\rEfLiNK{x1-74005r12}{\csname :autoref\endcsname{minipage}215}}{\rEfLiNK{x1-74005r12}{\csname :autoref\endcsname{minipage}LDA of the A--F--M data shown in a) logratio space and b) a ternary diagram.\vspace  \medskipamount \relax }}{minipage.1}{}}
\newlabel{sec:logratio-processes}{{\rEfLiNK{x1-7500014.5}{\csname :autoref\endcsname{section}14.5}}{\rEfLiNK{x1-7500014.5}{\csname :autoref\endcsname{section}215}}{\rEfLiNK{x1-7500014.5}{\csname :autoref\endcsname{section}Logratio processes}}{section.1}{}}
\newlabel{eq:decay}{{\rEfLiNK{x1-75001r7}{\csname :autoref\endcsname{equation}14.7}}{\rEfLiNK{x1-75001r7}{\csname :autoref\endcsname{equation}216}}{\rEfLiNK{x1-75001r7}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:expdecay}{{\rEfLiNK{x1-75002r8}{\csname :autoref\endcsname{equation}14.8}}{\rEfLiNK{x1-75002r8}{\csname :autoref\endcsname{equation}216}}{\rEfLiNK{x1-75002r8}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{fig:cath}{{\rEfLiNK{x1-75006r13}{\csname :autoref\endcsname{minipage}14.13}}{\rEfLiNK{x1-75006r13}{\csname :autoref\endcsname{minipage}218}}{\rEfLiNK{x1-75006r13}{\csname :autoref\endcsname{minipage}Exponential decay processes of compositional data become linear trends in logratio space. Initial composition $i$ forms the starting point of two trajectories, marked by dashed and solid lines, respectively. These trends are characterised by different decay parameters.\vspace  \medskipamount \relax }}{minipage.1}{}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:directional}{{\rEfLiNK{x1-7600015}{\csname :autoref\endcsname{chapter}15}}{\rEfLiNK{x1-7600015}{\csname :autoref\endcsname{chapter}219}}{\rEfLiNK{x1-7600015}{\csname :autoref\endcsname{chapter}Directional data}}{chapter.1}{}}
\newlabel{sec:circular}{{\rEfLiNK{x1-7700015.1}{\csname :autoref\endcsname{section}15.1}}{\rEfLiNK{x1-7700015.1}{\csname :autoref\endcsname{section}219}}{\rEfLiNK{x1-7700015.1}{\csname :autoref\endcsname{section}Circular data}}{section.1}{}}
\newlabel{fig:circle1}{{\rEfLiNK{x1-77001r1}{\csname :autoref\endcsname{minipage}15.1}}{\rEfLiNK{x1-77001r1}{\csname :autoref\endcsname{minipage}219}}{\rEfLiNK{x1-77001r1}{\csname :autoref\endcsname{minipage}The directions of 30\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}glacial striations. The values are roughly centred around zero but exhibit significant scatter, from the northwest (276$^\circ $) to the northeast (79$^\circ $).\vspace  \medskipamount \relax }}{minipage.1}{}}
\newlabel{fig:circle2}{{\rEfLiNK{x1-77003r2}{\csname :autoref\endcsname{minipage}15.2}}{\rEfLiNK{x1-77003r2}{\csname :autoref\endcsname{minipage}219}}{\rEfLiNK{x1-77003r2}{\csname :autoref\endcsname{minipage}The same glacial striation data of Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:circle1}, with their arithmetic mean shown as an arrow. Even though the individual striations are all pointing north, their arithmetic mean is pointing in exactly the opposite direction.\vspace  \medskipamount \relax }}{minipage.1}{}}
\newlabel{eq:anglediff}{{\rEfLiNK{x1-77005r1}{\csname :autoref\endcsname{equation}15.1}}{\rEfLiNK{x1-77005r1}{\csname :autoref\endcsname{equation}220}}{\rEfLiNK{x1-77005r1}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:averagedirection}{{\rEfLiNK{x1-77006r2}{\csname :autoref\endcsname{equation}15.2}}{\rEfLiNK{x1-77006r2}{\csname :autoref\endcsname{equation}220}}{\rEfLiNK{x1-77006r2}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{fig:circle3}{{\rEfLiNK{x1-77008r3}{\csname :autoref\endcsname{minipage}15.3}}{\rEfLiNK{x1-77008r3}{\csname :autoref\endcsname{minipage}221}}{\rEfLiNK{x1-77008r3}{\csname :autoref\endcsname{minipage}The vector sum of the 30 glacial striation measurements points in a direction of 357.6$^\circ $, which is marked by the arrow and does a much better job at capturing the `central' direction than the arithmetic mean of Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:circle2} does.\vspace  \medskipamount \relax }}{minipage.1}{}}
\newlabel{eq:circularR}{{\rEfLiNK{x1-77010r4}{\csname :autoref\endcsname{equation}15.4}}{\rEfLiNK{x1-77010r4}{\csname :autoref\endcsname{equation}221}}{\rEfLiNK{x1-77010r4}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{fig:vectorsum}{{\rEfLiNK{x1-77011r4}{\csname :autoref\endcsname{minipage}15.4}}{\rEfLiNK{x1-77011r4}{\csname :autoref\endcsname{minipage}221}}{\rEfLiNK{x1-77011r4}{\csname :autoref\endcsname{minipage}An average direction (dashed line) is obtained by taking the vector sum of four angular measurements (thin black arrows). Scaling by the number of measurements (thick black arrow) yields a measure of concentration ($0 \leq \let \prOteCt \relax \let \prOteCt \relax \Protect \csname acp:c\endcsname {18}{R} \leq 1$), which increases in length with decreasing angular dispersion and vice versa.\vspace  \medskipamount \relax }}{minipage.1}{}}
\newlabel{eq:circularSD}{{\rEfLiNK{x1-77013r5}{\csname :autoref\endcsname{equation}15.5}}{\rEfLiNK{x1-77013r5}{\csname :autoref\endcsname{equation}222}}{\rEfLiNK{x1-77013r5}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{fig:lowconcentration}{{\rEfLiNK{x1-77014r5}{\csname :autoref\endcsname{minipage}15.5}}{\rEfLiNK{x1-77014r5}{\csname :autoref\endcsname{minipage}222}}{\rEfLiNK{x1-77014r5}{\csname :autoref\endcsname{minipage}When the directional measurements are greatly dispersed, the normalised vector sum $\let \prOteCt \relax \let \prOteCt \relax \Protect \csname acp:c\endcsname {18}{R}$ is short ($\let \prOteCt \relax \let \prOteCt \relax \Protect \csname acp:c\endcsname {18}{R}=0.125$ in this example), and the circular standard deviation is large ($s_c=4.16$).\vspace  \medskipamount \relax }}{minipage.1}{}}
\newlabel{sec:circular-distributions}{{\rEfLiNK{x1-7800015.2}{\csname :autoref\endcsname{section}15.2}}{\rEfLiNK{x1-7800015.2}{\csname :autoref\endcsname{section}222}}{\rEfLiNK{x1-7800015.2}{\csname :autoref\endcsname{section}Circular distributions}}{section.1}{}}
\newlabel{eq:vonMises}{{\rEfLiNK{x1-78001r6}{\csname :autoref\endcsname{equation}15.6}}{\rEfLiNK{x1-78001r6}{\csname :autoref\endcsname{equation}222}}{\rEfLiNK{x1-78001r6}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{fig:vonMises}{{\rEfLiNK{x1-78002r6}{\csname :autoref\endcsname{document}15.6}}{\rEfLiNK{x1-78002r6}{\csname :autoref\endcsname{document}223}}{\rEfLiNK{x1-78002r6}{\csname :autoref\endcsname{document}Four examples of the von Mises distribution with different values for the mean $\mu $ and the concentration parameter $\kappa $, wrapped around a (grey) circle. The probability density is proportional to the distance from the circle to the black line.\vspace  \medskipamount \relax }}{document.1}{}}
\newlabel{eq:kappa}{{\rEfLiNK{x1-78004r7}{\csname :autoref\endcsname{equation}15.7}}{\rEfLiNK{x1-78004r7}{\csname :autoref\endcsname{equation}223}}{\rEfLiNK{x1-78004r7}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{fig:R2K}{{\rEfLiNK{x1-78005r7}{\csname :autoref\endcsname{minipage}15.7}}{\rEfLiNK{x1-78005r7}{\csname :autoref\endcsname{minipage}223}}{\rEfLiNK{x1-78005r7}{\csname :autoref\endcsname{minipage}$\let \prOteCt \relax \let \prOteCt \relax \Protect \csname acp:c\endcsname {18}{R}$ and $\kappa $ are two concentration parameters for circular data. $\let \prOteCt \relax \let \prOteCt \relax \Protect \csname acp:c\endcsname {18}{R}$ is easier to calculate than $\kappa $ (using Equation\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {eq:circularR}), and can be converted to $\kappa $ in order to fit a von Mises distribution to the data.\vspace  \medskipamount \relax }}{minipage.1}{}}
\newlabel{fig:kappastriations}{{\rEfLiNK{x1-78007r8}{\csname :autoref\endcsname{minipage}15.8}}{\rEfLiNK{x1-78007r8}{\csname :autoref\endcsname{minipage}223}}{\rEfLiNK{x1-78007r8}{\csname :autoref\endcsname{minipage}Density plot and rug plot for the glacial striation data. The bell shaped density curve represents a von Mises distribution with concentration parameter $\kappa =2.8$ and mean $\mu =357.6^{\circ }$. In contrast with Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:vonMises}, in which the von Mises distribution was wrapped around a circle, here it is stretched out along a linear interval from $-\pi $ to $+\pi $.\vspace  \medskipamount \relax }}{minipage.1}{}}
\newlabel{sec:spherical-data}{{\rEfLiNK{x1-7900015.3}{\csname :autoref\endcsname{section}15.3}}{\rEfLiNK{x1-7900015.3}{\csname :autoref\endcsname{section}224}}{\rEfLiNK{x1-7900015.3}{\csname :autoref\endcsname{section}Spherical data}}{section.1}{}}
\newlabel{eq:lL}{{\rEfLiNK{x1-79001r8}{\csname :autoref\endcsname{equation}15.8}}{\rEfLiNK{x1-79001r8}{\csname :autoref\endcsname{equation}224}}{\rEfLiNK{x1-79001r8}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:SD}{{\rEfLiNK{x1-79002r9}{\csname :autoref\endcsname{equation}15.9}}{\rEfLiNK{x1-79002r9}{\csname :autoref\endcsname{equation}224}}{\rEfLiNK{x1-79002r9}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:AD}{{\rEfLiNK{x1-79003r10}{\csname :autoref\endcsname{equation}15.10}}{\rEfLiNK{x1-79003r10}{\csname :autoref\endcsname{equation}225}}{\rEfLiNK{x1-79003r10}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{fig:wulffschmidt}{{\rEfLiNK{x1-79004r9}{\csname :autoref\endcsname{minipage}15.9}}{\rEfLiNK{x1-79004r9}{\csname :autoref\endcsname{minipage}225}}{\rEfLiNK{x1-79004r9}{\csname :autoref\endcsname{minipage}a) Wulff equal angle and b) Schmidt equal area stereonets, with circles of $10^{\circ }$ radius drawn at azimuths of $0^{\circ }, 90^{\circ }, 180^{\circ }$ and $270^{\circ }$, and dips of $10^{\circ }$ and $90^\circ $. As the names suggest, the Wulff equal angle net preserves the shape of the circles, and the Schmidt equal area net their size.\vspace  \medskipamount \relax }}{minipage.1}{}}
\newlabel{fig:Africa}{{\rEfLiNK{x1-79010r10}{\csname :autoref\endcsname{minipage}15.10}}{\rEfLiNK{x1-79010r10}{\csname :autoref\endcsname{minipage}226}}{\rEfLiNK{x1-79010r10}{\csname :autoref\endcsname{minipage}a) Wulff and b) Schmidt projection of the African continent. The Wulff net shows Africa in its right shape, and the Schmidt net shows it at the right size. No two dimensional projection can achieve both goals at once.\vspace  \medskipamount \relax }}{minipage.1}{}}
\newlabel{fig:palaeomag}{{\rEfLiNK{x1-79013r11}{\csname :autoref\endcsname{minipage}15.11}}{\rEfLiNK{x1-79013r11}{\csname :autoref\endcsname{minipage}227}}{\rEfLiNK{x1-79013r11}{\csname :autoref\endcsname{minipage}The palaeomagnetic declination (=azimuth) and inclination (=dip) of 10\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}samples shown in a Schmidt equal area diagram.\vspace  \medskipamount \relax }}{minipage.1}{}}
\newlabel{fig:fault}{{\rEfLiNK{x1-79016r12}{\csname :autoref\endcsname{minipage}15.12}}{\rEfLiNK{x1-79016r12}{\csname :autoref\endcsname{minipage}227}}{\rEfLiNK{x1-79016r12}{\csname :autoref\endcsname{minipage}10 fault plane measurements shown on a Wulff stereonet. The white circles mark the `pole' of the planar measurements, i.e. a perpendicular line to the plane. The circular segments mark the intersection of the planes with bottom half of a sphere, shown in an equal angle projection.\vspace  \medskipamount \relax }}{minipage.1}{}}
\newlabel{sec:spherical-distributions}{{\rEfLiNK{x1-8000015.4}{\csname :autoref\endcsname{section}15.4}}{\rEfLiNK{x1-8000015.4}{\csname :autoref\endcsname{section}227}}{\rEfLiNK{x1-8000015.4}{\csname :autoref\endcsname{section}Spherical distributions}}{section.1}{}}
\newlabel{eq:vonMisesFisher}{{\rEfLiNK{x1-80001r14}{\csname :autoref\endcsname{equation}15.14}}{\rEfLiNK{x1-80001r14}{\csname :autoref\endcsname{equation}227}}{\rEfLiNK{x1-80001r14}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:3Dvectormean}{{\rEfLiNK{x1-80002r15}{\csname :autoref\endcsname{equation}15.15}}{\rEfLiNK{x1-80002r15}{\csname :autoref\endcsname{equation}228}}{\rEfLiNK{x1-80002r15}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{fig:sphericalmean}{{\rEfLiNK{x1-80007r13}{\csname :autoref\endcsname{minipage}15.13}}{\rEfLiNK{x1-80007r13}{\csname :autoref\endcsname{minipage}230}}{\rEfLiNK{x1-80007r13}{\csname :autoref\endcsname{minipage}a) the palaeomagnetic data of Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:palaeomag} shown in grey, with their vector mean marked as a black square. b) the fault data of Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:fault} in grey with the vector mean as a black square and great circle, respectively.\vspace  \medskipamount \relax }}{minipage.1}{}}
\newlabel{sec:orientations}{{\rEfLiNK{x1-8100015.5}{\csname :autoref\endcsname{section}15.5}}{\rEfLiNK{x1-8100015.5}{\csname :autoref\endcsname{section}230}}{\rEfLiNK{x1-8100015.5}{\csname :autoref\endcsname{section}Orientations}}{section.1}{}}
\newlabel{fig:pebbles}{{\rEfLiNK{x1-81001r14}{\csname :autoref\endcsname{minipage}15.14}}{\rEfLiNK{x1-81001r14}{\csname :autoref\endcsname{minipage}230}}{\rEfLiNK{x1-81001r14}{\csname :autoref\endcsname{minipage}20 pebbles in a sedimentary bed are aligned in a NE--SW direction. Half of the measurements were recorded as northeasterly and the other half as southwesterly. Both of angles are equivalent. The white circle marks the vector mean, which points in a southeasterly direction.\vspace  \medskipamount \relax }}{minipage.1}{}}
\newlabel{fig:inertia}{{\rEfLiNK{x1-81003r15}{\csname :autoref\endcsname{minipage}15.15}}{\rEfLiNK{x1-81003r15}{\csname :autoref\endcsname{minipage}231}}{\rEfLiNK{x1-81003r15}{\csname :autoref\endcsname{minipage}The diagonal line tilts at an angle of 40$^\circ $. It marks the direction that minimises the moment of inertia of a circle with point masses attached to the angular measurements marked by the radial ticks. If the circle and the point masses were spun around this line, then they would do so without wobbling. The same is true for the perpendicular direction, which maximises the moment of inertia.\vspace  \medskipamount \relax }}{minipage.1}{}}
\newlabel{eq:sphericalsigma}{{\rEfLiNK{x1-81005r20}{\csname :autoref\endcsname{equation}15.20}}{\rEfLiNK{x1-81005r20}{\csname :autoref\endcsname{equation}231}}{\rEfLiNK{x1-81005r20}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{fig:subhorizontal}{{\rEfLiNK{x1-81006r16}{\csname :autoref\endcsname{minipage}15.16}}{\rEfLiNK{x1-81006r16}{\csname :autoref\endcsname{minipage}232}}{\rEfLiNK{x1-81006r16}{\csname :autoref\endcsname{minipage}Open circles and grey lines shows 20 strike and dip measurements of a subhorizontal bed. The vector mean of these measurements is shown as a filled square and black line. It is perpendicular to the bed, which is clearly wrong. The large open square marks the direction that minimises the moment of inertia of the poles. This is a much more sensible orientational average.\vspace  \medskipamount \relax }}{minipage.1}{}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:spatial}{{\rEfLiNK{x1-8200016}{\csname :autoref\endcsname{chapter}16}}{\rEfLiNK{x1-8200016}{\csname :autoref\endcsname{chapter}233}}{\rEfLiNK{x1-8200016}{\csname :autoref\endcsname{chapter}Spatial data}}{chapter.1}{}}
\newlabel{tab:Meuse}{{\rEfLiNK{x1-82001r1}{\csname :autoref\endcsname{center}16.1}}{\rEfLiNK{x1-82001r1}{\csname :autoref\endcsname{center}233}}{\rEfLiNK{x1-82001r1}{\csname :autoref\endcsname{center} Locations and zinc concentrations of 155 samples of topsoil, collected in a flood plain of the river Meuse, near the village of Stein (Netherlands). $x$ and $y$ are the Easting and Northing (as metres), respectively, in Rijksdriehoek (RDH, Netherlands topographical) map coordinates. The column labelled `Zn' contains the zinc concentration (in ppm) from composite samples of approximately $15\times {15}$m areas.\relax }}{center.1}{}}
\newlabel{fig:meusepoints}{{\rEfLiNK{x1-82004r1}{\csname :autoref\endcsname{minipage}16.1}}{\rEfLiNK{x1-82004r1}{\csname :autoref\endcsname{minipage}234}}{\rEfLiNK{x1-82004r1}{\csname :autoref\endcsname{minipage}Location map of the Meuse data. The question mark is located at a position $\{x=179850,y=331650\}$ where no measurement was taken. This chapter will use a geostatistical technique called \textbf  {kriging} to estimate the (logarithm of) the zinc concentration at this location from the surrounding measurements.\vspace  \medskipamount \relax }}{minipage.1}{}}
\newlabel{sec:semivariogram}{{\rEfLiNK{x1-8300016.1}{\csname :autoref\endcsname{section}16.1}}{\rEfLiNK{x1-8300016.1}{\csname :autoref\endcsname{section}234}}{\rEfLiNK{x1-8300016.1}{\csname :autoref\endcsname{section}The semivariogram}}{section.1}{}}
\newlabel{tab:h=100}{{\rEfLiNK{x1-83001r2}{\csname :autoref\endcsname{center}16.2}}{\rEfLiNK{x1-83001r2}{\csname :autoref\endcsname{center}235}}{\rEfLiNK{x1-83001r2}{\csname :autoref\endcsname{center}Composite data table with the inter-sample distances shown above the diagonal and the log-contrasts of the corresponding zinc-concentrations shown below the diagonal. Boxed values mark sample pairs that are located within 150 metres of each other.\relax }}{center.1}{}}
\newlabel{tab:h=300}{{\rEfLiNK{x1-83004r3}{\csname :autoref\endcsname{center}16.3}}{\rEfLiNK{x1-83004r3}{\csname :autoref\endcsname{center}236}}{\rEfLiNK{x1-83004r3}{\csname :autoref\endcsname{center}The same composite matrix of Table\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {tab:h=100}, with boxed values marking inter-sample distances of 150 to 300 metres (above the diagonal) and their corresponding log-contrast (below the diagonal).\relax }}{center.1}{}}
\newlabel{fig:semivariogram}{{\rEfLiNK{x1-83006r2}{\csname :autoref\endcsname{minipage}16.2}}{\rEfLiNK{x1-83006r2}{\csname :autoref\endcsname{minipage}237}}{\rEfLiNK{x1-83006r2}{\csname :autoref\endcsname{minipage}Empirical semivariogram of the Meuse data. The semivariance is low at first and then gradually rises with increasing lag before levelling off to a plateau. Because the semivariance is inversely proportional to the degree of spatial correlation, the semivariogram tells us that there is relatively little variability among samples that are located close together, and more variability among samples that are far apart. In other words, the semivariogram formalises Tobler's \textit  {First Law of Geography}, which was quoted at the beginning of this chapter.\relax }}{minipage.1}{}}
\newlabel{sec:semivariogram-models}{{\rEfLiNK{x1-8400016.2}{\csname :autoref\endcsname{section}16.2}}{\rEfLiNK{x1-8400016.2}{\csname :autoref\endcsname{section}237}}{\rEfLiNK{x1-8400016.2}{\csname :autoref\endcsname{section}Semivariogram models}}{section.1}{}}
\newlabel{eq:semivariogramspherical}{{\rEfLiNK{x1-84001r2}{\csname :autoref\endcsname{equation}16.2}}{\rEfLiNK{x1-84001r2}{\csname :autoref\endcsname{equation}237}}{\rEfLiNK{x1-84001r2}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:semivariogramexponential}{{\rEfLiNK{x1-84002r3}{\csname :autoref\endcsname{equation}16.3}}{\rEfLiNK{x1-84002r3}{\csname :autoref\endcsname{equation}238}}{\rEfLiNK{x1-84002r3}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:semivariogramgaussian}{{\rEfLiNK{x1-84003r4}{\csname :autoref\endcsname{equation}16.4}}{\rEfLiNK{x1-84003r4}{\csname :autoref\endcsname{equation}238}}{\rEfLiNK{x1-84003r4}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{fig:semivariogramfit}{{\rEfLiNK{x1-84004r3}{\csname :autoref\endcsname{document}16.3}}{\rEfLiNK{x1-84004r3}{\csname :autoref\endcsname{document}238}}{\rEfLiNK{x1-84004r3}{\csname :autoref\endcsname{document}Parametric semivariogram fits for the Meuse data, using the a) spherical, b) exponential, and c) Gaussian models.\vspace  \medskipamount \relax }}{document.1}{}}
\newlabel{fig:snr}{{\rEfLiNK{x1-84009r4}{\csname :autoref\endcsname{minipage}16.4}}{\rEfLiNK{x1-84009r4}{\csname :autoref\endcsname{minipage}239}}{\rEfLiNK{x1-84009r4}{\csname :autoref\endcsname{minipage}Illustration of the range, sill and nugget of a spherical semivariogram model. The same three parameters also control the shape of the exponential and Gaussian models. In the case of the Meuse dataset, the nugget effect is likely caused by the fact that the zinc concentrations were averaged over a 15$\times $15\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}metre area, and that the variability within this sampling area is not captured by the semivariogram.\relax }}{minipage.1}{}}
\newlabel{sec:kriging}{{\rEfLiNK{x1-8500016.3}{\csname :autoref\endcsname{section}16.3}}{\rEfLiNK{x1-8500016.3}{\csname :autoref\endcsname{section}239}}{\rEfLiNK{x1-8500016.3}{\csname :autoref\endcsname{section}Kriging interpolation}}{section.1}{}}
\newlabel{eq:kriging}{{\rEfLiNK{x1-85001r5}{\csname :autoref\endcsname{equation}16.5}}{\rEfLiNK{x1-85001r5}{\csname :autoref\endcsname{equation}239}}{\rEfLiNK{x1-85001r5}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:krigingweights}{{\rEfLiNK{x1-85002r6}{\csname :autoref\endcsname{equation}16.6}}{\rEfLiNK{x1-85002r6}{\csname :autoref\endcsname{equation}240}}{\rEfLiNK{x1-85002r6}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{fig:meusecontour}{{\rEfLiNK{x1-85005r5}{\csname :autoref\endcsname{minipage}16.5}}{\rEfLiNK{x1-85005r5}{\csname :autoref\endcsname{minipage}241}}{\rEfLiNK{x1-85005r5}{\csname :autoref\endcsname{minipage} The kriging interpolation results for the Meuse dataset shown as a filled contour plot, with the measurements of Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:meusepoints} added as points of comparison. The extent of the contour plot is limited to the immediate vicinity of the measurements because kriging is an interpolation method and \emph  {not} an \emph  {extrapolation} method.\vspace  \medskipamount \relax }}{minipage.1}{}}
\newlabel{eq:kriginvar}{{\rEfLiNK{x1-85007r9}{\csname :autoref\endcsname{equation}16.9}}{\rEfLiNK{x1-85007r9}{\csname :autoref\endcsname{equation}242}}{\rEfLiNK{x1-85007r9}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{fig:meusecontourerr}{{\rEfLiNK{x1-85008r6}{\csname :autoref\endcsname{minipage}16.6}}{\rEfLiNK{x1-85008r6}{\csname :autoref\endcsname{minipage}242}}{\rEfLiNK{x1-85008r6}{\csname :autoref\endcsname{minipage} Contour plot of the relative uncertainty (coefficient of variation) of the kriging interpolation estimates of Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:meusecontour}. Note how the uncertainties are the greatest near the edges of the dataset, confirming again that kriging is not an extrapolation but an interpolation method.\vspace  \medskipamount \relax }}{minipage.1}{}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:R}{{\rEfLiNK{x1-8600017}{\csname :autoref\endcsname{chapter}17}}{\rEfLiNK{x1-8600017}{\csname :autoref\endcsname{chapter}245}}{\rEfLiNK{x1-8600017}{\csname :autoref\endcsname{chapter}An introduction to \texttt  {R}}}{chapter.1}{}}
\newlabel{sec:R-basics}{{\rEfLiNK{x1-8700017.1}{\csname :autoref\endcsname{section}17.1}}{\rEfLiNK{x1-8700017.1}{\csname :autoref\endcsname{section}245}}{\rEfLiNK{x1-8700017.1}{\csname :autoref\endcsname{section}The basics}}{section.1}{}}
\newlabel{it:geostats}{{\rEfLiNK{x1-8724913}{\csname :autoref\endcsname{section}13}}{\rEfLiNK{x1-8724913}{\csname :autoref\endcsname{section}250}}{\rEfLiNK{x1-8724913}{\csname :autoref\endcsname{section}The basics}}{section.1}{}}
\newlabel{sec:R-plotting}{{\rEfLiNK{x1-8800017.2}{\csname :autoref\endcsname{section}17.2}}{\rEfLiNK{x1-8800017.2}{\csname :autoref\endcsname{section}251}}{\rEfLiNK{x1-8800017.2}{\csname :autoref\endcsname{section}Plotting data}}{section.1}{}}
\newlabel{it:anscombe}{{\rEfLiNK{x1-880031}{\csname :autoref\endcsname{section}1}}{\rEfLiNK{x1-880031}{\csname :autoref\endcsname{section}251}}{\rEfLiNK{x1-880031}{\csname :autoref\endcsname{section}Plotting data}}{section.1}{}}
\newlabel{it:KDE}{{\rEfLiNK{x1-880675}{\csname :autoref\endcsname{section}5}}{\rEfLiNK{x1-880675}{\csname :autoref\endcsname{section}252}}{\rEfLiNK{x1-880675}{\csname :autoref\endcsname{section}Plotting data}}{section.1}{}}
\newlabel{sec:R-summary-statistics}{{\rEfLiNK{x1-8900017.3}{\csname :autoref\endcsname{section}17.3}}{\rEfLiNK{x1-8900017.3}{\csname :autoref\endcsname{section}254}}{\rEfLiNK{x1-8900017.3}{\csname :autoref\endcsname{section}Summary Statistics}}{section.1}{}}
\newlabel{it:mode}{{\rEfLiNK{x1-890242}{\csname :autoref\endcsname{section}2}}{\rEfLiNK{x1-890242}{\csname :autoref\endcsname{section}254}}{\rEfLiNK{x1-890242}{\csname :autoref\endcsname{section}Summary Statistics}}{section.1}{}}
\newlabel{fn:mode}{{\rEfLiNK{x1-89025f2}{\csname :autoref\endcsname{section}2}}{\rEfLiNK{x1-89025f2}{\csname :autoref\endcsname{section}254}}{\rEfLiNK{x1-89025f2}{\csname :autoref\endcsname{section}Summary Statistics}}{section.1}{}}
\newlabel{sec:R-probability}{{\rEfLiNK{x1-9000017.4}{\csname :autoref\endcsname{section}17.4}}{\rEfLiNK{x1-9000017.4}{\csname :autoref\endcsname{section}255}}{\rEfLiNK{x1-9000017.4}{\csname :autoref\endcsname{section}Probability}}{section.1}{}}
\newlabel{sec:R-binomial}{{\rEfLiNK{x1-9100017.5}{\csname :autoref\endcsname{section}17.5}}{\rEfLiNK{x1-9100017.5}{\csname :autoref\endcsname{section}256}}{\rEfLiNK{x1-9100017.5}{\csname :autoref\endcsname{section}The binomial distribution}}{section.1}{}}
\newlabel{it:1sidedbinomR}{{\rEfLiNK{x1-910284}{\csname :autoref\endcsname{section}4}}{\rEfLiNK{x1-910284}{\csname :autoref\endcsname{section}257}}{\rEfLiNK{x1-910284}{\csname :autoref\endcsname{section}The binomial distribution}}{section.1}{}}
\newlabel{it:2sidedbinomR}{{\rEfLiNK{x1-910415}{\csname :autoref\endcsname{section}5}}{\rEfLiNK{x1-910415}{\csname :autoref\endcsname{section}257}}{\rEfLiNK{x1-910415}{\csname :autoref\endcsname{section}The binomial distribution}}{section.1}{}}
\newlabel{sec:R-poisson}{{\rEfLiNK{x1-9200017.6}{\csname :autoref\endcsname{section}17.6}}{\rEfLiNK{x1-9200017.6}{\csname :autoref\endcsname{section}258}}{\rEfLiNK{x1-9200017.6}{\csname :autoref\endcsname{section}The Poisson distribution}}{section.1}{}}
\newlabel{it:1sidedpoisR}{{\rEfLiNK{x1-920264}{\csname :autoref\endcsname{section}4}}{\rEfLiNK{x1-920264}{\csname :autoref\endcsname{section}259}}{\rEfLiNK{x1-920264}{\csname :autoref\endcsname{section}The Poisson distribution}}{section.1}{}}
\newlabel{it:2sidedpoisR}{{\rEfLiNK{x1-920315}{\csname :autoref\endcsname{section}5}}{\rEfLiNK{x1-920315}{\csname :autoref\endcsname{section}259}}{\rEfLiNK{x1-920315}{\csname :autoref\endcsname{section}The Poisson distribution}}{section.1}{}}
\newlabel{sec:R-gauss}{{\rEfLiNK{x1-9300017.7}{\csname :autoref\endcsname{section}17.7}}{\rEfLiNK{x1-9300017.7}{\csname :autoref\endcsname{section}260}}{\rEfLiNK{x1-9300017.7}{\csname :autoref\endcsname{section}The normal distribution}}{section.1}{}}
\newlabel{sec:R-errorprop}{{\rEfLiNK{x1-9400017.8}{\csname :autoref\endcsname{section}17.8}}{\rEfLiNK{x1-9400017.8}{\csname :autoref\endcsname{section}261}}{\rEfLiNK{x1-9400017.8}{\csname :autoref\endcsname{section}Error propagation}}{section.1}{}}
\newlabel{fig:LLpois}{{\rEfLiNK{x1-94023r1}{\csname :autoref\endcsname{minipage}17.1}}{\rEfLiNK{x1-94023r1}{\csname :autoref\endcsname{minipage}262}}{\rEfLiNK{x1-94023r1}{\csname :autoref\endcsname{minipage}Log-likelihood function for the Poisson distribution, evaluated at different values for the parameter $\lambda $, given an observation of $k=4$ successes. The function reaches a maximum value at $\let \prOteCt \relax \let \prOteCt \relax \Protect \csname acp:c\endcsname {4}{\lambda }=4$. \relax }}{minipage.1}{}}
\newlabel{sec:R-comparingdistributions}{{\rEfLiNK{x1-9500017.9}{\csname :autoref\endcsname{section}17.9}}{\rEfLiNK{x1-9500017.9}{\csname :autoref\endcsname{section}263}}{\rEfLiNK{x1-9500017.9}{\csname :autoref\endcsname{section}Comparing distributions}}{section.1}{}}
\newlabel{it:2sided2samplettest}{{\rEfLiNK{x1-950233}{\csname :autoref\endcsname{section}3}}{\rEfLiNK{x1-950233}{\csname :autoref\endcsname{section}263}}{\rEfLiNK{x1-950233}{\csname :autoref\endcsname{section}Comparing distributions}}{section.1}{}}
\newlabel{sec:R-regression}{{\rEfLiNK{x1-9600017.10}{\csname :autoref\endcsname{section}17.10}}{\rEfLiNK{x1-9600017.10}{\csname :autoref\endcsname{section}265}}{\rEfLiNK{x1-9600017.10}{\csname :autoref\endcsname{section}Regression}}{section.1}{}}
\newlabel{sec:R-fractals}{{\rEfLiNK{x1-9700017.11}{\csname :autoref\endcsname{section}17.11}}{\rEfLiNK{x1-9700017.11}{\csname :autoref\endcsname{section}268}}{\rEfLiNK{x1-9700017.11}{\csname :autoref\endcsname{section}Fractals and chaos}}{section.1}{}}
\newlabel{it:finland}{{\rEfLiNK{x1-970011}{\csname :autoref\endcsname{section}1}}{\rEfLiNK{x1-970011}{\csname :autoref\endcsname{section}268}}{\rEfLiNK{x1-970011}{\csname :autoref\endcsname{section}Fractals and chaos}}{section.1}{}}
\newlabel{sec:R-unsupervised}{{\rEfLiNK{x1-9800017.12}{\csname :autoref\endcsname{section}17.12}}{\rEfLiNK{x1-9800017.12}{\csname :autoref\endcsname{section}269}}{\rEfLiNK{x1-9800017.12}{\csname :autoref\endcsname{section}Unsupervised learning}}{section.1}{}}
\newlabel{it:R-kmeans}{{\rEfLiNK{x1-980505}{\csname :autoref\endcsname{section}5}}{\rEfLiNK{x1-980505}{\csname :autoref\endcsname{section}270}}{\rEfLiNK{x1-980505}{\csname :autoref\endcsname{section}Unsupervised learning}}{section.1}{}}
\newlabel{sec:R-supervised}{{\rEfLiNK{x1-9900017.13}{\csname :autoref\endcsname{section}17.13}}{\rEfLiNK{x1-9900017.13}{\csname :autoref\endcsname{section}271}}{\rEfLiNK{x1-9900017.13}{\csname :autoref\endcsname{section}Supervised learning}}{section.1}{}}
\newlabel{it:LDA}{{\rEfLiNK{x1-990011}{\csname :autoref\endcsname{section}1}}{\rEfLiNK{x1-990011}{\csname :autoref\endcsname{section}271}}{\rEfLiNK{x1-990011}{\csname :autoref\endcsname{section}Supervised learning}}{section.1}{}}
\newlabel{sec:R-compositional}{{\rEfLiNK{x1-10000017.14}{\csname :autoref\endcsname{section}17.14}}{\rEfLiNK{x1-10000017.14}{\csname :autoref\endcsname{section}272}}{\rEfLiNK{x1-10000017.14}{\csname :autoref\endcsname{section}Compositional data}}{section.1}{}}
\newlabel{sec:R-directional}{{\rEfLiNK{x1-10100017.15}{\csname :autoref\endcsname{section}17.15}}{\rEfLiNK{x1-10100017.15}{\csname :autoref\endcsname{section}274}}{\rEfLiNK{x1-10100017.15}{\csname :autoref\endcsname{section}Directional data}}{section.1}{}}
\newlabel{it:Rbar}{{\rEfLiNK{x1-1010102}{\csname :autoref\endcsname{section}2}}{\rEfLiNK{x1-1010102}{\csname :autoref\endcsname{section}274}}{\rEfLiNK{x1-1010102}{\csname :autoref\endcsname{section}Directional data}}{section.1}{}}
\newlabel{sec:R-spatial}{{\rEfLiNK{x1-10200017.16}{\csname :autoref\endcsname{section}17.16}}{\rEfLiNK{x1-10200017.16}{\csname :autoref\endcsname{section}276}}{\rEfLiNK{x1-10200017.16}{\csname :autoref\endcsname{section}Spatial data}}{section.1}{}}
\newlabel{it:R-meuse-contour}{{\rEfLiNK{x1-1020304}{\csname :autoref\endcsname{section}4}}{\rEfLiNK{x1-1020304}{\csname :autoref\endcsname{section}276}}{\rEfLiNK{x1-1020304}{\csname :autoref\endcsname{section}Spatial data}}{section.1}{}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:exercises}{{\rEfLiNK{x1-10300018}{\csname :autoref\endcsname{chapter}18}}{\rEfLiNK{x1-10300018}{\csname :autoref\endcsname{chapter}279}}{\rEfLiNK{x1-10300018}{\csname :autoref\endcsname{chapter}Exercises}}{chapter.1}{}}
\newlabel{sec:ex-basics}{{\rEfLiNK{x1-10400018.1}{\csname :autoref\endcsname{section}18.1}}{\rEfLiNK{x1-10400018.1}{\csname :autoref\endcsname{section}279}}{\rEfLiNK{x1-10400018.1}{\csname :autoref\endcsname{section}The basics}}{section.1}{}}
\newlabel{sec:ex-plotting}{{\rEfLiNK{x1-10500018.2}{\csname :autoref\endcsname{section}18.2}}{\rEfLiNK{x1-10500018.2}{\csname :autoref\endcsname{section}279}}{\rEfLiNK{x1-10500018.2}{\csname :autoref\endcsname{section}Plotting data}}{section.1}{}}
\newlabel{it:AB}{{\rEfLiNK{x1-1050022}{\csname :autoref\endcsname{section}2}}{\rEfLiNK{x1-1050022}{\csname :autoref\endcsname{section}280}}{\rEfLiNK{x1-1050022}{\csname :autoref\endcsname{section}Plotting data}}{section.1}{}}
\newlabel{it:randxy}{{\rEfLiNK{x1-1050033}{\csname :autoref\endcsname{section}3}}{\rEfLiNK{x1-1050033}{\csname :autoref\endcsname{section}280}}{\rEfLiNK{x1-1050033}{\csname :autoref\endcsname{section}Plotting data}}{section.1}{}}
\newlabel{sec:ex-summary-statistics}{{\rEfLiNK{x1-10600018.3}{\csname :autoref\endcsname{section}18.3}}{\rEfLiNK{x1-10600018.3}{\csname :autoref\endcsname{section}280}}{\rEfLiNK{x1-10600018.3}{\csname :autoref\endcsname{section}Summary statistics}}{section.1}{}}
\newlabel{sec:ex-probability}{{\rEfLiNK{x1-10700018.4}{\csname :autoref\endcsname{section}18.4}}{\rEfLiNK{x1-10700018.4}{\csname :autoref\endcsname{section}280}}{\rEfLiNK{x1-10700018.4}{\csname :autoref\endcsname{section}Probability}}{section.1}{}}
\newlabel{sec:ex-binomial}{{\rEfLiNK{x1-10800018.5}{\csname :autoref\endcsname{section}18.5}}{\rEfLiNK{x1-10800018.5}{\csname :autoref\endcsname{section}281}}{\rEfLiNK{x1-10800018.5}{\csname :autoref\endcsname{section}The binomial distribution}}{section.1}{}}
\newlabel{sec:ex-poisson}{{\rEfLiNK{x1-10900018.6}{\csname :autoref\endcsname{section}18.6}}{\rEfLiNK{x1-10900018.6}{\csname :autoref\endcsname{section}281}}{\rEfLiNK{x1-10900018.6}{\csname :autoref\endcsname{section}The Poisson distribution}}{section.1}{}}
\newlabel{sec:ex-gauss}{{\rEfLiNK{x1-11000018.7}{\csname :autoref\endcsname{section}18.7}}{\rEfLiNK{x1-11000018.7}{\csname :autoref\endcsname{section}282}}{\rEfLiNK{x1-11000018.7}{\csname :autoref\endcsname{section}The normal distribution}}{section.1}{}}
\newlabel{sec:ex-errorprop}{{\rEfLiNK{x1-11100018.8}{\csname :autoref\endcsname{section}18.8}}{\rEfLiNK{x1-11100018.8}{\csname :autoref\endcsname{section}283}}{\rEfLiNK{x1-11100018.8}{\csname :autoref\endcsname{section}Error propagation}}{section.1}{}}
\newlabel{sec:ex-comparingdistributions}{{\rEfLiNK{x1-11200018.9}{\csname :autoref\endcsname{section}18.9}}{\rEfLiNK{x1-11200018.9}{\csname :autoref\endcsname{section}285}}{\rEfLiNK{x1-11200018.9}{\csname :autoref\endcsname{section}Comparing distributions}}{section.1}{}}
\newlabel{it:wine-t}{{\rEfLiNK{x1-1120011}{\csname :autoref\endcsname{section}1}}{\rEfLiNK{x1-1120011}{\csname :autoref\endcsname{section}285}}{\rEfLiNK{x1-1120011}{\csname :autoref\endcsname{section}Comparing distributions}}{section.1}{}}
\newlabel{it:one-sided}{{\rEfLiNK{x1-1120021}{\csname :autoref\endcsname{section}1a}}{\rEfLiNK{x1-1120021}{\csname :autoref\endcsname{section}285}}{\rEfLiNK{x1-1120021}{\csname :autoref\endcsname{section}Comparing distributions}}{section.1}{}}
\newlabel{it:two-sided}{{\rEfLiNK{x1-1120032}{\csname :autoref\endcsname{section}1b}}{\rEfLiNK{x1-1120032}{\csname :autoref\endcsname{section}286}}{\rEfLiNK{x1-1120032}{\csname :autoref\endcsname{section}Comparing distributions}}{section.1}{}}
\newlabel{sec:ex-regression}{{\rEfLiNK{x1-11300018.10}{\csname :autoref\endcsname{section}18.10}}{\rEfLiNK{x1-11300018.10}{\csname :autoref\endcsname{section}287}}{\rEfLiNK{x1-11300018.10}{\csname :autoref\endcsname{section}Regression}}{section.1}{}}
\newlabel{sec:ex-fractals}{{\rEfLiNK{x1-11400018.11}{\csname :autoref\endcsname{section}18.11}}{\rEfLiNK{x1-11400018.11}{\csname :autoref\endcsname{section}287}}{\rEfLiNK{x1-11400018.11}{\csname :autoref\endcsname{section}Fractals and chaos}}{section.1}{}}
\newlabel{sec:ex-unsupervised}{{\rEfLiNK{x1-11500018.12}{\csname :autoref\endcsname{section}18.12}}{\rEfLiNK{x1-11500018.12}{\csname :autoref\endcsname{section}288}}{\rEfLiNK{x1-11500018.12}{\csname :autoref\endcsname{section}Unsupervised learning}}{section.1}{}}
\newlabel{it:ex-withinss}{{\rEfLiNK{x1-1150033}{\csname :autoref\endcsname{section}3}}{\rEfLiNK{x1-1150033}{\csname :autoref\endcsname{section}288}}{\rEfLiNK{x1-1150033}{\csname :autoref\endcsname{section}Unsupervised learning}}{section.1}{}}
\newlabel{sec:ex-supervised}{{\rEfLiNK{x1-11600018.13}{\csname :autoref\endcsname{section}18.13}}{\rEfLiNK{x1-11600018.13}{\csname :autoref\endcsname{section}288}}{\rEfLiNK{x1-11600018.13}{\csname :autoref\endcsname{section}Supervised learning}}{section.1}{}}
\newlabel{it:LDA-training}{{\rEfLiNK{x1-1160011}{\csname :autoref\endcsname{section}1}}{\rEfLiNK{x1-1160011}{\csname :autoref\endcsname{section}288}}{\rEfLiNK{x1-1160011}{\csname :autoref\endcsname{section}Supervised learning}}{section.1}{}}
\newlabel{it:LDA-test}{{\rEfLiNK{x1-1160022}{\csname :autoref\endcsname{section}2}}{\rEfLiNK{x1-1160022}{\csname :autoref\endcsname{section}289}}{\rEfLiNK{x1-1160022}{\csname :autoref\endcsname{section}Supervised learning}}{section.1}{}}
\newlabel{it:rpart-training}{{\rEfLiNK{x1-1160033}{\csname :autoref\endcsname{section}3}}{\rEfLiNK{x1-1160033}{\csname :autoref\endcsname{section}289}}{\rEfLiNK{x1-1160033}{\csname :autoref\endcsname{section}Supervised learning}}{section.1}{}}
\newlabel{it:rpart-test}{{\rEfLiNK{x1-1160044}{\csname :autoref\endcsname{section}4}}{\rEfLiNK{x1-1160044}{\csname :autoref\endcsname{section}289}}{\rEfLiNK{x1-1160044}{\csname :autoref\endcsname{section}Supervised learning}}{section.1}{}}
\newlabel{sec:ex-compositional}{{\rEfLiNK{x1-11700018.14}{\csname :autoref\endcsname{section}18.14}}{\rEfLiNK{x1-11700018.14}{\csname :autoref\endcsname{section}289}}{\rEfLiNK{x1-11700018.14}{\csname :autoref\endcsname{section}Compositional data}}{section.1}{}}
\newlabel{it:mpg1}{{\rEfLiNK{x1-1170021}{\csname :autoref\endcsname{section}1a}}{\rEfLiNK{x1-1170021}{\csname :autoref\endcsname{section}289}}{\rEfLiNK{x1-1170021}{\csname :autoref\endcsname{section}Compositional data}}{section.1}{}}
\newlabel{it:mpg2}{{\rEfLiNK{x1-1170032}{\csname :autoref\endcsname{section}1b}}{\rEfLiNK{x1-1170032}{\csname :autoref\endcsname{section}289}}{\rEfLiNK{x1-1170032}{\csname :autoref\endcsname{section}Compositional data}}{section.1}{}}
\newlabel{it:mpg3}{{\rEfLiNK{x1-1170043}{\csname :autoref\endcsname{section}1c}}{\rEfLiNK{x1-1170043}{\csname :autoref\endcsname{section}289}}{\rEfLiNK{x1-1170043}{\csname :autoref\endcsname{section}Compositional data}}{section.1}{}}
\newlabel{it:mpg4}{{\rEfLiNK{x1-1170054}{\csname :autoref\endcsname{section}1d}}{\rEfLiNK{x1-1170054}{\csname :autoref\endcsname{section}289}}{\rEfLiNK{x1-1170054}{\csname :autoref\endcsname{section}Compositional data}}{section.1}{}}
\newlabel{it:mpg5}{{\rEfLiNK{x1-1170065}{\csname :autoref\endcsname{section}1e}}{\rEfLiNK{x1-1170065}{\csname :autoref\endcsname{section}290}}{\rEfLiNK{x1-1170065}{\csname :autoref\endcsname{section}Compositional data}}{section.1}{}}
\newlabel{sec:ex-directional}{{\rEfLiNK{x1-11800018.15}{\csname :autoref\endcsname{section}18.15}}{\rEfLiNK{x1-11800018.15}{\csname :autoref\endcsname{section}290}}{\rEfLiNK{x1-11800018.15}{\csname :autoref\endcsname{section}Directional data}}{section.1}{}}
\newlabel{sec:ex-spatial}{{\rEfLiNK{x1-11900018.16}{\csname :autoref\endcsname{section}18.16}}{\rEfLiNK{x1-11900018.16}{\csname :autoref\endcsname{section}291}}{\rEfLiNK{x1-11900018.16}{\csname :autoref\endcsname{section}Spatial data}}{section.1}{}}
\newlabel{it:hillsemivariogram}{{\rEfLiNK{x1-1190011}{\csname :autoref\endcsname{section}1}}{\rEfLiNK{x1-1190011}{\csname :autoref\endcsname{section}291}}{\rEfLiNK{x1-1190011}{\csname :autoref\endcsname{section}Spatial data}}{section.1}{}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:solutions}{{\rEfLiNK{x1-12000019}{\csname :autoref\endcsname{chapter}19}}{\rEfLiNK{x1-12000019}{\csname :autoref\endcsname{chapter}293}}{\rEfLiNK{x1-12000019}{\csname :autoref\endcsname{chapter}Solutions}}{chapter.1}{}}
\newlabel{sec:sol-basics}{{\rEfLiNK{x1-12100019.1}{\csname :autoref\endcsname{section}19.1}}{\rEfLiNK{x1-12100019.1}{\csname :autoref\endcsname{section}293}}{\rEfLiNK{x1-12100019.1}{\csname :autoref\endcsname{section}The basics}}{section.1}{}}
\newlabel{sec:sol-plotting}{{\rEfLiNK{x1-12200019.2}{\csname :autoref\endcsname{section}19.2}}{\rEfLiNK{x1-12200019.2}{\csname :autoref\endcsname{section}294}}{\rEfLiNK{x1-12200019.2}{\csname :autoref\endcsname{section}Plotting data}}{section.1}{}}
\newlabel{it:ABsol}{{\rEfLiNK{x1-1220122}{\csname :autoref\endcsname{section}2}}{\rEfLiNK{x1-1220122}{\csname :autoref\endcsname{section}294}}{\rEfLiNK{x1-1220122}{\csname :autoref\endcsname{section}Plotting data}}{section.1}{}}
\newlabel{it:xyrandsol}{{\rEfLiNK{x1-1220373}{\csname :autoref\endcsname{section}3}}{\rEfLiNK{x1-1220373}{\csname :autoref\endcsname{section}294}}{\rEfLiNK{x1-1220373}{\csname :autoref\endcsname{section}Plotting data}}{section.1}{}}
\newlabel{sec:sol-summary-statistics}{{\rEfLiNK{x1-12300019.3}{\csname :autoref\endcsname{section}19.3}}{\rEfLiNK{x1-12300019.3}{\csname :autoref\endcsname{section}295}}{\rEfLiNK{x1-12300019.3}{\csname :autoref\endcsname{section}Summary statistics}}{section.1}{}}
\newlabel{sec:sol-probability}{{\rEfLiNK{x1-12400019.4}{\csname :autoref\endcsname{section}19.4}}{\rEfLiNK{x1-12400019.4}{\csname :autoref\endcsname{section}297}}{\rEfLiNK{x1-12400019.4}{\csname :autoref\endcsname{section}Probability}}{section.1}{}}
\newlabel{sec:sol-binomial}{{\rEfLiNK{x1-12500019.5}{\csname :autoref\endcsname{section}19.5}}{\rEfLiNK{x1-12500019.5}{\csname :autoref\endcsname{section}299}}{\rEfLiNK{x1-12500019.5}{\csname :autoref\endcsname{section}The binomial distribution}}{section.1}{}}
\newlabel{it:binomtest}{{\rEfLiNK{x1-1250624}{\csname :autoref\endcsname{section}4}}{\rEfLiNK{x1-1250624}{\csname :autoref\endcsname{section}301}}{\rEfLiNK{x1-1250624}{\csname :autoref\endcsname{section}The binomial distribution}}{section.1}{}}
\newlabel{sec:sol-poisson}{{\rEfLiNK{x1-12600019.6}{\csname :autoref\endcsname{section}19.6}}{\rEfLiNK{x1-12600019.6}{\csname :autoref\endcsname{section}301}}{\rEfLiNK{x1-12600019.6}{\csname :autoref\endcsname{section}The Poisson distribution}}{section.1}{}}
\newlabel{sec:sol-gauss}{{\rEfLiNK{x1-12700019.7}{\csname :autoref\endcsname{section}19.7}}{\rEfLiNK{x1-12700019.7}{\csname :autoref\endcsname{section}303}}{\rEfLiNK{x1-12700019.7}{\csname :autoref\endcsname{section}The normal distribution}}{section.1}{}}
\newlabel{sec:sol-errorprop}{{\rEfLiNK{x1-12800019.8}{\csname :autoref\endcsname{section}19.8}}{\rEfLiNK{x1-12800019.8}{\csname :autoref\endcsname{section}305}}{\rEfLiNK{x1-12800019.8}{\csname :autoref\endcsname{section}Error propagation}}{section.1}{}}
\newlabel{sec:sol-comparingdistributions}{{\rEfLiNK{x1-12900019.9}{\csname :autoref\endcsname{section}19.9}}{\rEfLiNK{x1-12900019.9}{\csname :autoref\endcsname{section}308}}{\rEfLiNK{x1-12900019.9}{\csname :autoref\endcsname{section}Comparing distributions}}{section.1}{}}
\newlabel{it:sol-DZ-MDS}{{\rEfLiNK{x1-1291135}{\csname :autoref\endcsname{section}5}}{\rEfLiNK{x1-1291135}{\csname :autoref\endcsname{section}317}}{\rEfLiNK{x1-1291135}{\csname :autoref\endcsname{section}Comparing distributions}}{section.1}{}}
\newlabel{sec:sol-regression}{{\rEfLiNK{x1-13000019.10}{\csname :autoref\endcsname{section}19.10}}{\rEfLiNK{x1-13000019.10}{\csname :autoref\endcsname{section}318}}{\rEfLiNK{x1-13000019.10}{\csname :autoref\endcsname{section}Regression}}{section.1}{}}
\newlabel{sec:sol-fractals}{{\rEfLiNK{x1-13100019.11}{\csname :autoref\endcsname{section}19.11}}{\rEfLiNK{x1-13100019.11}{\csname :autoref\endcsname{section}320}}{\rEfLiNK{x1-13100019.11}{\csname :autoref\endcsname{section}Fractals and chaos}}{section.1}{}}
\newlabel{sec:sol-unsupervised}{{\rEfLiNK{x1-13200019.12}{\csname :autoref\endcsname{section}19.12}}{\rEfLiNK{x1-13200019.12}{\csname :autoref\endcsname{section}321}}{\rEfLiNK{x1-13200019.12}{\csname :autoref\endcsname{section}Unsupervised learning}}{section.1}{}}
\newlabel{fig:PCAiris}{{\rEfLiNK{x1-132006r1}{\csname :autoref\endcsname{minipage}19.1}}{\rEfLiNK{x1-132006r1}{\csname :autoref\endcsname{minipage}322}}{\rEfLiNK{x1-132006r1}{\csname :autoref\endcsname{minipage}PCA biplot for the \texttt  {iris} data. This diagram indicates there are two groups of flowers that with distinct petal width and length measurements (PC1). Within these two groups, additional variability is caused by the sepal width and length (PC2). The vector loadings of the petal width and length point in the same direction, which means that these two variables are correlated with each other. The vector loading of the sepal width is perpendicular to that of the petal measurements, which means that the sepal and petal dimensions vary independently.\vspace  \medskipamount \relax }}{minipage.1}{}}
\newlabel{fig:elbow}{{\rEfLiNK{x1-132032r2}{\csname :autoref\endcsname{minipage}19.2}}{\rEfLiNK{x1-132032r2}{\csname :autoref\endcsname{minipage}322}}{\rEfLiNK{x1-132032r2}{\csname :autoref\endcsname{minipage}Evaluating the within-cluster sum-of-squares ($ss$) of the k-means algorithm for different numbers of clusters. The $ss$-misfit drops off very quicky before making an `elbow' at $k=3$ clusters. Hence we cannot justify more than 3 clusters.\vspace  \medskipamount \relax }}{minipage.1}{}}
\newlabel{fig:DZtree}{{\rEfLiNK{x1-132041r3}{\csname :autoref\endcsname{minipage}19.3}}{\rEfLiNK{x1-132041r3}{\csname :autoref\endcsname{minipage}323}}{\rEfLiNK{x1-132041r3}{\csname :autoref\endcsname{minipage}Hierarchical clustering tree for the detrital zircon data. The tree conveys the same information as the MDS configuration of Figure\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {fig:DZmds}: there are two main clusters; T and Y are the two most similar samples; whilst L and 8 are the two most dissimilar samples.\relax }}{minipage.1}{}}
\newlabel{sec:sol-supervised}{{\rEfLiNK{x1-13300019.13}{\csname :autoref\endcsname{section}19.13}}{\rEfLiNK{x1-13300019.13}{\csname :autoref\endcsname{section}323}}{\rEfLiNK{x1-13300019.13}{\csname :autoref\endcsname{section}Supervised learning}}{section.1}{}}
\newlabel{it:sol-LDA-training}{{\rEfLiNK{x1-1330011}{\csname :autoref\endcsname{section}1}}{\rEfLiNK{x1-1330011}{\csname :autoref\endcsname{section}323}}{\rEfLiNK{x1-1330011}{\csname :autoref\endcsname{section}Supervised learning}}{section.1}{}}
\newlabel{it:sol-LDA-test}{{\rEfLiNK{x1-1330142}{\csname :autoref\endcsname{section}2}}{\rEfLiNK{x1-1330142}{\csname :autoref\endcsname{section}324}}{\rEfLiNK{x1-1330142}{\csname :autoref\endcsname{section}Supervised learning}}{section.1}{}}
\newlabel{sec:sol-compositional}{{\rEfLiNK{x1-13400019.14}{\csname :autoref\endcsname{section}19.14}}{\rEfLiNK{x1-13400019.14}{\csname :autoref\endcsname{section}325}}{\rEfLiNK{x1-13400019.14}{\csname :autoref\endcsname{section}Compositional data}}{section.1}{}}
\newlabel{it:sol-mpg1}{{\rEfLiNK{x1-1340021}{\csname :autoref\endcsname{section}1a}}{\rEfLiNK{x1-1340021}{\csname :autoref\endcsname{section}325}}{\rEfLiNK{x1-1340021}{\csname :autoref\endcsname{section}Compositional data}}{section.1}{}}
\newlabel{it:sol-mpg2}{{\rEfLiNK{x1-1340092}{\csname :autoref\endcsname{section}1b}}{\rEfLiNK{x1-1340092}{\csname :autoref\endcsname{section}325}}{\rEfLiNK{x1-1340092}{\csname :autoref\endcsname{section}Compositional data}}{section.1}{}}
\newlabel{it:sol-mpg3}{{\rEfLiNK{x1-1340123}{\csname :autoref\endcsname{section}1c}}{\rEfLiNK{x1-1340123}{\csname :autoref\endcsname{section}325}}{\rEfLiNK{x1-1340123}{\csname :autoref\endcsname{section}Compositional data}}{section.1}{}}
\newlabel{it:sol-mpg4}{{\rEfLiNK{x1-1340194}{\csname :autoref\endcsname{section}1d}}{\rEfLiNK{x1-1340194}{\csname :autoref\endcsname{section}325}}{\rEfLiNK{x1-1340194}{\csname :autoref\endcsname{section}Compositional data}}{section.1}{}}
\newlabel{it:sol-mpg5}{{\rEfLiNK{x1-1340265}{\csname :autoref\endcsname{section}1e}}{\rEfLiNK{x1-1340265}{\csname :autoref\endcsname{section}326}}{\rEfLiNK{x1-1340265}{\csname :autoref\endcsname{section}Compositional data}}{section.1}{}}
\newlabel{it:sol-ternary-test}{{\rEfLiNK{x1-1340372}{\csname :autoref\endcsname{section}2}}{\rEfLiNK{x1-1340372}{\csname :autoref\endcsname{section}326}}{\rEfLiNK{x1-1340372}{\csname :autoref\endcsname{section}Compositional data}}{section.1}{}}
\newlabel{fig:testPCA}{{\rEfLiNK{x1-134063r4}{\csname :autoref\endcsname{minipage}19.4}}{\rEfLiNK{x1-134063r4}{\csname :autoref\endcsname{minipage}327}}{\rEfLiNK{x1-134063r4}{\csname :autoref\endcsname{minipage}PCA biplot of the oceanic basalt compositions. The data roughly fall into three clusters, corresponding to OIB, MORB and IAB. OIBs are relatively rich in K\textsubscript  {2}O and poor in CaO; MORBs are rich in MgO and TiO\textsubscript  {2}; and IABs are rich in MnO and poor in TiO\textsubscript  {2}. K\textsubscript  {2}O and CaO are anti-correlated, and so are TiO\textsubscript  {2} and MnO. The variability in MnO/TiO\textsubscript  {2} is independent of the variability in MgO/K\textsubscript  {2}O. The `outliers' on the ternary diagram of exercise\leavevmode \special {t4ht@+&{35}x00A0{59}}x{}\let \prOteCt \relax \let \prOteCt \relax \Protect \::ref {it:sol-ternary-test} are OIBs that are particularly poor in CaO.\relax }}{minipage.1}{}}
\newlabel{sec:sol-directional}{{\rEfLiNK{x1-13500019.15}{\csname :autoref\endcsname{section}19.15}}{\rEfLiNK{x1-13500019.15}{\csname :autoref\endcsname{section}327}}{\rEfLiNK{x1-13500019.15}{\csname :autoref\endcsname{section}Directional data}}{section.1}{}}
\newlabel{sec:sol-spatial}{{\rEfLiNK{x1-13600019.16}{\csname :autoref\endcsname{section}19.16}}{\rEfLiNK{x1-13600019.16}{\csname :autoref\endcsname{section}329}}{\rEfLiNK{x1-13600019.16}{\csname :autoref\endcsname{section}Spatial data}}{section.1}{}}
\newlabel{it:normodel}{{\rEfLiNK{x1-1360011}{\csname :autoref\endcsname{section}1}}{\rEfLiNK{x1-1360011}{\csname :autoref\endcsname{section}329}}{\rEfLiNK{x1-1360011}{\csname :autoref\endcsname{section}Spatial data}}{section.1}{}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:tables}{{\rEfLiNK{x1-13700020}{\csname :autoref\endcsname{chapter}20}}{\rEfLiNK{x1-13700020}{\csname :autoref\endcsname{chapter}333}}{\rEfLiNK{x1-13700020}{\csname :autoref\endcsname{chapter}Tables}}{chapter.1}{}}
\newlabel{tab:normal}{{\rEfLiNK{x1-137001r1}{\csname :autoref\endcsname{table}20.1}}{\rEfLiNK{x1-137001r1}{\csname :autoref\endcsname{table}335}}{\rEfLiNK{x1-137001r1}{\csname :autoref\endcsname{table}Upper quantiles of the standard normal distribution (lower quantiles are the negative equivalents of these values for reasons of symmetry)\relax }}{table.1}{}}
\newlabel{tab:t}{{\rEfLiNK{x1-137003r2}{\csname :autoref\endcsname{table}20.2}}{\rEfLiNK{x1-137003r2}{\csname :autoref\endcsname{table}338}}{\rEfLiNK{x1-137003r2}{\csname :autoref\endcsname{table}Upper quantiles of the t-distribution with $df$ degrees of freedom (lower quantiles are the negative equivalents of these values for reasons of symmetry)\relax }}{table.1}{}}
\newlabel{tab:chi2}{{\rEfLiNK{x1-137005r3}{\csname :autoref\endcsname{table}20.3}}{\rEfLiNK{x1-137005r3}{\csname :autoref\endcsname{table}341}}{\rEfLiNK{x1-137005r3}{\csname :autoref\endcsname{table}Quantiles of the Chi-square distribution with $df$ degrees of freedom\relax }}{table.1}{}}
\newlabel{tab:w0.025}{{\rEfLiNK{x1-137007r4}{\csname :autoref\endcsname{table}20.4}}{\rEfLiNK{x1-137007r4}{\csname :autoref\endcsname{table}344}}{\rEfLiNK{x1-137007r4}{\csname :autoref\endcsname{table}Lower 2.5-percentiles (upper bounds of the rejection region for $\alpha =0.025$) of Wilcoxon rank-sum distributions for different sample sizes $n_1$ (column labels) and $n_2$ (row labels), where ${n_1}\leq {n_2}$.\relax }}{table.1}{}}
\newlabel{tab:w0.05}{{\rEfLiNK{x1-137009r5}{\csname :autoref\endcsname{table}20.5}}{\rEfLiNK{x1-137009r5}{\csname :autoref\endcsname{table}347}}{\rEfLiNK{x1-137009r5}{\csname :autoref\endcsname{table}Lower 5-percentiles (upper bounds of the rejection region for $\alpha =0.05$) of Wilcoxon rank-sum distributions for sample sizes $n_1$ (column labels) and $n_2$ (row labels), where ${n_1}\leq {n_2}$.\relax }}{table.1}{}}
\newlabel{tab:wilcox}{{\rEfLiNK{x1-137009r5}{\csname :autoref\endcsname{table}20.5}}{\rEfLiNK{x1-137009r5}{\csname :autoref\endcsname{table}347}}{\rEfLiNK{x1-137009r5}{\csname :autoref\endcsname{table}Lower 5-percentiles (upper bounds of the rejection region for $\alpha =0.05$) of Wilcoxon rank-sum distributions for sample sizes $n_1$ (column labels) and $n_2$ (row labels), where ${n_1}\leq {n_2}$.\relax }}{table.1}{}}
\gdef \@abspage@last{352}
